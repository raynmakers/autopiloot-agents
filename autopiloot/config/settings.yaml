# Autopiloot Configuration File
# Runtime settings for orchestrator, scraper, transcriber, summarizer, observability, and linkedin agents

# Modular Agent Configuration
enabled_agents:
  - "orchestrator_agent"  # Required: CEO agent coordinates all workflows
  - "scraper_agent"       # YouTube content discovery
  - "transcriber_agent"   # Video transcription via AssemblyAI
  - "summarizer_agent"    # Business-focused summarization
  - "observability_agent" # Monitoring and notifications
  - "linkedin_agent"      # LinkedIn content ingestion
  - "strategy_agent"      # Strategic analysis and playbooks
  - "drive_agent"         # Google Drive content tracking

# Communication flows between agents (source -> target)
communication_flows:
  # CEO (orchestrator) coordinates all other agents
  - ["orchestrator_agent", "scraper_agent"]
  - ["orchestrator_agent", "transcriber_agent"]
  - ["orchestrator_agent", "summarizer_agent"]
  - ["orchestrator_agent", "linkedin_agent"]
  - ["orchestrator_agent", "strategy_agent"]
  - ["orchestrator_agent", "drive_agent"]
  - ["orchestrator_agent", "observability_agent"]

  # Primary content processing workflow
  - ["scraper_agent", "transcriber_agent"]      # Discovered videos -> transcription
  - ["transcriber_agent", "summarizer_agent"]   # Transcripts -> summaries

  # LinkedIn content analysis workflow
  - ["linkedin_agent", "strategy_agent"]        # LinkedIn content -> strategic analysis

  # Observability monitoring (bidirectional)
  - ["observability_agent", "scraper_agent"]
  - ["observability_agent", "transcriber_agent"]
  - ["observability_agent", "summarizer_agent"]
  - ["observability_agent", "linkedin_agent"]
  - ["observability_agent", "strategy_agent"]
  - ["observability_agent", "drive_agent"]

  # Error reporting and status updates back to observability
  - ["scraper_agent", "observability_agent"]
  - ["transcriber_agent", "observability_agent"]
  - ["summarizer_agent", "observability_agent"]
  - ["linkedin_agent", "observability_agent"]
  - ["strategy_agent", "observability_agent"]
  - ["drive_agent", "observability_agent"]

# ID of the Google Sheet used for backfill links
sheet: "1ikj9GU4_LG-q4i0ACsgrJ8BECX_YW-ujfr4kKWlJt8A"

scraper:
  handles:
    - "@AlexHormozi"
    - "@danmartell"
    - "@KimPerell"
  daily_limit_per_channel: 10
  page_size: 50  # Maximum number of videos to return per page (1-50)

sheets:
  daily_limit_per_channel: 10
  range_a1: "Sheet1!A:D"

llm:
  tasks:
    # Summarizer Agent Tasks
    summarizer_generate_short:
      model: "o3-mini"  # GPT-5 reasoning model
      temperature: 1.0  # High reasoning mode
      max_output_tokens: 50000  # No practical limit
      reasoning_effort: "high"  # Maximum reasoning
      prompt_id: "comprehensive_coach_v2"
      prompt_version: "v2"

    # Strategy Agent Tasks
    strategy_analyze_tone:
      model: "gpt-4o"
      temperature: 0.1
      max_output_tokens: 500
      prompt_id: "tone_analysis_v1"

    strategy_classify_posts:
      model: "gpt-4o"
      temperature: 0.1
      max_output_tokens: 1000
      prompt_id: "post_classification_v1"

    strategy_synthesize_playbook:
      model: "gpt-4o"
      temperature: 0.3
      max_output_tokens: 800
      prompt_id: "playbook_synthesis_v1"

    strategy_generate_briefs:
      model: "gpt-4o"
      temperature: 0.7
      max_output_tokens: 1000
      prompt_id: "content_briefs_v1"

    strategy_cluster_topics:
      embedding_model: "text-embedding-3-small"
      clustering_method: "kmeans"
      prompt_id: "topic_clustering_v1"

    # RAG (Hybrid Retrieval) Tasks
    rag_answer_question:
      model: "gpt-4o"  # GPT-4 for reasoning with citations
      temperature: 0.2  # Low temperature for factual accuracy
      max_output_tokens: 2000  # Sufficient for comprehensive answers with citations
      prompt_id: "rag_qa_v1"  # Question answering with hybrid retrieval context

notifications:
  slack:
    channel: "ops-autopiloot"
    digest:
      enabled: true
      time: "07:00"
      timezone: "Europe/Amsterdam"
      channel: "ops-autopiloot"
      sections:
        - "summary"
        - "budgets"
        - "issues"
        - "links"

budgets:
  transcription_daily_usd: 5.0
  alert_threshold: 0.8  # 80% threshold for budget alerts

idempotency:
  max_video_duration_sec: 4200  # 70 minutes maximum
  status_progression:
    - "discovered"
    - "transcribed" 
    - "summarized"
  drive_naming_format: "{video_id}_{date}_{type}.{ext}"

reliability:
  retry:
    max_attempts: 3  # Maximum retry attempts before DLQ
    base_delay_sec: 60  # Base delay for exponential backoff in seconds
  quotas:
    youtube_daily_limit: 10000  # YouTube Data API daily quota
    assemblyai_daily_limit: 100  # AssemblyAI daily transcription limit

orchestrator:
  parallelism:
    max_parallel_jobs: 5  # Maximum concurrent jobs across all agents
    max_dispatch_batch: 10  # Maximum items per batch dispatch
  coordination:
    run_timeout_minutes: 120  # Maximum runtime for daily runs
    dlq_escalation_threshold: 5  # DLQ items before escalation
  policies:
    budget_enforcement: true  # Enable budget limit enforcement
    quota_enforcement: true  # Enable API quota enforcement
    max_retries_per_video: 3  # Global retry limit per video

rapidapi:
  # Centralized RapidAPI plugin configuration
  # Each plugin has its own host and API key
  # Multiple tools can reference the same plugin
  plugins:
    linkedin_scraper:
      host: "fresh-linkedin-scraper-api.p.rapidapi.com"
      api_key_env: "RAPIDAPI_LINKEDIN_SCRAPER_KEY"  # References env var in .env
      endpoints:
        user_posts: "/api/v1/user/posts"
        user_profile: "/api/v1/user/profile"
        user_comments: "/api/v1/user/comments"
        post_comments: "/api/v1/post/comments"
        post_comment_replies: "/api/v1/post/comments/replies"
        post_reactions: "/api/v1/post/reactions"
        post_reposts: "/api/v1/post/reposts"
    # Add more plugins as needed:
    # example_plugin:
    #   host: "another-api.p.rapidapi.com"
    #   api_key_env: "RAPIDAPI_EXAMPLE_KEY"

summarizer:
  zep:
    # Zep v3 Architecture (Threads API via HTTP - no SDK due to Python 3.13 incompatibility)
    # - Users: Represent YouTube channels (user_id = channel_handle without @, lowercase)
    # - Threads: Represent individual videos (thread_id = "summary_{video_id}")
    # - Messages: Contain summary content with metadata
    # - Knowledge Graph: Zep automatically builds from message content
    enabled: true
    api_version: "v3"  # Using Threads API with direct HTTP calls
    user_id_format: "channel_handle_lowercase"  # e.g., "@DanMartell" -> "danmartell"
    thread_id_format: "summary_{video_id}"  # e.g., "summary_mZxDw92UXmA"

linkedin:
  api:
    # RapidAPI configuration is in rapidapi.plugins.linkedin_scraper section
    rate_limit_per_minute: 60  # Rate limiting for LinkedIn API calls
    max_posts_per_profile: 50  # Maximum posts to fetch per profile
    rapidapi_plugin: "linkedin_scraper"  # References rapidapi.plugins.linkedin_scraper
  profiles:
    # Target LinkedIn profiles for content ingestion
    - "alexhormozi"  # Example LinkedIn username/handle
  zep:
    group_prefix: "linkedin"  # Prefix for Zep group names (e.g., "linkedin_posts", "linkedin_comments")
    collection_name: "linkedin_content"  # Zep collection for LinkedIn content
  processing:
    daily_limit_per_profile: 25  # Maximum content items per profile per day
    content_types:
      - "posts"
      - "comments"
      - "reactions"
    min_engagement_threshold: 5  # Minimum likes/comments to process content

drive:
  tracking:
    # Configured Drive files and folders to track for content changes
    targets:
      # Example folder configuration
      - type: "folder"
        id: "example_folder_id_here"
        recursive: true
        name: "Strategy Documents"
      # Example file configuration
      - type: "file"
        id: "example_file_id_here"
        name: "Playbook.docx"
    sync_interval_minutes: 60  # How often to check for changes
    max_file_size_mb: 10  # Maximum file size to process
    supported_formats:
      - ".txt"
      - ".md"
      - ".pdf"
      - ".docx"
      - ".html"
      - ".csv"

rag:
  # Automatic workflow integration
  auto_ingest_after_transcription: true  # Automatically ingest full transcripts to RAG systems after save_transcript_record
  # When enabled, transcript workflow will automatically call:
  # 1. UpsertFullTranscriptToZep (semantic search)
  # 2. IndexFullTranscriptToOpenSearch (keyword search, if configured)
  # 3. StreamFullTranscriptToBigQuery (SQL analytics, if configured)
  # Failures in RAG ingestion do NOT block the transcript workflow

  # Adaptive Query Routing
  # Intelligently routes queries to optimal retrieval sources based on query characteristics
  routing:
    mode: "adaptive"  # Options: "adaptive" (smart routing), "always_on" (use all sources)
    always_use_all_sources: false  # Override: always use all available sources regardless of query

    # Routing Strategy Rules:
    # - Strong filters (dates + channel) → OpenSearch + BigQuery (precise filtering)
    # - Conceptual queries without filters → Zep (semantic understanding)
    # - Factual queries with filters → OpenSearch + BigQuery (keyword + structured)
    # - Mixed intent or uncertainty → All available sources (comprehensive coverage)
    # - Fallback → All available sources (maximum coverage)

    logging:
      enabled: true  # Log all routing decisions with reasoning
      log_level: "info"  # Options: "debug", "info", "warning", "error"

  zep:
    namespace:
      drive: "autopiloot-dev"  # Zep namespace for Drive content indexing

    # Full transcript storage configuration for Hybrid RAG
    transcripts:
      enabled: true
      group_format: "youtube_transcripts_{channel_id}"  # Group format for transcript threads
      thread_id_format: "transcript_{video_id}"  # Thread ID format: e.g., "transcript_mZxDw92UXmA"
      user_id_format: "youtube_{channel_id}"  # User ID format for channel organization

      chunking:
        # Token-aware chunking for long transcripts
        max_tokens_per_chunk: 1000  # Maximum tokens per chunk (balance retrieval granularity vs context)
        overlap_tokens: 100  # Token overlap between chunks for context continuity
        strategy: "token_aware"  # Chunking strategy: "token_aware" or "paragraph"

      metadata:
        # Metadata fields to store with each chunk
        include_video_metadata: true  # Include title, channel_id, published_at, duration_sec
        include_content_hash: true  # SHA-256 hash for idempotency and deduplication
        include_chunk_position: true  # Store chunk_index and total_chunks for ordering
        include_timestamp_range: true  # Store start_time and end_time if available

      deduplication:
        # Idempotency strategy for transcript chunks
        strategy: "content_hash"  # Options: "content_hash", "video_id_chunk_id", "both"
        hash_algorithm: "sha256"  # Hash algorithm for content deduplication

  opensearch:
    # OpenSearch configuration for keyword/boolean retrieval in Hybrid RAG
    # Used alongside Zep for semantic + keyword search combination
    enabled: true
    host: ""  # Set via OPENSEARCH_HOST environment variable
    index_transcripts: "autopiloot_transcripts"  # Index name for transcript storage
    top_k: 20  # Number of results to retrieve from OpenSearch
    timeout_ms: 1500  # Request timeout in milliseconds
    weights:
      # Hybrid search weights (must sum to 1.0)
      semantic: 0.6  # Weight for semantic (vector) search results
      keyword: 0.4   # Weight for keyword (BM25) search results
    connection:
      # Connection settings
      verify_certs: true  # Verify SSL certificates
      use_ssl: true       # Use SSL/TLS for connections
      max_retries: 3      # Maximum number of retry attempts
      retry_on_timeout: true  # Retry on timeout errors

  bigquery:
    # BigQuery configuration for transcript chunk storage and analytics
    # Provides SQL-based filtering, reporting, and structured data access for Hybrid RAG
    # Storage Strategy: METADATA ONLY (no full text) with optional text_snippet (<=256 chars) for previews
    # Uses existing GOOGLE_APPLICATION_CREDENTIALS and GCP_PROJECT_ID from environment
    enabled: true
    dataset: "autopiloot"  # BigQuery dataset name
    location: "EU"  # Dataset location (EU, US, etc.)
    tables:
      transcript_chunks: "transcript_chunks"  # Table for transcript chunk storage
    schema:
      # Schema for transcript_chunks table (auto-created if missing)
      # - video_id: STRING (YouTube video ID)
      # - chunk_id: STRING (Unique chunk identifier)
      # - title: STRING (Video title)
      # - channel_id: STRING (YouTube channel ID)
      # - published_at: TIMESTAMP (Video publication date)
      # - duration_sec: INT64 (Video duration in seconds)
      # - content_sha256: STRING (SHA256 hash of chunk text for idempotency)
      # - tokens: INT64 (Token count for chunk)
      # - text_snippet: STRING (Preview text, max 256 chars - metadata only, no full text)
      # Idempotent writes handled by (video_id, chunk_id) or content_sha256
    write_disposition: "WRITE_APPEND"  # Append new rows (handle deduplication in queries)
    batch_size: 500  # Number of rows per batch insert

  # Policy Enforcement
  # Uniform authorization and content filtering for retrieval results
  # Applied after fusion (HybridRetrieval) but before LLM reasoning
  policy:
    enabled: true  # Enable policy enforcement
    default_mode: "filter"  # Options: "filter" (remove), "redact" (mask), "audit_only" (log only)

    # Sensitive content patterns for redaction
    # Each pattern has: name, regex pattern, severity (low/medium/high/critical), replacement text
    sensitive_patterns:
      - name: "email"
        pattern: "\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b"
        severity: "medium"
        replacement: "[EMAIL REDACTED]"

      - name: "phone"
        pattern: "\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b"
        severity: "medium"
        replacement: "[PHONE REDACTED]"

      - name: "ssn"
        pattern: "\\b\\d{3}-\\d{2}-\\d{4}\\b"
        severity: "high"
        replacement: "[SSN REDACTED]"

      - name: "credit_card"
        pattern: "\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b"
        severity: "high"
        replacement: "[CARD REDACTED]"

    # Authorization rules
    authorization:
      channel_based: true  # Enable channel-based access control
      date_based: false  # Enable date-based access control (max age restrictions)
      user_based: false  # Enable user-based access control

    # Audit logging
    audit:
      enabled: true  # Log all policy enforcement decisions
      log_violations: true  # Log policy violations
      log_redactions: true  # Log redaction actions
      retention_days: 90  # Audit log retention period

  # Evidence Alignment and Conflict Resolution
  # Detects overlapping evidence across sources and resolves contradictions
  # Applied after hybrid retrieval fusion for consistency verification
  evidence_alignment:
    enabled: true  # Enable evidence alignment detection
    similarity_threshold: 0.85  # Text similarity threshold for detecting overlaps (0.0-1.0)

    # Trust hierarchy for conflict resolution
    trust_hierarchy:
      multi_source: 3  # Highest trust (2+ sources)
      zep: 2           # High trust (semantic understanding)
      bigquery: 2      # High trust (structured data)
      opensearch: 1    # Medium trust (keyword search)

    # BigQuery verification for fact-checking
    bigquery_verification:
      enabled: false  # Enable BigQuery structured fact verification
      # When enabled, uses BigQuery metadata to verify conflicting claims

    # Conflict detection settings
    conflict_detection:
      numerical_threshold: 0.5  # Similarity threshold for detecting numerical conflicts
      temporal_threshold: 0.5   # Similarity threshold for detecting date conflicts
      enable_categorical: true  # Enable categorical conflict detection

    # Logging and analysis
    logging:
      enabled: true  # Log all alignment analysis and resolutions
      log_level: "info"  # Options: "debug", "info", "warning", "error"
      include_rationale: true  # Include detailed rationale in outputs

  # Tracing and Observability
  # Comprehensive instrumentation for hybrid RAG pipeline monitoring
  # Tracks per-source latency, error rates, coverage, and fusion performance
  observability:
    enabled: true  # Enable tracing and observability
    trace_all_requests: true  # Generate trace IDs for all retrieval requests

    # Latency monitoring and thresholds
    latency:
      slow_path_threshold_ms: 1000  # Threshold for identifying slow execution paths
      alert_p95_threshold_ms: 2000  # Alert if p95 latency exceeds this value
      alert_max_threshold_ms: 5000  # Alert if max latency exceeds this value

    # Error rate monitoring and thresholds
    error_rates:
      warning_threshold: 25  # Warning alert if error rate exceeds 25%
      critical_threshold: 50  # Critical alert if error rate exceeds 50%
      track_per_source: true  # Track error rates separately per source

    # Coverage monitoring and thresholds
    coverage:
      warning_threshold: 67  # Warning if coverage below 67% (less than 2 sources)
      critical_threshold: 33  # Critical if coverage below 33% (less than 1 source)
      track_source_availability: true  # Track which sources are available

    # Alert configuration
    alerts:
      enabled: true  # Enable alerting based on thresholds
      slack_channel: "ops-autopiloot"  # Slack channel for RAG alerts
      throttle_minutes: 60  # Throttle duplicate alerts (1 hour)
      severity_levels:
        - "critical"  # Immediate attention required
        - "warning"   # Degraded performance
        - "info"      # Informational alerts

    # Daily digest configuration
    daily_digest:
      enabled: true  # Include RAG metrics in daily digest
      time: "07:00"  # Digest generation time
      timezone: "Europe/Amsterdam"  # Digest timezone
      sections:
        - "summary"        # Overall statistics
        - "per_source"     # Per-source performance breakdown
        - "slow_paths"     # Slow execution paths
        - "errors"         # Error summaries
        - "coverage"       # Source coverage statistics
        - "fusion"         # Fusion performance metrics

    # Metrics collection and storage
    metrics:
      retention_days: 30  # Keep metrics for 30 days
      aggregation_interval_minutes: 5  # Aggregate metrics every 5 minutes
      store_raw_traces: false  # Store full trace data (set true for debugging)

    # Logging configuration
    logging:
      enabled: true  # Log all trace events
      log_level: "info"  # Options: "debug", "info", "warning", "error"
      include_trace_context: true  # Include trace IDs in logs

  # Security and IAM Configuration
  # Comprehensive security settings for hybrid RAG pipeline
  # Enforces least-privilege access, secure transport, and credential management
  security:
    enabled: true  # Enable security validation

    # BigQuery IAM and Security
    bigquery:
      # Required IAM roles for service account (least-privilege)
      required_roles:
        - "roles/bigquery.dataEditor"  # Create/write datasets and tables
        - "roles/bigquery.jobUser"     # Run queries and jobs
      # Security recommendations
      recommendations:
        - "Use service account with least-privilege roles"
        - "Store credentials via GOOGLE_APPLICATION_CREDENTIALS env var"
        - "Never commit service account JSON to version control"
        - "Rotate service account keys regularly"
        - "Enable Cloud Audit Logs for BigQuery access"
      # Required environment variables
      required_env_vars:
        - "GCP_PROJECT_ID"
        - "GOOGLE_APPLICATION_CREDENTIALS"

    # OpenSearch Security
    opensearch:
      # Transport security
      tls:
        enabled: true  # Require TLS/SSL for connections
        verify_certs: true  # Verify SSL certificates
      # Authentication methods (choose one)
      authentication:
        method: "api_key"  # Options: "api_key", "basic_auth"
        # For API key authentication
        api_key_env_var: "OPENSEARCH_API_KEY"
        # For basic authentication
        username_env_var: "OPENSEARCH_USERNAME"
        password_env_var: "OPENSEARCH_PASSWORD"
      # Security recommendations
      recommendations:
        - "Use HTTPS (TLS) for all OpenSearch connections"
        - "Prefer API key authentication over basic auth"
        - "Use read-only credentials for query operations"
        - "Store credentials in environment variables only"
        - "Enable certificate verification in production"
        - "Rotate API keys/passwords regularly"
      # Required environment variables
      required_env_vars:
        - "OPENSEARCH_HOST"

    # Zep Security
    zep:
      # Transport security
      tls:
        enabled: true  # Require TLS/SSL for connections
      # Authentication
      authentication:
        api_key_env_var: "ZEP_API_KEY"
      # Security recommendations
      recommendations:
        - "Use HTTPS for all Zep API connections"
        - "Store ZEP_API_KEY in environment variables only"
        - "Never commit API keys to version control"
        - "Use separate API keys for dev/staging/prod"
        - "Rotate API keys regularly"
      # Required environment variables
      required_env_vars:
        - "ZEP_API_KEY"

    # Credential Management
    credentials:
      # Enforce environment-based credentials
      require_env_vars: true  # All credentials must be from environment
      # Prohibited patterns (never hardcode these)
      prohibited_patterns:
        - "api_key\\s*=\\s*['\\\"]\\w+['\\\"]"
        - "password\\s*=\\s*['\\\"]\\w+['\\\"]"
        - "token\\s*=\\s*['\\\"]\\w+['\\\"]"
        - "secret\\s*=\\s*['\\\"]\\w+['\\\"]"
      # Security recommendations
      recommendations:
        - "Never hardcode API keys or secrets in code"
        - "Use environment variables for all credentials"
        - "Add .env to .gitignore"
        - "Use separate credentials for different environments"
        - "Implement secret rotation policies"
        - "Consider using secret management services (e.g., Google Secret Manager)"
      # Critical environment variables (must be set)
      critical_env_vars:
        - "OPENAI_API_KEY"
        - "ASSEMBLYAI_API_KEY"
        - "YOUTUBE_API_KEY"
        - "ZEP_API_KEY"
        - "GCP_PROJECT_ID"
        - "GOOGLE_APPLICATION_CREDENTIALS"

    # PII Protection
    pii:
      # PII masking/redaction (uses rag.policy.sensitive_patterns)
      enabled: true  # Enable PII detection and redaction
      reference_config: "rag.policy.sensitive_patterns"
      # Security recommendations
      recommendations:
        - "Enable PII redaction for sensitive data"
        - "Configure custom patterns for domain-specific PII"
        - "Log all PII redaction actions for compliance"
        - "Review redaction patterns regularly"

    # Security Validation
    validation:
      enabled: true  # Enable security validation tool
      strict_mode: false  # Fail on warnings (set true for production)
      check_on_startup: true  # Validate security on system startup
      validation_frequency: "daily"  # Options: "startup", "daily", "manual"

    # Audit and Compliance
    audit:
      enabled: true  # Enable security audit logging
      log_credential_usage: false  # Log credential usage (sensitive)
      log_access_patterns: true  # Log access patterns for anomaly detection
      retention_days: 90  # Audit log retention period
      compliance_frameworks:
        - "GDPR"  # EU data protection
        - "SOC2"  # Security and availability

