---
description: "Add A/B tuning for fusion weights and scoring parameters"
globs: []
alwaysApply: false
---

id: "TASK-RAG-0072J"
title: "A/B tuning for hybrid fusion"
status: "completed"
priority: "P3"
labels: ["tuning", "retrieval", "observability_agent"]
dependencies: ["TASK-RAG-0072E"]
created: "2025-10-12"
completed: "2025-10-13"

# 1) Objective

Enable runtime-adjustable weights and experiment tags to evaluate fusion.

# 2) Steps

1. Configurable weights/top_k/timeouts with overrides via flags.
2. Log parameters and outcomes; basic evaluator comparing fused vs single-source.
3. Observability dashboards for experiments.

# 3) Acceptance

- Weights can be toggled without redeploy; experiment metrics visible.

---

# Implementation Summary

## âœ… Deliverables

**Experiment Management**: `summarizer_agent/tools/manage_rag_experiment.py` (~560 lines)
**Evaluation Tool**: `summarizer_agent/tools/evaluate_rag_experiment.py` (~570 lines)
**Test Suites**: 2 comprehensive test files (28 + 27 = 55 tests total)
**Configuration**: Added `rag.experiments` section to `config/settings.yaml`

### Features Implemented

**Experiment Management (ManageRAGExperiment)**:
1. âœ… CRUD operations (create, read, update, delete, list)
2. âœ… Activate/deactivate experiments
3. âœ… Runtime-adjustable fusion weights (semantic, keyword, SQL)
4. âœ… Configurable top_k, timeouts, fusion algorithms
5. âœ… Experiment tags for grouping
6. âœ… Weight validation (0-1 range, sum to 1.0)
7. âœ… Parameter constraints enforcement

**Evaluation Tool (EvaluateRAGExperiment)**:
1. âœ… Relevance metrics (Precision@K, Recall@K, NDCG@K, MRR)
2. âœ… Source comparison (fused vs single-source overlap)
3. âœ… Performance metrics tracking (latency, coverage)
4. âœ… Ground truth evaluation support
5. âœ… User feedback integration
6. âœ… Outcome logging to experiment records

### Configuration (settings.yaml)

```yaml
rag:
  experiments:
    enabled: true
    default_parameters:
      weights:
        semantic: 0.6  # Zep
        keyword: 0.3   # OpenSearch
        sql: 0.1       # BigQuery
      retrieval:
        top_k: 20
        timeout_ms: 2000
      fusion:
        algorithm: "rrf"
        rrf_k: 60
      reranking:
        enabled: false

    evaluation:
      metrics:
        - "precision@5/10/20"
        - "recall@5/10/20"
        - "ndcg@5/10/20"
        - "mrr"

    observability:
      dashboard_enabled: true
      slack:
        enabled: true
        notify_on_activate: true
        notify_on_complete: true
```

### Usage Examples

**Create Experiment:**
```python
tool = ManageRAGExperiment(
    operation="create",
    experiment_name="Weight Tuning v1",
    experiment_tag="weight-tuning",
    weights_semantic=0.7,
    weights_keyword=0.3,
    weights_sql=0.0,
    top_k=20,
    status="active"
)
result = tool.run()
```

**Evaluate Experiment:**
```python
tool = EvaluateRAGExperiment(
    experiment_id="exp_12345",
    query="How to increase revenue",
    fused_results=json.dumps([...]),
    zep_results=json.dumps([...]),
    opensearch_results=json.dumps([...]),
    ground_truth=json.dumps(["doc1", "doc2"])
)
result = tool.run()
# Returns: precision, recall, NDCG, MRR metrics
```

## âœ… Acceptance Criteria

1. âœ… **Weights togglable without redeploy**: Runtime parameter adjustment via experiment management
2. âœ… **Experiment metrics visible**: Comprehensive evaluation with relevance metrics and source comparison

## ðŸŽ¯ Production Status

**READY FOR PRODUCTION**

### Key Benefits
- No redeployment needed for parameter changes
- A/B testing capability for fusion optimization
- Comprehensive relevance metrics (Precision, Recall, NDCG, MRR)
- Source-level comparison and overlap analysis
- Integration with observability dashboards
- Slack notifications for experiment lifecycle

**Recommendation**: Production-ready for systematic A/B testing of hybrid RAG fusion parameters with comprehensive evaluation metrics and runtime configurability.
