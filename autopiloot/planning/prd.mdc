---
# Autopiloot — PRD v2 (Additive Agents Only)

- **Agency Name:** Autopiloot
- **Version:** v2 Draft (additive changes only)
- **Date:** <YYYY-MM-DD>
- **Owner:** <your name>
- **Baseline Reference:** `planning/archive/01-initial-foundation-prd.mdc` (v1 baseline; unchanged)

---

## Purpose (Delta Only)

Add new agents to extend the Autopiloot agency. The existing four agents and their tools, data model, schedules, and flows from v1 remain unchanged unless explicitly modified here.

---

## Communication Flows (New/Modified Only)

- Between Agents (new flows only):
  - <New Agent X> -> <Existing/New Agent Y>: <trigger and expected outcome>
- Agent ↔ User (changes only):
  - Channels: <Slack/Other> (if new)
  - Cadence: <if any new communications>

---

## New Agents and Tools

> Define only agents that are new in v2. Use one section per agent.

### New Agent 1 — LinkedIn Agent

- Role: Discover and ingest a user’s LinkedIn content footprint: fetch posts (historical up to 1000) and daily deltas, gather comments on those posts, and collect comments made by the user on other people’s posts. Normalize and upsert all items into Zep under a per-user group for GraphRAG queries.
- Integration: Writes to Zep (new group/collection per LinkedIn user), optional audit records in Firestore; exposes artifacts to Summarizer and Observability for downstream analysis and reporting.
- Reference: RapidAPI Fresh LinkedIn Scraper (see docs: `https://docs.saleleads.ai/api-reference/user-additional-data/get-user-experiences`).
- Tools (snake_case, BaseTool, JSON string outputs):
  - get_user_posts
    - Description: Fetch a user’s posts with pagination; supports historical backfill up to last 1000 items and daily incremental fetch.
    - Inputs: `urn` (str, required), `page` (int, default 1), `page_size` (int, default per API), `max_items` (int, default 1000), `since_iso` (str, optional)
    - Validation: `urn` required; enforce `max_items` cap; retry/backoff on 429/5xx.
    - Core Functions: Call RapidAPI endpoint(s) to list posts, collate pages until limits reached; dedupe by post URN/ID; normalize fields.
    - APIs: RapidAPI Fresh LinkedIn Scraper (GET endpoints for user posts).
    - Output: JSON `{ items: [...], total, has_more }`
  - get_post_comments
    - Description: For a batch of post IDs, fetch comments (paginated) and normalize.
    - Inputs: `post_ids` (List[str], required), `page` (int, default 1)
    - Validation: Non-empty list; batch sizes <= API constraints.
    - Core Functions: Iterate posts; fetch comments; dedupe and normalize; handle pagination.
    - APIs: RapidAPI endpoints for post comments.
    - Output: JSON `{ comments: [...], counts: {post_id: n} }`
  - get_post_reactions
    - Description: Fetch reactions/engagement metrics for a batch of post IDs (likes, celebrates, etc.).
    - Inputs: `post_ids` (List[str], required)
    - Validation: Batch within API limits; retry/backoff on 429/5xx.
    - Core Functions: Call RapidAPI reactions endpoint; aggregate counts per reaction type.
    - APIs: RapidAPI endpoint for post reactions.
    - Output: JSON `{ reactions: {post_id: { total: int, breakdown: {...} }}, missing: [...] }`
  - get_user_comment_activity
    - Description: Fetch comments authored by the user on other people’s posts.
    - Inputs: `urn` (str, required), `page` (int, default 1), `max_items` (int, default 1000)
    - Validation: `urn` required; cap `max_items`; backoff on rate limits.
    - Core Functions: Query user comments; extract referenced post metadata when available; normalize.
    - APIs: RapidAPI endpoints for user comments.
    - Output: JSON `{ comments_authored: [...], total }`
  - normalize_linkedin_content
    - Description: Transform raw LinkedIn payloads into stable internal schema (post, comment, author, timestamps, links, media, metrics).
    - Inputs: `raw_items` (JSON), `entity_type` ("post"|"comment")
    - Validation: Required fields present; drop/flag malformed.
    - Core Functions: Map fields; attach computed keys (e.g., `source: linkedin`, `urn`, `external_url`); include metrics such as `reaction_count`, `comment_count`, `view_count` when available.
    - APIs: None.
    - Output: JSON `{ normalized: [...] }`
  - compute_linkedin_stats
    - Description: Compute statistics across fetched posts/comments, including top posts by reactions, comments, and views; summary totals and time buckets.
    - Inputs: `posts` (JSON list), `comments` (JSON list), `reactions` (JSON), `top_n` (int, default 10)
    - Validation: Inputs non-empty; sanitize missing metrics.
    - Core Functions: Aggregate totals; sort to find top N by `reaction_count`, `comment_count`, and `view_count`; produce per-day histograms.
    - APIs: None.
    - Output: JSON `{ totals: {...}, top: { by_reactions: [...], by_comments: [...], by_views: [...] }, histograms: {...} }`
  - upsert_to_zep_group
    - Description: Upsert posts and comments into a Zep group dedicated to the LinkedIn user for GraphRAG.
    - Inputs: `group_id` (str, required), `items` (JSON list), `metadata` (JSON)
    - Validation: Zep credentials present; group exists or create-on-write; idempotent by `urn`.
    - Core Functions: Create/find group `linkedin_<username_or_hash>`; upsert documents with metadata (urn, author, type, timestamps, links, metrics like reaction_count/comment_count/view_count); optionally attach a stats summary document per run.
    - APIs: Zep SDK/API
    - Output: JSON `{ upserted: n, skipped: n, group_id }`
  - save_ingestion_record
    - Description: Write an audit record of the ingestion run (counts, durations, errors) to Firestore.
    - Inputs: `urn` (str), `stats` (JSON), `started_at` (str), `ended_at` (str)
    - Validation: ISO timestamps; stats presence.
    - Core Functions: Upsert `linkedin_ingestions/{urn}/{date}` doc with metrics.
    - APIs: Firestore Admin SDK
    - Output: string status
  - deduplicate_entities
    - Description: Deduplicate posts/comments by natural keys (URN/ID + timestamp) prior to storage.
    - Inputs: `items` (JSON), `key_fields` (List[str])
    - Validation: Key fields exist.
    - Core Functions: Hash/set-based dedupe; emit duplicates count.
    - APIs: None
    - Output: JSON `{ unique: [...], duplicates: n }`

### New Agent 2 — Strategy Agent

- Role: Perform deep research over the LinkedIn corpus to determine what works: identify high-engagement topics, exact phrases that trigger responses, audience language patterns, and synthesis into actionable content strategy to amplify future content.
- Integration: Consumes the LinkedIn Agent’s Zep group (`linkedin_<username_or_hash>`) and statistics; outputs a Strategy Playbook stored in Firestore/Drive and optional Zep documents for retrieval; may notify via Slack.
- Tools (snake_case, BaseTool, JSON string outputs):
  - fetch_corpus_from_zep
    - Description: Retrieve posts/comments (and optional stats docs) for a specific LinkedIn Zep group.
    - Inputs: `group_id` (str, required), `filters` (JSON, optional), `limit` (int, default 2000)
    - Validation: Group must exist; sensible limit caps.
    - Core Functions: Query Zep; return documents with text + metadata (urn, reaction_count, comment_count, view_count, timestamps, author, type).
    - APIs: Zep SDK/API
    - Output: JSON `{ items: [...], total }`
  - compute_engagement_signals
    - Description: Compute normalized engagement signals per item (e.g., reactions, comments, views; optionally reaction rates if impressions available).
    - Inputs: `items` (JSON), `weights` (JSON, optional), `min_engagement_threshold` (float, default 0)
    - Validation: Missing metrics handled; numeric conversions; thresholding.
    - Core Functions: Produce `engagement_score` per item and summary aggregates.
    - APIs: None
    - Output: JSON `{ items: [...], aggregates: {...} }`
  - extract_keywords_and_phrases
    - Description: Extract salient n-grams/keyphrases and entities correlated with engagement.
    - Inputs: `items` (JSON), `top_n` (int, default 50)
    - Validation: Text length, language checks.
    - Core Functions: NLP keyphrase extraction + TF-IDF/YAKE variants; rank by correlation with `engagement_score`.
    - APIs: None
    - Output: JSON `{ keywords: [...], phrases: [...], entities: [...] }`
  - classify_post_types
    - Description: Classify each post into a post type taxonomy (e.g., personal story, how-to, listicle, opinion, case study, announcement, CTA/promo, question/poll).
    - Inputs: `items` (JSON), `taxonomy` (List[str], optional), `model` (str, optional)
    - Validation: Minimum text length; handle links-only posts.
    - Core Functions: LLM-assisted or heuristics+embeddings classification; output per-item `post_type` with confidence; aggregate distribution.
    - APIs: OpenAI (or compatible) or local classifier
    - Output: JSON `{ items: [{id, post_type, confidence}], distribution: {type: count} }`
  - analyze_tone_of_voice
    - Description: Detect tone/style for each post (e.g., authoritative, conversational, inspirational, contrarian, humorous, urgent) and aggregate patterns.
    - Inputs: `items` (JSON), `labels` (List[str], optional), `model` (str, optional)
    - Validation: Language detection; minimum length checks.
    - Core Functions: LLM-assisted tone detection or zero-shot classifier; add per-item `tone_labels` with scores; compute overall tone distribution and co-occurrence with engagement.
    - APIs: OpenAI (or compatible) or local zero-shot classifier
    - Output: JSON `{ items: [{id, tones: [{label, score}]}], distribution: {label: count}, correlations: {...} }`
  - cluster_topics_embeddings
    - Description: Topic clustering using embeddings (e.g., sentence-transformers via Zep or local) with labeling.
    - Inputs: `items` (JSON), `num_clusters` (int, optional), `min_cluster_size` (int, optional)
    - Validation: Enough items; parameters sane.
    - Core Functions: Embed texts; cluster; compute cluster-level engagement; auto-label with top terms.
    - APIs: Zep embeddings or local model
    - Output: JSON `{ clusters: [{id, label, items:[...], engagement: {...}}], embeddings_meta: {...} }`
  - mine_trigger_phrases
    - Description: Identify “trigger” phrases statistically associated with high engagement (e.g., log-odds with informative Dirichlet prior).
    - Inputs: `items` (JSON), `top_n` (int, default 50)
    - Validation: Minimum positives; text normalization.
    - Core Functions: Build high vs low engagement cohorts; compute association scores per phrase; return ranked triggers and anti-triggers.
    - APIs: None
    - Output: JSON `{ triggers: [...], anti_triggers: [...], method: "log_odds" }`
  - synthesize_strategy_playbook
    - Description: Synthesize findings into an actionable Strategy Playbook: winning topics, trigger phrases, audience vocabulary, formats, hooks, call-to-action patterns.
    - Inputs: `keywords` (JSON), `topics` (JSON), `triggers` (JSON), `post_types` (JSON), `tones` (JSON), `examples` (JSON), `constraints` (JSON, optional)
    - Validation: Required sections present.
    - Core Functions: LLM-assisted synthesis using `llm.tasks.strategy_playbook` settings; produce structured Markdown/JSON.
    - APIs: OpenAI (or compatible)
    - Output: JSON `{ playbook_md, playbook_json, prompt_id, token_usage }`
  - generate_content_briefs
    - Description: Produce content briefs/templates based on the playbook (angles, hooks, outline, keywords) for amplification.
    - Inputs: `playbook_json` (JSON), `count` (int, default 5)
    - Validation: Non-empty playbook.
    - Core Functions: LLM-assisted brief generation; ensure diversity across topics/triggers.
    - APIs: OpenAI (or compatible)
    - Output: JSON `{ briefs: [...], prompt_id, token_usage }`
  - save_strategy_artifacts
    - Description: Persist Strategy Playbook and briefs to Drive (Markdown/JSON) and Firestore (`strategy_reports/{urn}/{date}`), and optionally Zep document for retrieval.
    - Inputs: `urn` (str), `playbook_md` (str), `playbook_json` (JSON), `briefs` (JSON)
    - Validation: Required fields; service accounts present.
    - Core Functions: Upload to Drive; write Firestore doc with references; upsert Zep doc tagged `strategy`.
    - APIs: Google Drive, Firestore, Zep
    - Output: JSON `{ drive_ids: {...}, report_doc_ref, zep_doc_id }`

### New Agent 3 — <optional>

- Role:
- Integration:
- Tools:

---

## Configuration (Delta Only)

- Environment Variables: <new envs only>
- `settings.yaml` keys: <new/updated keys>

---

## Data & Storage (Delta Only)

- Firestore (new/updated): <collections/doc fields added>
- Google Drive (new folders/naming): <if any>
- Zep (new collections/metadata): <if any>

---

## Scheduling & Triggers (Delta Only)

- New cron schedules: <e.g., 07:00 daily digest>
- New Firestore triggers: <path and conditions>

---

## Reliability & Observability (Delta Only)

- New budgets/thresholds: <if any>
- New alerts: <channels/throttling>
- Audit logging additions: <events/fields>

---

## Acceptance Criteria (For New Agents)

- <Agent X> instantiated with instructions.md and tools wired.
- All new tools inherit `agency_swarm.tools.BaseTool` and return JSON strings.
- Tests: one dedicated test file per new tool + one agent-level test.
- CI passes; lint and naming rules (snake_case) respected.
- Documentation updated (README sections for new agents).

---

## Open Questions

- Which new agent(s) to add (name, role)?
- For each new agent: list tools (4–16), inputs/outputs, and external APIs.
- Any new schedules/triggers or Slack communications?
- Any new Firestore/Drive/Zep artifacts?
- KPIs for the new agents (what success looks like)?
