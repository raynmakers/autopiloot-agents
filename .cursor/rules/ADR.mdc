---
description: Architectural Decision Records
globs:
alwaysApply: false
---

# Architecture Decision Log

<!--
ADR_AGENT_PROTOCOL v1.0

You (the agent) manage this file as the single source of truth for all ADRs.

INVARIANTS
- Keep this exact file structure and headings.
- All ADR entries use H2 headings: "## ADR-XXXX — <Title>" (4-digit zero-padded ID).
- Allowed Status values: Proposed | Accepted | Superseded
- Date format: YYYY-MM-DD
- New entries must be appended to the END of the file.
- The Index table between the INDEX markers must always reflect the latest state and be sorted by ID desc (newest on top).
- Each ADR MUST contain: Date, Status, Owner, Context, Decision, Consequences.
- Each ADR must include an explicit anchor `<a id="adr-XXXX"></a>` so links remain stable.

HOW TO ADD A NEW ADR
1) Read the whole file.
2) Compute next ID:
   - Scan for headings matching: ^## ADR-(\d{4}) — .+$
   - next_id = (max captured number) + 1, left-pad to 4 digits.
3) Create a new ADR section using the “New ADR Entry Template” below.
   - Place it AFTER the last ADR section in the file.
   - Add an `<a id="adr-XXXX"></a>` line immediately below the heading.
4) Update the Index (between the INDEX markers):
   - Insert/replace the row for this ADR keeping the table sorted by ID descending.
   - Title in the Index MUST link to the anchor: [<Title>](#adr-XXXX)
   - If this ADR supersedes another: set “Supersedes” in this row, and update that older ADR:
       a) Change its Status to “Superseded”
       b) Add “Superseded by: ADR-XXXX” in its Consequences block
       c) Update the older ADR’s Index row “Superseded by” column to ADR-XXXX
5) Validate before saving:
   - Exactly one heading exists for ADR-XXXX
   - All required fields are present and non-empty
   - Index contains a row for ADR-XXXX and remains properly sorted
6) Concurrency resolution:
   - If a merge conflict or duplicate ID is detected after reading: recompute next_id from the current file state, rename your heading, anchor, and Index row accordingly, and retry once.

COMMIT MESSAGE SUGGESTION
- "ADR-XXXX: <Short Title> — <Status>"

END ADR_AGENT_PROTOCOL
-->

## Index

<!-- BEGIN:ADR_INDEX -->

| ID   | Title                                                        | Date       | Status   | Supersedes | Superseded by |
| ---- | ------------------------------------------------------------ | ---------- | -------- | ---------- | ------------- |
| 0011 | [Google Sheets and Web Page YouTube Extraction Tools](#adr-0011) | 2025-09-14 | Accepted | —          | —             |
| 0010 | [YouTube Uploads Playlist Discovery Tool](#adr-0010)         | 2025-09-14 | Accepted | —          | —             |
| 0009 | [YouTube Channel Handle Resolution Tool](#adr-0009)          | 2025-09-14 | Accepted | —          | —             |
| 0008 | [Agency Swarm v1.0.0 Tool Implementation](#adr-0008)        | 2025-09-14 | Accepted | —          | —             |
| 0007 | [Firebase Functions v2 Scheduling Architecture](#adr-0007)   | 2025-09-14 | Accepted | —          | —             |
| 0006 | [Modular Agent Tool Architecture](#adr-0006)                | 2025-09-14 | Accepted | —          | —             |
| 0005 | [Reliability and Quota Management System](#adr-0005)         | 2025-09-14 | Accepted | —          | —             |
| 0004 | [Comprehensive Environment Configuration System](#adr-0004)  | 2025-01-15 | Accepted | —          | —             |
| 0003 | [MVP Orchestration and Tooling Choices](#adr-0003)           | 2025-09-11 | Accepted | —          | —             |
| 0002 | [Event-Driven Broker Architecture with Firestore](#adr-0002) | 2025-01-27 | Accepted | —          | —             |
| 0001 | [Agent-Focused Repository Structure](#adr-0001)              | 2025-01-27 | Accepted | —          | —             |

<!-- END:ADR_INDEX -->

---

## ADR-0004 — Comprehensive Environment Configuration System

<a id="adr-0004"></a>
**Date**: 2025-01-15
**Status**: Accepted
**Owner**: AI Agent

### Context

The Autopiloot system integrates with 8+ external services requiring API credentials: OpenAI (GPT-4.1), AssemblyAI (transcription), YouTube Data API (discovery), Slack (notifications), Google Drive & Sheets (storage), Zep (GraphRAG), and optional Langfuse (observability). Manual environment setup across development and production environments is error-prone, lacks validation, and poses security risks. Previous approaches with simple .env files led to deployment failures due to missing or incorrectly formatted variables.

### Alternatives

- **Simple .env files only**: No validation, unclear required vs optional variables, prone to deployment errors with cryptic failure messages
- **Hard-coded configuration**: Major security risk, no environment flexibility, violates 12-factor app principles
- **Complex configuration frameworks** (e.g., Hydra, OmegaConf): Overkill for MVP scope, unnecessary dependencies, learning curve overhead
- **Environment-specific config files**: Risk of committing secrets, configuration drift between environments, maintenance burden
- **Cloud provider secret management**: Platform lock-in, additional complexity for development environment, cost for MVP

### Decision

Implement comprehensive environment configuration system with structured validation and developer experience focus:

**Core Components:**

- `env.template` - Complete variable reference without secrets, serves as documentation
- `config/env_loader.py` - Validation module with type checking, clear error messages, and service-specific getters
- Production-ready error handling with graceful fallback to system environment variables
- Complete test coverage (17 test cases) covering validation scenarios, edge cases, and error conditions
- `ENVIRONMENT.md` - Step-by-step setup guide with API key acquisition instructions and troubleshooting

**Key Features:**

- Service-specific API key access (`get_api_key('openai')`, `get_api_key('slack')`, etc.)
- Required vs optional variable distinction with sensible defaults
- File existence validation for Google service account credentials
- Clear separation between development (.env) and production (system vars) configuration
- Comprehensive error messages guiding developers to resolution

### Consequences

- **Pros**: Eliminates deployment configuration errors, provides immediate validation feedback, ensures secure credential handling, comprehensive documentation reduces onboarding time, production-ready error handling, 100% test coverage prevents regressions, service-specific getters prevent API key mix-ups
- **Cons / risks**: Additional complexity over simple .env approach, requires initial setup documentation maintenance, dependency on python-dotenv package, potential over-engineering for simple use cases
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All 17 environment loader tests pass with 100% coverage of validation scenarios
- No secrets committed to repository (env.template verified to contain only placeholder values)
- Clear error messages tested for all missing/invalid variable scenarios
- Complete documentation verified with step-by-step API key acquisition for all services
- Service-specific getters tested to prevent cross-service API key usage errors
- Production deployment tested with system environment variables (no .env file)

## ADR-0003 — MVP Orchestration and Tooling Choices

<a id="adr-0003"></a>
**Date**: 2025-09-11
**Status**: Accepted
**Owner**: AI Agent

### Context

Autopiloot v1 targets an MVP for LinkedIn-first content creators (6-figure revenue entrepreneurs) requiring automated YouTube content processing pipeline. Core requirements: daily video discovery from @AlexHormozi, high-quality transcription with cost controls ($5/day budget), actionable coaching-style summaries, and internal operational alerting. Infrastructure must minimize operational complexity while ensuring reliability, cost visibility, and audit trails for business intelligence.

### Alternatives

- **GitHub Actions for scheduling**: External dependency outside Google Cloud ecosystem, timezone/DST handling complexity, less direct integration with Firestore triggers, additional authentication overhead
- **Direct video scraping vs YouTube Data API**: Higher brittleness, maintenance burden, rate limiting challenges, risk of breaking with platform changes, potential ToS violations
- **Custom vector store vs Zep**: Significant development overhead, time-to-market delay, maintenance burden, feature parity challenges, no GraphRAG capabilities
- **Open client ingestion vs controlled workflow**: Increased security surface area, data quality concerns, audit complexity, inappropriate for MVP scope
- **Custom transcription vs AssemblyAI**: Development complexity, quality concerns, no speaker diarization, cost optimization challenges

### Decision

Implement minimal-complexity cloud-native architecture optimized for reliability and observability:

**Scheduling & Orchestration:**

- Firebase Functions v2 scheduled functions with Cloud Scheduler at 01:00 Europe/Amsterdam (CET/CEST with automatic DST handling)
- Event-driven budget alerts triggered by Firestore document writes to `transcripts/{video_id}`
- Dead-letter queue pattern for failed operations with exponential backoff

**Data Architecture:**

- Firestore as primary event broker and system of record with collections: `videos`, `transcripts`, `summaries`, `jobs/transcription`, `costs_daily`, `audit_logs`, `jobs_deadletter`
- Server-only security rules (no client access) using Firebase Admin SDK for all data operations
- Audit logging for all critical operations with structured metadata

**External Services Integration:**

- YouTube Data API v3 for discovery with `lastPublishedAt` checkpoint persistence and quota exhaustion fallback/resume
- AssemblyAI for transcription with strict 70-minute video duration cap (4200 seconds)
- OpenAI GPT-4.1 (temperature 0.2, ~1500 token output) for coaching-style short summaries with `prompt_version: v1` tracking
- Zep collection `autopiloot_guidelines` for GraphRAG storage with explicit metadata linkage to source transcripts
- Slack API for internal alerts to `#ops-autopiloot` channel with 1 alert/type/hour throttling

### Consequences

- **Pros**: Minimal operational complexity and maintenance overhead, clear reliability measures with built-in monitoring, transparent cost visibility with automated budget controls, reproducible summary generation with version tracking, seamless timezone handling, strong audit trail for business intelligence
- **Cons / risks**: API quota limits may delay processing during high-volume periods, external service dependencies (AssemblyAI, Zep) create failure points, no end-user interface limits MVP feedback, vendor lock-in to Google Cloud ecosystem, limited to single channel initially
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Scheduled function execution logged daily with success/failure metrics and runtime duration tracking
- Budget alert system verified to trigger at 80% of daily $5 transcription limit using `costs_daily/{YYYY-MM-DD}` aggregation
- Summary generation verified to include `prompt_version: v1` metadata and bidirectional Zep document linkage
- Dead-letter queue population monitored for repeated failures with automatic escalation
- Firestore indexes configured for efficient querying of video status progression and cost aggregation
- Audit logs created for all key operations: video discovery, transcription submission, summary generation, cost threshold breaches

## New ADR Entry Template (copy for each new decision)

> Replace placeholders, keep section headers. Keep prose concise.

```

## ADR-XXXX — \<Short, specific title>

<a id="adr-XXXX"></a>
**Date**: YYYY-MM-DD
**Status**: Proposed | Accepted | Superseded
**Owner**: <Name>

### Context

<1–3 sentences: what changed or what forces drive this decision now>

### Alternatives

<Quick bullet list of alternatives considered, and why they were rejected.>

### Decision

\<Single clear decision in active voice; make it testable/verifiable>

### Consequences

* **Pros**: \<benefit 1>, \<benefit 2>
* **Cons / risks**: \<cost 1>, \<risk 1>
* **Supersedes**: ADR-NNNN (if any)
* **Superseded by**: ADR-MMMM (filled later if replaced)

### (Optional) Compliance / Verification

\<How we’ll check this is honored: tests, checks, fitness functions, runbooks>

```

---

## ADR-0002 — Event-Driven Broker Architecture with Firestore

<a id="adr-0002"></a>
**Date**: 2025-01-27  
**Status**: Accepted  
**Owner**: AI Agent

### Context

Modern full-stack applications require consistent data state between frontend and backend components. Traditional request-response patterns create tight coupling and require complex state synchronization logic. Real-time applications need immediate UI updates when data changes, regardless of the source of the change.

### Alternatives

- **Direct API communication**: Backend returns data directly to frontend, requires manual state management
- **Event streaming with external broker**: Use services like Redis Pub/Sub or RabbitMQ, adds infrastructure complexity
- **WebSocket connections**: Real-time but requires connection management and doesn't persist data
- **Firestore as event-driven broker**: Leverages built-in real-time capabilities and acts as single source of truth

### Decision

Implement event-driven broker architecture where Firestore serves as both the data store and event broker:

- All data mutations flow through Firestore exclusively
- Backend functions save data to Firestore without returning responses to frontend
- Frontend subscribes to Firestore collections/documents using hooks for real-time updates
- Firestore acts as the single source of truth for both backend and frontend
- UI updates automatically through Firestore real-time listeners

### Consequences

- **Pros**: Eliminates data synchronization issues, automatic real-time updates, reduced coupling between frontend and backend, simplified state management, leverages Firebase's built-in capabilities
- **Cons / risks**: Increased Firestore read operations, requires proper security rules design, potential data consistency challenges with complex operations, network dependency for all data access
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

Backend functions must only perform Firestore writes without returning data responses. Frontend components must use Firestore hooks (useFirestore, real-time listeners) for all data access. No direct API data responses to frontend. All data mutations trigger UI updates through Firestore change events.

---

## ADR-0001 — Agent-Focused Repository Structure

<a id="adr-0001"></a>
**Date**: 2025-01-27
**Status**: Accepted
**Owner**: AI Agent

### Context

The repository has evolved from a full-stack Firebase template to focus specifically on AI agent development and management. The original frontend and backend directories are no longer present, and the repository now contains agent-specific configuration, documentation, and task templates.

### Alternatives

- **Maintain full-stack structure**: Keep frontend and backend directories even if unused, adds complexity
- **Create separate agent repository**: Move agent files to a new repository, loses Firebase configuration context
- **Hybrid structure**: Keep both full-stack and agent components, creates confusion about primary purpose
- **Agent-focused structure**: Simplify to focus on agent development with Firebase as supporting infrastructure

### Decision

Implement agent-focused repository structure:

- `/agents/` - Primary directory containing all agent-related files and configuration
- Firebase configuration files (`firebase.json`, `firestore.rules`, `storage.rules`) at the agents level
- Agent documentation and rules in `.cursor/rules/` subdirectory
- Task templates in `tasks/` subdirectory
- Remove references to frontend and backend directories that no longer exist

### Consequences

- **Pros**: Clear focus on agent development, simplified structure, Firebase configuration remains available for agent backend needs, easier maintenance
- **Cons / risks**: Loss of full-stack template capabilities, may need to recreate frontend/backend if needed later, Firebase configuration references non-existent backend directory
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

Repository structure contains only agent-related files. Firebase configuration is updated to reflect actual directory structure. No references to non-existent frontend or backend directories in documentation.

---

## ADR-0005 — Reliability and Quota Management System

<a id="adr-0005"></a>
**Date**: 2025-09-14
**Status**: Accepted
**Owner**: AI Agent

### Context

The Autopiloot system integrates with multiple external APIs (YouTube Data API, AssemblyAI, OpenAI, Slack) that have quota limits and can experience transient failures. Without proper reliability mechanisms, API quota exhaustion could crash the system, failed operations would be lost, and there was no systematic way to resume processing from checkpoints. The system needed enterprise-grade reliability features including dead letter queues, exponential backoff, quota management, and configurable retry behavior to ensure resilient operation under API constraints.

### Alternatives

- **Simple retry loops**: Basic retry without backoff, prone to overwhelming failing services, no quota awareness, lost failed operations
- **External queue systems**: Use Redis/RabbitMQ for DLQ, adds infrastructure complexity, additional operational overhead, cost for MVP
- **Hard-coded retry limits**: No configuration flexibility, requires code changes for tuning, inconsistent behavior across services
- **Manual failure handling**: Operational burden, no systematic recovery, human error prone, doesn't scale
- **No checkpoint system**: Repeated API calls from beginning, quota waste, longer recovery times, inefficient processing

### Decision

Implement comprehensive reliability and quota management system with configuration-driven behavior:

**Dead Letter Queue (DLQ) System:**

- Structured DLQ entries in Firestore `jobs_deadletter` collection with job_type, video_id, reason, retry_count, timestamps
- Automatic DLQ routing after maximum retry attempts exceeded
- DLQ monitoring and query tools for operational insights and failure analysis
- Support for multiple job types: youtube_discovery, transcription, summarization, sheets_processing, slack_notification

**Quota Management:**

- `QuotaManager` class for tracking API usage across services (YouTube: 10,000 units/day, AssemblyAI: configurable)
- Automatic quota exhaustion detection with next reset time calculation
- Service availability checking before making API calls to prevent quota violations
- Graceful handling of quota exhaustion with resume capabilities

**Intelligent Retry Strategy:**

- Exponential backoff with configurable base delay (default: 60 seconds)
- Configurable maximum retry attempts (default: 3) before DLQ routing
- Backoff progression: base_delay × 2^retry_count (60s → 120s → 240s → 480s)
- Settings.yaml configuration support for retry behavior tuning

**Checkpoint System:**

- `lastPublishedAt` checkpoint for YouTube API to resume from last processed video
- Reduces API calls by avoiding reprocessing of already-handled content
- Firestore persistence for checkpoint data with automatic updates
- ISO 8601 timestamp handling for accurate temporal ordering

**Configuration Integration:**

- YAML-based configuration in `settings.yaml` under `reliability` section
- TypedDict validation for configuration structure and value constraints
- Helper functions for accessing configuration values with sensible defaults
- Real-time configuration loading without code deployment

### Consequences

- **Pros**: Enterprise-grade reliability prevents data loss, configurable retry behavior allows fine-tuning without code changes, quota management prevents API violations and system crashes, checkpoint system reduces API usage and speeds recovery, comprehensive monitoring enables proactive operational management, Firestore-based architecture leverages existing infrastructure, extensive test coverage ensures reliable operation
- **Cons / risks**: Increased system complexity with additional moving parts, Firestore storage costs for DLQ and checkpoint data, potential for configuration errors affecting retry behavior, dependency on external API reset schedules, monitoring overhead for DLQ and quota status
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- 22 comprehensive reliability tests with 100% pass rate covering DLQ operations, retry logic, quota management, and checkpoint functionality
- Configuration validation tests ensure invalid values are rejected with clear error messages
- Dead letter queue entries automatically created after max retry attempts with complete error context
- Quota exhaustion triggers graceful pausing with automatic resumption when quota resets
- Checkpoint system demonstrated to reduce YouTube API calls by resuming from `lastPublishedAt` timestamp
- Settings.yaml integration verified with different retry values affecting actual backoff calculations
- Firestore composite indexes defined for efficient DLQ querying and operational monitoring
- Tools provided for DLQ analysis, quota monitoring, and checkpoint management

---

## ADR-0006 — Modular Agent Tool Architecture

<a id="adr-0006"></a>
**Date**: 2025-09-14
**Status**: Accepted
**Owner**: AI Agent

### Context

The Autopiloot system requires 20 distinct tools across 4 specialized agents (Scraper, Transcriber, Summarizer, Assistant) to orchestrate the YouTube content processing pipeline. Each agent needs domain-specific capabilities while maintaining consistent patterns for error handling, environment configuration, and testing. Without proper tool architecture, code duplication would proliferate, maintenance complexity would increase, and integration with agency_swarm or other orchestration frameworks would be challenging.

### Alternatives

- **Monolithic agent classes**: Single class per agent with all tools as methods, reduces modularity, difficult testing, violates single responsibility principle
- **Function-based tools**: Simple functions without classes, no state management, inconsistent error handling, difficult to extend
- **External tool libraries**: Use existing libraries for each service, integration overhead, inconsistent interfaces, missing domain-specific logic
- **Copy-paste implementations**: Quick initial development, maintenance nightmare, no code reuse, error propagation across copies
- **Framework-specific tools**: Lock-in to specific orchestration framework (agency_swarm), reduced portability, harder standalone testing

### Decision

Implement modular tool architecture with inheritance-based consistency and service isolation:

**Core Architecture:**

- `BaseTool` abstract class providing environment validation, error handling, and standard interface
- One tool class per specific capability (20 total: 7 Scraper, 5 Transcriber, 4 Summarizer, 4 Assistant)
- TypedDict-based request/response contracts for type safety and documentation
- Service-specific tool directories: `{agent}/tools/*.py` with __init__.py for clean imports
- Consistent `run()` method interface accepting Dict[str, Any] and returning Dict[str, Any]

**Tool Distribution:**

- **Scraper (7)**: ResolveChannelHandle, ListRecentUploads, ReadSheetLinks, ExtractYouTubeFromPage, SaveVideoMetadata, EnqueueTranscription, RemoveSheetRow
- **Transcriber (5)**: GetVideoAudioUrl, SubmitAssemblyAIJob, PollTranscriptionJob, StoreTranscriptToDrive, SaveTranscriptRecord
- **Summarizer (4)**: GenerateShortSummary, StoreShortInZep, StoreShortSummaryToDrive, SaveSummaryRecord
- **Assistant (4)**: FormatSlackBlocks, SendSlackMessage, MonitorTranscriptionBudget, SendErrorAlert

**Environment Integration:**

- All secrets via environment variables (no hardcoded values)
- `_validate_env_vars()` method enforces required configuration at initialization
- `get_env_var()` helper with required/optional support and clear error messages
- Service account path validation for Google Cloud services

**Testing Strategy:**

- Every tool includes `if __name__ == "__main__"` test block for standalone execution
- Mock-friendly design with dependency injection capability
- Test requests demonstrate expected input/output formats
- Error scenarios validated with appropriate exception handling

### Consequences

- **Pros**: Clean separation of concerns with single responsibility per tool, consistent error handling and validation across all tools, easy integration with multiple orchestration frameworks, standalone testability accelerates development, environment variable enforcement prevents production failures, modular architecture enables selective tool usage, type hints improve IDE support and documentation
- **Cons / risks**: More files to manage (20 tools + base class + init files), inheritance coupling to BaseTool class, potential for interface drift without strict governance, initial setup complexity for new developers, dependency on external service SDKs
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All 20 tools implemented with BaseTool inheritance and consistent run() interface
- Each tool validates required environment variables at initialization with clear error messages
- Test blocks verified to execute without errors when valid mock data provided
- No hardcoded secrets found in any tool implementation (verified by code review)
- Import structure validated: `from autopiloot.{agent}.tools import {ToolClass}`
- Service-specific SDK usage: googleapiclient for Google services, assemblyai for transcription, slack_sdk for notifications
- Firestore integration uses google-cloud-firestore with proper project/credential configuration
- Error handling demonstrates graceful degradation with actionable error messages

---

## ADR-0007: Firebase Functions v2 Scheduling Architecture

- **Date**: 2024-09-14
- **Status**: Accepted
- **Context**: Autopiloot requires scheduled execution of agent workflows (daily YouTube scraping) and event-driven processing (budget monitoring on transcript creation). Need cloud-native solution with reliable scheduling, automatic scaling, and integration with existing Firestore data layer.

### Decision

Implement Firebase Functions v2 with scheduled and event-driven execution patterns:

**Scheduled Functions:**
- Daily scraper execution at 01:00 Europe/Amsterdam using Cloud Scheduler cron triggers
- `@scheduler_fn.on_schedule(schedule="0 1 * * *", timezone="Europe/Amsterdam")`
- 9-minute timeout with 512MB memory allocation for agency processing
- Single instance constraint to prevent concurrent executions

**Event-Driven Functions:**
- Budget monitoring triggered by Firestore document writes to `transcripts/{video_id}`
- Real-time cost calculation with daily threshold alerting at 80% of $5 budget
- Slack notifications via Assistant agent tools integration
- Automatic audit logging to `audit_logs` and `costs_daily` collections

**Architecture Components:**
- `autopiloot/firebase/functions/scheduler.py`: Main functions implementation
- `autopiloot/firebase/functions/requirements.txt`: Python dependencies for Firebase runtime
- Manual deployment via Firebase CLI with service account authentication
- Centralized error handling with structured logging and alert propagation

**Integration Strategy:**
- Direct import of agency classes: `from agency import AutopilootAgency`
- Reuse existing environment configuration via `core.env_loader`
- Leverage Assistant tools for Slack formatting and messaging
- Firestore as event broker between scheduled and on-demand executions

### Alternatives Considered

**Google Cloud Run Jobs**: More flexible but requires custom scheduling logic and lacks native Firestore event integration. Higher operational complexity for simple cron scheduling.

**GitHub Actions**: Limited to repository events, not suitable for production data processing workflows. No direct Firestore integration or timezone handling.

**Local Cron + Self-Hosted**: Requires infrastructure management, no automatic scaling, single point of failure. No native cloud logging or monitoring.

**AWS Lambda + EventBridge**: Vendor lock-in to AWS ecosystem, would require migration of existing Firestore data layer. More complex cold start behavior.

### Consequences

- **Pros**: Native Google Cloud integration with existing Firestore infrastructure, automatic scaling and cold start optimization, built-in monitoring and logging via Cloud Functions, timezone-aware scheduling with daylight saving handling, event-driven architecture eliminates polling overhead, cost-effective pay-per-invocation model, Firebase CLI provides streamlined deployment workflow
- **Cons / risks**: Vendor lock-in to Google Cloud Platform, 9-minute execution timeout may constrain large channel processing, cold start latency for infrequent functions, requires manual Firebase project setup and service account configuration, debugging complexity in cloud environment, potential for cascading failures across function dependencies
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Firebase Functions v2 decorators implemented with correct scheduling syntax and timezone configuration
- Environment variables properly loaded via centralized `env_loader` with validation
- Budget calculations tested with mock Firestore data and verified against daily cost thresholds
- Slack alerting integration confirmed with Assistant agent tools in both emulator and production
- Audit logging captures all function executions with structured data for operational monitoring
- Manual deployment procedures documented with step-by-step Firebase CLI instructions
- Error handling includes graceful degradation and automatic retry logic for transient failures
- Function timeouts and memory limits configured appropriately for agency workflow requirements

---

## ADR-0008: Agency Swarm v1.0.0 Tool Implementation

<a id="adr-0008"></a>

- **Date**: 2024-09-14
- **Status**: Accepted
- **Context**: Autopiloot requires 20 production-ready tools across 4 agents (Scraper, Transcriber, Summarizer, Assistant) using Agency Swarm framework v1.0.0. Need consistent tool architecture, proper validation, environment configuration, and comprehensive testing for YouTube content processing automation.

### Decision

Implement all 20 required tools using Agency Swarm v1.0.0 BaseTool framework:

**Tool Distribution:**
- **Scraper (7 tools)**: ResolveChannelHandle, ListRecentUploads, ReadSheetLinks, ExtractYouTubeFromPage, SaveVideoMetadata, EnqueueTranscription, RemoveSheetRow
- **Transcriber (5 tools)**: GetVideoAudioUrl, SubmitAssemblyAIJob, PollTranscriptionJob, StoreTranscriptToDrive, SaveTranscriptRecord
- **Summarizer (4 tools)**: GenerateShortSummary, StoreShortInZep, StoreShortSummaryToDrive, SaveSummaryRecord
- **Assistant (4 tools)**: FormatSlackBlocks, SendSlackMessage, MonitorTranscriptionBudget, SendErrorAlert

**Implementation Standards:**
- All tools inherit from `agency_swarm.tools.BaseTool` with Pydantic field validation
- Environment variables only for secrets - no API keys as tool parameters
- Standardized `run()` method returning strings directly (not Dict objects)
- Comprehensive test blocks in every tool file with `if __name__ == "__main__":`
- Production-ready error handling with graceful failure modes

**API Integration:**
- YouTube Data API v3: google-api-python-client SDK
- AssemblyAI: Official assemblyai SDK for transcription services
- Google Drive/Sheets: google-api-python-client with service account auth
- Slack: slack_sdk for notification systems
- Firestore: firebase-admin SDK for data persistence
- OpenAI: openai SDK for LLM summarization

**Tool Architecture:**
- Centralized environment loading via `core/env_loader.py`
- Configuration management through `config/settings.yaml`
- Consistent error handling and logging patterns
- Type safety with Pydantic Field validation and TypedDict structures

### Alternatives Considered

**Custom BaseTool Implementation**: Initially considered custom tool base class but Agency Swarm v1.0.0 provides superior validation, documentation generation, and agent integration. Maintains framework consistency.

**Direct Function Tools**: Agency Swarm supports @function_tool decorator but BaseTool classes provide better structure, validation, and testing capabilities for complex production tools.

**Multiple Tool Directories**: Considered consolidating all tools in single directory but agent-specific tool folders improve organization and align with Agency Swarm best practices.

### Consequences

- **Pros**: Full Agency Swarm v1.0.0 compatibility ensures seamless agent integration, Pydantic validation prevents runtime errors and improves tool reliability, standardized architecture enables consistent development patterns across all agents, comprehensive testing validates production readiness, environment-based configuration supports multiple deployment environments, proper SDK usage reduces API integration complexity
- **Cons / risks**: Framework dependency creates coupling to Agency Swarm ecosystem, tool inheritance requires understanding of BaseTool internals, comprehensive validation may impact tool execution performance, extensive testing increases development time and maintenance overhead
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All 20 required tools implemented with agency_swarm.tools.BaseTool inheritance and consistent interfaces
- Pydantic Field validation enforced for all tool parameters with descriptive error messages
- Environment variable usage verified - no hardcoded secrets found in any tool implementation
- Test blocks confirmed functional in all 32 tool files with realistic test scenarios
- API SDK integration validated: YouTube Data API, AssemblyAI, Google APIs, Slack, Firebase, OpenAI
- Tool directory structure follows Agency Swarm conventions with proper __init__.py imports
- Error handling demonstrates graceful degradation with actionable error messages
- Production deployment tested with proper service account authentication and quota management

---

## ADR-0009: YouTube Channel Handle Resolution Tool

<a id="adr-0009"></a>

- **Date**: 2024-09-14
- **Status**: Accepted
- **Context**: Autopiloot scraper agent requires batch resolution of YouTube channel handles (e.g., @AlexHormozi) to canonical channel IDs for subsequent API operations. Need configuration-driven solution that loads handles from settings.yaml, implements retry logic for rate limits, and provides comprehensive error handling for production reliability.

### Decision

Implement ResolveChannelHandles tool using Agency Swarm v1.0.0 BaseTool framework:

**Core Functionality:**
- Batch resolution of multiple YouTube channel handles from configuration
- Loads handles from `settings.yaml` under `scraper.handles` array
- Returns mapping as JSON string: `{"@AlexHormozi": "UCfV36TX5AejfAGIbtwTc7Zw", ...}`
- Continues processing remaining handles even when individual handles fail

**YouTube API Integration:**
- Primary resolution via YouTube Data API v3 Search endpoint with exact handle matching
- Fallback to legacy Channels API with forUsername parameter for older channels
- Dual authentication: Service account credentials (preferred) + API key fallback
- Smart handle verification through custom URL checking for accurate matches

**Reliability Features:**
- Exponential backoff retry logic: 1s, 2s, 4s delays for rate limit (429) and server errors (5xx)
- Configurable max_retries parameter with Pydantic field validation (default: 3)
- Graceful error handling that preserves partial results and continues processing
- Handle normalization ensuring all handles start with @ symbol for consistency

**Configuration Integration:**
- Centralized environment loading via `core/env_loader.py` for API key management
- Configuration loading via `config/loader.py` with `get_config_value("scraper.handles")`
- Default fallback to `["@AlexHormozi"]` if configuration missing or empty
- Integration with existing settings.yaml structure and validation patterns

### Alternatives Considered

**Individual Handle Resolution Tool**: Could create separate tool for single handle resolution but batch processing reduces API calls and provides better error resilience for production workflows.

**Direct API Integration**: Could embed YouTube API calls in other tools but centralized handle resolution enables caching, rate limit management, and reusability across multiple scraper operations.

**Synchronous vs Async Processing**: Considered async implementation but synchronous approach provides better error tracking and simpler retry logic for the current use case with small handle counts.

### Consequences

- **Pros**: Batch processing reduces YouTube API quota usage through efficient single-tool execution, configuration-driven approach enables easy channel management without code changes, comprehensive retry logic ensures reliable operation under API rate limits, error isolation prevents single handle failures from blocking entire operation, JSON output format enables easy integration with other tools and logging systems, proper Agency Swarm integration provides consistent validation and error handling patterns
- **Cons / risks**: YouTube API dependency creates external service coupling with quota limitations, handle resolution accuracy depends on YouTube's search algorithm and custom URL availability, batch processing may timeout with very large handle lists requiring chunking strategies, configuration changes require deployment updates rather than runtime modifications
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Agency Swarm v1.0.0 BaseTool inheritance with proper Pydantic field validation implemented
- Configuration loading from settings.yaml verified with scraper.handles array structure
- YouTube Data API v3 integration tested with both Search and Channels endpoints
- Retry logic verified with exponential backoff timing and proper HTTP error code handling
- Error handling tested for missing handles, API failures, configuration errors, and partial failures
- JSON output format validated against TypedDict specification with handle->channel_id mapping
- Comprehensive test suite created with 15+ unit tests covering all error scenarios and edge cases
- Production authentication tested with service account credentials and API key fallback
- Handle normalization verified to ensure consistent @ prefix formatting across all inputs

---

## ADR-0010: YouTube Uploads Playlist Discovery Tool

<a id="adr-0010"></a>

- **Date**: 2024-09-14
- **Status**: Accepted
- **Context**: Autopiloot scraper agent requires efficient video discovery from YouTube channels within specific time windows. Need solution that uses uploads playlist for optimal API quota usage, implements checkpoint-based processing to skip already-processed videos, and includes comprehensive quota management for production reliability.

### Decision

Implement ListRecentUploads tool using uploads playlist API with checkpoint persistence:

**Core Functionality:**
- Channel uploads playlist discovery via YouTube Data API v3 channels endpoint
- Video fetching through playlistItems.list API for efficient playlist-based discovery
- Batch video details retrieval via videos.list API for durations and metadata
- Time window filtering between since_utc and until_utc parameters with early exit optimization

**Checkpoint Management:**
- LastPublishedAt timestamp persistence in Firestore checkpoints collection
- Incremental processing that skips videos processed in previous runs
- Configurable checkpoint usage via use_checkpoint boolean parameter
- Graceful fallback when Firestore unavailable without breaking functionality

**Quota Management:**
- Integration with QuotaManager for YouTube API availability checking
- Request tracking and quota exhaustion detection with proper error handling
- Batch processing of video details in 50-video chunks to optimize API usage
- Early termination when quota exhausted with structured error responses

**Production Features:**
- ISO 8601 duration parsing (PT1H30M45S format) with regex pattern matching
- Dual authentication: service account credentials preferred, API key fallback
- JSON output format with items array, total counts, and checkpoint status
- Error resilience that continues processing despite individual video failures

### Alternatives Considered

**Search API Approach**: YouTube search API provides video discovery but consumes more quota units and lacks the efficiency of playlist-based retrieval. Uploads playlist provides chronological ordering ideal for checkpoint-based processing.

**Direct Channel Videos**: Could fetch all channel videos but uploads playlist specifically contains only uploads, filtering out playlists, live streams, and other content types that may not be relevant for transcription workflows.

**Synchronous vs Async Processing**: Considered async implementation but synchronous approach provides better error tracking and simpler checkpoint management for the current batch sizes and quota constraints.

### Consequences

- **Pros**: Uploads playlist provides most efficient API quota usage for video discovery, checkpoint-based processing eliminates duplicate work and enables incremental updates, batch video details fetching optimizes API calls while respecting rate limits, time window filtering ensures only relevant videos are processed, QuotaManager integration prevents quota exhaustion and provides graceful degradation, Firestore persistence enables stateful processing across multiple runs
- **Cons / risks**: Uploads playlist dependency means tool cannot discover videos from other playlist types, checkpoint persistence requires Firestore availability for optimal operation, batch processing may still hit rate limits with very active channels, time window precision depends on YouTube's publishedAt accuracy, quota exhaustion can interrupt processing requiring manual intervention or retry logic
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- YouTube Data API v3 integration verified with uploads playlist discovery and video details fetching
- Checkpoint persistence implemented with Firestore document structure and graceful fallback handling
- Quota management tested with availability checking and request tracking integration
- Time window filtering validated with ISO 8601 timestamp parsing and comparison logic
- Batch processing confirmed with 50-video chunks and proper API parameter handling
- Duration parsing tested with regex pattern for PT1H30M45S format conversion to seconds
- Error handling verified for rate limits, quota exhaustion, missing playlists, and invalid data
- Agency Swarm BaseTool integration with proper Pydantic field validation and string return type
- Comprehensive test suite with 15+ unit tests covering all functionality and error scenarios

---

## ADR-0011 — Google Sheets and Web Page YouTube Extraction Tools

<a id="adr-0011"></a>
**Date**: 2025-09-14
**Status**: Accepted
**Owner**: AI Agent

### Context

Autopiloot scraper agent requires two complementary tools for YouTube URL discovery: ReadSheetLinksSimple for extracting YouTube links from single-column Google Sheets (backfill scenarios), and ExtractYouTubeFromPage for discovering YouTube videos embedded in web pages. These tools support different content ingestion workflows - direct URL lists from curated sheets and dynamic discovery from blog posts, social media, or content aggregators.

### Alternatives

- **Combined single tool**: Single tool handling both sheets and web pages but would violate single responsibility principle and complicate parameter validation
- **Raw HTML parsing only**: Skip BeautifulSoup for faster parsing but would miss structured content in iframes, meta tags, and complex JavaScript scenarios
- **Google Sheets API v4 batch operations**: More complex batch reading but current single-range approach sufficient for MVP requirements
- **External URL extraction services**: Third-party APIs for web scraping but adds dependency, cost, and rate limiting complexity
- **Selenium for JavaScript rendering**: Handle dynamic content but significant overhead and complexity for static URL extraction

### Decision

Implement two specialized tools using Agency Swarm v1.0.0 BaseTool framework:

**ReadSheetLinksSimple Tool:**

- Single-column Google Sheet processing with configurable A1 notation range (default: "Sheet1!A:A")
- YouTube URL validation using comprehensive regex patterns for all YouTube URL formats
- URL normalization to standard `https://www.youtube.com/watch?v=` format
- Automatic deduplication with set-based unique URL tracking
- Max rows limiting with optional parameter for large sheet handling
- JSON response format with items array and processing summary statistics

**ExtractYouTubeFromPage Tool:**

- Multi-source YouTube URL extraction: raw HTML text, iframe embeds, anchor links, meta og:video tags, JavaScript content
- BeautifulSoup HTML parsing with proper error handling for malformed content
- Comprehensive YouTube URL pattern recognition including youtu.be, youtube.com/embed, youtube.com/v formats
- HTTP client with proper User-Agent headers to avoid blocking
- Cross-source deduplication ensuring URLs found in multiple places appear only once
- Structured JSON response with videos array and extraction summary

**Shared Implementation Standards:**

- Google Sheets API integration using service account authentication via `core/env_loader.py`
- Configuration loading from `settings.yaml` with graceful fallback for missing sheet IDs
- Comprehensive error handling with JSON error responses maintaining consistent interface
- Pydantic field validation for all tool parameters with descriptive help text
- Production-ready test blocks with realistic scenarios and error condition coverage

### Consequences

- **Pros**: Specialized tools follow single responsibility principle enabling focused testing and maintenance, comprehensive URL pattern recognition ensures high extraction accuracy, deduplication prevents duplicate processing reducing downstream API costs, Google Sheets integration enables non-technical content curation workflows, BeautifulSoup parsing handles complex HTML structures reliably, Agency Swarm compliance ensures consistent tool interfaces across entire agent ecosystem, flexible configuration via settings.yaml supports multiple deployment environments
- **Cons / risks**: Two separate tools require coordination for workflows using both ingestion methods, Google Sheets API dependency creates external service coupling with quota constraints, web scraping may fail on sites with bot protection or complex JavaScript rendering, HTTP timeout issues with slow-loading pages could interrupt processing, regex pattern maintenance required as YouTube introduces new URL formats
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Agency Swarm v1.0.0 BaseTool inheritance implemented with proper Pydantic field validation and string return types
- Google Sheets API v4 integration verified with service account authentication and configurable range selection
- YouTube URL validation tested against 8+ different URL formats including edge cases and malformed inputs
- Web page extraction validated across multiple HTML structure types: iframes, links, meta tags, JavaScript, raw text
- Deduplication algorithms tested to ensure identical videos in different URL formats are merged correctly
- Error handling verified for network failures, API errors, malformed HTML, missing configuration, and quota exhaustion
- Comprehensive test suites created with 12+ unit tests per tool covering functionality and error scenarios
- JSON response format standardized with items arrays, summary statistics, and error handling consistency
- Configuration integration tested with settings.yaml loading and graceful fallback behavior

---
