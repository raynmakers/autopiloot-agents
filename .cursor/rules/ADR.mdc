---
description: Architectural Decision Records
globs:
alwaysApply: false
---

# Architecture Decision Log

<!--
ADR_AGENT_PROTOCOL v1.0

You (the agent) manage this file as the single source of truth for all ADRs.

INVARIANTS
- Keep this exact file structure and headings.
- All ADR entries use H2 headings: "## ADR-XXXX — <Title>" (4-digit zero-padded ID).
- Allowed Status values: Proposed | Accepted | Superseded
- Date format: YYYY-MM-DD
- New entries must be appended to the END of the file.
- The Index table between the INDEX markers must always reflect the latest state and be sorted by ID desc (newest on top).
- Each ADR MUST contain: Date, Status, Owner, Context, Decision, Consequences.
- Each ADR must include an explicit anchor `<a id="adr-XXXX"></a>` so links remain stable.

HOW TO ADD A NEW ADR
1) Read the whole file.
2) Compute next ID:
   - Scan for headings matching: ^## ADR-(\d{4}) — .+$
   - next_id = (max captured number) + 1, left-pad to 4 digits.
3) Create a new ADR section using the “New ADR Entry Template” below.
   - Place it AFTER the last ADR section in the file.
   - Add an `<a id="adr-XXXX"></a>` line immediately below the heading.
4) Update the Index (between the INDEX markers):
   - Insert/replace the row for this ADR keeping the table sorted by ID descending.
   - Title in the Index MUST link to the anchor: [<Title>](#adr-XXXX)
   - If this ADR supersedes another: set “Supersedes” in this row, and update that older ADR:
       a) Change its Status to “Superseded”
       b) Add “Superseded by: ADR-XXXX” in its Consequences block
       c) Update the older ADR’s Index row “Superseded by” column to ADR-XXXX
5) Validate before saving:
   - Exactly one heading exists for ADR-XXXX
   - All required fields are present and non-empty
   - Index contains a row for ADR-XXXX and remains properly sorted
6) Concurrency resolution:
   - If a merge conflict or duplicate ID is detected after reading: recompute next_id from the current file state, rename your heading, anchor, and Index row accordingly, and retry once.

COMMIT MESSAGE SUGGESTION
- "ADR-XXXX: <Short Title> — <Status>"

END ADR_AGENT_PROTOCOL
-->

## Index

<!-- BEGIN:ADR_INDEX -->

| ID   | Title                                                        | Date       | Status   | Supersedes | Superseded by |
| ---- | ------------------------------------------------------------ | ---------- | -------- | ---------- | ------------- |
| 0036 | [Comprehensive Testing Framework and Dependency Management](#adr-0036) | 2025-09-19 | Accepted | —          | —             |
| 0035 | [Google Drive Agent Implementation with Zep GraphRAG Integration](#adr-0035) | 2025-09-19 | Accepted | —          | —             |
| 0034 | [LinkedIn Agent Implementation with Scheduled Ingestion](#adr-0034) | 2025-09-18 | Accepted | —          | —             |
| 0033 | [Daily Digest Operational Documentation Standardization](#adr-0033) | 2025-09-17 | Accepted | —          | —             |
| 0032 | [Edge Case Testing Framework Enhancement](#adr-0032)        | 2025-09-17 | Accepted | —          | —             |
| 0031 | [Firebase Functions Configuration Normalization](#adr-0031)  | 2025-09-17 | Accepted | —          | —             |
| 0030 | [Tool Filename Standardization to Snake Case](#adr-0030)    | 2025-09-16 | Accepted | —          | —             |
| 0029 | [Security Architecture and Service Account Management](#adr-0029) | 2025-09-15 | Accepted | —          | —             |
| 0028 | [Documentation Standardization and Roadmap Implementation](#adr-0028) | 2025-09-15 | Accepted | —          | —             |
| 0027 | [Comprehensive Testing Infrastructure and CI/CD Implementation](#adr-0027) | 2025-09-15 | Accepted | —          | —             |
| 0026 | [Centralized Utilities and Code Quality Architecture](#adr-0026) | 2025-09-15 | Accepted | —          | —             |
| 0025 | [Observability Alerts and Testing Framework Implementation](#adr-0025) | 2025-09-15 | Accepted | —          | —             |
| 0024 | [Orchestrator Agent Architecture and Event Contracts](#adr-0024) | 2025-09-15 | Accepted | —          | —             |
| 0023 | [Comprehensive Observability Operations Suite Implementation](#adr-0023) | 2025-09-15 | Accepted | —          | —             |
| 0022 | [Comprehensive Audit Logging System for Security Compliance](#adr-0022) | 2025-09-15 | Accepted | —          | —             |
| 0021 | [Assistant Agent Alerting System Implementation](#adr-0021) | 2025-09-15 | Accepted | —          | —             |
| 0020 | [LLM Observability and Configuration Enhancement](#adr-0020) | 2025-09-15 | Accepted | —          | —             |
| 0019 | [Enhanced Zep GraphRAG Integration with Workflow Orchestration](#adr-0019) | 2025-09-15 | Accepted | —          | —             |
| 0018 | [Multi-Platform Summary Storage and Reference Linking](#adr-0018) | 2025-09-15 | Accepted | —          | —             |
| 0017 | [Coaching-Focused Summary Generation with LLM Integration](#adr-0017) | 2025-09-15 | Accepted | —          | —             |
| 0016 | [AssemblyAI Transcript Processing Pipeline](#adr-0016)      | 2025-09-15 | Accepted | —          | —             |
| 0015 | [Agency Swarm v1.0.0 Framework Restructuring](#adr-0015)    | 2025-09-15 | Accepted | —          | —             |
| 0014 | [Google Sheets Row Management and Archival Tool](#adr-0014) | 2025-09-14 | Accepted | —          | —             |
| 0013 | [Transcription Job Queue Management Tool](#adr-0013)        | 2025-09-14 | Accepted | —          | —             |
| 0012 | [Firestore Video Metadata Storage Tool](#adr-0012)          | 2025-09-14 | Accepted | —          | —             |
| 0011 | [Google Sheets and Web Page YouTube Extraction Tools](#adr-0011) | 2025-09-14 | Accepted | —          | —             |
| 0010 | [YouTube Uploads Playlist Discovery Tool](#adr-0010)         | 2025-09-14 | Accepted | —          | —             |
| 0009 | [YouTube Channel Handle Resolution Tool](#adr-0009)          | 2025-09-14 | Accepted | —          | —             |
| 0008 | [Agency Swarm v1.0.0 Tool Implementation](#adr-0008)        | 2025-09-14 | Accepted | —          | —             |
| 0007 | [Firebase Functions v2 Scheduling Architecture](#adr-0007)   | 2025-09-14 | Accepted | —          | —             |
| 0006 | [Modular Agent Tool Architecture](#adr-0006)                | 2025-09-14 | Accepted | —          | —             |
| 0005 | [Reliability and Quota Management System](#adr-0005)         | 2025-09-14 | Accepted | —          | —             |
| 0004 | [Comprehensive Environment Configuration System](#adr-0004)  | 2025-01-15 | Accepted | —          | —             |
| 0003 | [MVP Orchestration and Tooling Choices](#adr-0003)           | 2025-09-11 | Accepted | —          | —             |
| 0002 | [Event-Driven Broker Architecture with Firestore](#adr-0002) | 2025-01-27 | Accepted | —          | —             |
| 0001 | [Agent-Focused Repository Structure](#adr-0001)              | 2025-01-27 | Accepted | —          | —             |

<!-- END:ADR_INDEX -->

---

## ADR-0004 — Comprehensive Environment Configuration System

<a id="adr-0004"></a>
**Date**: 2025-01-15
**Status**: Accepted
**Owner**: AI Agent

### Context

The Autopiloot system integrates with 8+ external services requiring API credentials: OpenAI (GPT-4.1), AssemblyAI (transcription), YouTube Data API (discovery), Slack (notifications), Google Drive & Sheets (storage), Zep (GraphRAG), and optional Langfuse (observability). Manual environment setup across development and production environments is error-prone, lacks validation, and poses security risks. Previous approaches with simple .env files led to deployment failures due to missing or incorrectly formatted variables.

### Alternatives

- **Simple .env files only**: No validation, unclear required vs optional variables, prone to deployment errors with cryptic failure messages
- **Hard-coded configuration**: Major security risk, no environment flexibility, violates 12-factor app principles
- **Complex configuration frameworks** (e.g., Hydra, OmegaConf): Overkill for MVP scope, unnecessary dependencies, learning curve overhead
- **Environment-specific config files**: Risk of committing secrets, configuration drift between environments, maintenance burden
- **Cloud provider secret management**: Platform lock-in, additional complexity for development environment, cost for MVP

### Decision

Implement comprehensive environment configuration system with structured validation and developer experience focus:

**Core Components:**

- `env.template` - Complete variable reference without secrets, serves as documentation
- `config/env_loader.py` - Validation module with type checking, clear error messages, and service-specific getters
- Production-ready error handling with graceful fallback to system environment variables
- Complete test coverage (17 test cases) covering validation scenarios, edge cases, and error conditions
- `ENVIRONMENT.md` - Step-by-step setup guide with API key acquisition instructions and troubleshooting

**Key Features:**

- Service-specific API key access (`get_api_key('openai')`, `get_api_key('slack')`, etc.)
- Required vs optional variable distinction with sensible defaults
- File existence validation for Google service account credentials
- Clear separation between development (.env) and production (system vars) configuration
- Comprehensive error messages guiding developers to resolution

### Consequences

- **Pros**: Eliminates deployment configuration errors, provides immediate validation feedback, ensures secure credential handling, comprehensive documentation reduces onboarding time, production-ready error handling, 100% test coverage prevents regressions, service-specific getters prevent API key mix-ups
- **Cons / risks**: Additional complexity over simple .env approach, requires initial setup documentation maintenance, dependency on python-dotenv package, potential over-engineering for simple use cases
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All 17 environment loader tests pass with 100% coverage of validation scenarios
- No secrets committed to repository (env.template verified to contain only placeholder values)
- Clear error messages tested for all missing/invalid variable scenarios
- Complete documentation verified with step-by-step API key acquisition for all services
- Service-specific getters tested to prevent cross-service API key usage errors
- Production deployment tested with system environment variables (no .env file)

## ADR-0003 — MVP Orchestration and Tooling Choices

<a id="adr-0003"></a>
**Date**: 2025-09-11
**Status**: Accepted
**Owner**: AI Agent

### Context

Autopiloot v1 targets an MVP for LinkedIn-first content creators (6-figure revenue entrepreneurs) requiring automated YouTube content processing pipeline. Core requirements: daily video discovery from @AlexHormozi, high-quality transcription with cost controls ($5/day budget), actionable coaching-style summaries, and internal operational alerting. Infrastructure must minimize operational complexity while ensuring reliability, cost visibility, and audit trails for business intelligence.

### Alternatives

- **GitHub Actions for scheduling**: External dependency outside Google Cloud ecosystem, timezone/DST handling complexity, less direct integration with Firestore triggers, additional authentication overhead
- **Direct video scraping vs YouTube Data API**: Higher brittleness, maintenance burden, rate limiting challenges, risk of breaking with platform changes, potential ToS violations
- **Custom vector store vs Zep**: Significant development overhead, time-to-market delay, maintenance burden, feature parity challenges, no GraphRAG capabilities
- **Open client ingestion vs controlled workflow**: Increased security surface area, data quality concerns, audit complexity, inappropriate for MVP scope
- **Custom transcription vs AssemblyAI**: Development complexity, quality concerns, no speaker diarization, cost optimization challenges

### Decision

Implement minimal-complexity cloud-native architecture optimized for reliability and observability:

**Scheduling & Orchestration:**

- Firebase Functions v2 scheduled functions with Cloud Scheduler at 01:00 Europe/Amsterdam (CET/CEST with automatic DST handling)
- Event-driven budget alerts triggered by Firestore document writes to `transcripts/{video_id}`
- Dead-letter queue pattern for failed operations with exponential backoff

**Data Architecture:**

- Firestore as primary event broker and system of record with collections: `videos`, `transcripts`, `summaries`, `jobs/transcription`, `costs_daily`, `audit_logs`, `jobs_deadletter`
- Server-only security rules (no client access) using Firebase Admin SDK for all data operations
- Audit logging for all critical operations with structured metadata

**External Services Integration:**

- YouTube Data API v3 for discovery with `lastPublishedAt` checkpoint persistence and quota exhaustion fallback/resume
- AssemblyAI for transcription with strict 70-minute video duration cap (4200 seconds)
- OpenAI GPT-4.1 (temperature 0.2, ~1500 token output) for coaching-style short summaries with `prompt_version: v1` tracking
- Zep collection `autopiloot_guidelines` for GraphRAG storage with explicit metadata linkage to source transcripts
- Slack API for internal alerts to `#ops-autopiloot` channel with 1 alert/type/hour throttling

### Consequences

- **Pros**: Minimal operational complexity and maintenance overhead, clear reliability measures with built-in monitoring, transparent cost visibility with automated budget controls, reproducible summary generation with version tracking, seamless timezone handling, strong audit trail for business intelligence
- **Cons / risks**: API quota limits may delay processing during high-volume periods, external service dependencies (AssemblyAI, Zep) create failure points, no end-user interface limits MVP feedback, vendor lock-in to Google Cloud ecosystem, limited to single channel initially
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Scheduled function execution logged daily with success/failure metrics and runtime duration tracking
- Budget alert system verified to trigger at 80% of daily $5 transcription limit using `costs_daily/{YYYY-MM-DD}` aggregation
- Summary generation verified to include `prompt_version: v1` metadata and bidirectional Zep document linkage
- Dead-letter queue population monitored for repeated failures with automatic escalation
- Firestore indexes configured for efficient querying of video status progression and cost aggregation
- Audit logs created for all key operations: video discovery, transcription submission, summary generation, cost threshold breaches

## New ADR Entry Template (copy for each new decision)

> Replace placeholders, keep section headers. Keep prose concise.

```

## ADR-XXXX — \<Short, specific title>

<a id="adr-XXXX"></a>
**Date**: YYYY-MM-DD
**Status**: Proposed | Accepted | Superseded
**Owner**: <Name>

### Context

<1–3 sentences: what changed or what forces drive this decision now>

### Alternatives

<Quick bullet list of alternatives considered, and why they were rejected.>

### Decision

\<Single clear decision in active voice; make it testable/verifiable>

### Consequences

* **Pros**: \<benefit 1>, \<benefit 2>
* **Cons / risks**: \<cost 1>, \<risk 1>
* **Supersedes**: ADR-NNNN (if any)
* **Superseded by**: ADR-MMMM (filled later if replaced)

### (Optional) Compliance / Verification

\<How we’ll check this is honored: tests, checks, fitness functions, runbooks>

```

---

## ADR-0002 — Event-Driven Broker Architecture with Firestore

<a id="adr-0002"></a>
**Date**: 2025-01-27  
**Status**: Accepted  
**Owner**: AI Agent

### Context

Modern full-stack applications require consistent data state between frontend and backend components. Traditional request-response patterns create tight coupling and require complex state synchronization logic. Real-time applications need immediate UI updates when data changes, regardless of the source of the change.

### Alternatives

- **Direct API communication**: Backend returns data directly to frontend, requires manual state management
- **Event streaming with external broker**: Use services like Redis Pub/Sub or RabbitMQ, adds infrastructure complexity
- **WebSocket connections**: Real-time but requires connection management and doesn't persist data
- **Firestore as event-driven broker**: Leverages built-in real-time capabilities and acts as single source of truth

### Decision

Implement event-driven broker architecture where Firestore serves as both the data store and event broker:

- All data mutations flow through Firestore exclusively
- Backend functions save data to Firestore without returning responses to frontend
- Frontend subscribes to Firestore collections/documents using hooks for real-time updates
- Firestore acts as the single source of truth for both backend and frontend
- UI updates automatically through Firestore real-time listeners

### Consequences

- **Pros**: Eliminates data synchronization issues, automatic real-time updates, reduced coupling between frontend and backend, simplified state management, leverages Firebase's built-in capabilities
- **Cons / risks**: Increased Firestore read operations, requires proper security rules design, potential data consistency challenges with complex operations, network dependency for all data access
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

Backend functions must only perform Firestore writes without returning data responses. Frontend components must use Firestore hooks (useFirestore, real-time listeners) for all data access. No direct API data responses to frontend. All data mutations trigger UI updates through Firestore change events.

---

## ADR-0001 — Agent-Focused Repository Structure

<a id="adr-0001"></a>
**Date**: 2025-01-27
**Status**: Accepted
**Owner**: AI Agent

### Context

The repository has evolved from a full-stack Firebase template to focus specifically on AI agent development and management. The original frontend and backend directories are no longer present, and the repository now contains agent-specific configuration, documentation, and task templates.

### Alternatives

- **Maintain full-stack structure**: Keep frontend and backend directories even if unused, adds complexity
- **Create separate agent repository**: Move agent files to a new repository, loses Firebase configuration context
- **Hybrid structure**: Keep both full-stack and agent components, creates confusion about primary purpose
- **Agent-focused structure**: Simplify to focus on agent development with Firebase as supporting infrastructure

### Decision

Implement agent-focused repository structure:

- `/agents/` - Primary directory containing all agent-related files and configuration
- Firebase configuration files (`firebase.json`, `firestore.rules`, `storage.rules`) at the agents level
- Agent documentation and rules in `.cursor/rules/` subdirectory
- Task templates in `tasks/` subdirectory
- Remove references to frontend and backend directories that no longer exist

### Consequences

- **Pros**: Clear focus on agent development, simplified structure, Firebase configuration remains available for agent backend needs, easier maintenance
- **Cons / risks**: Loss of full-stack template capabilities, may need to recreate frontend/backend if needed later, Firebase configuration references non-existent backend directory
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

Repository structure contains only agent-related files. Firebase configuration is updated to reflect actual directory structure. No references to non-existent frontend or backend directories in documentation.

---

## ADR-0005 — Reliability and Quota Management System

<a id="adr-0005"></a>
**Date**: 2025-09-14
**Status**: Accepted
**Owner**: AI Agent

### Context

The Autopiloot system integrates with multiple external APIs (YouTube Data API, AssemblyAI, OpenAI, Slack) that have quota limits and can experience transient failures. Without proper reliability mechanisms, API quota exhaustion could crash the system, failed operations would be lost, and there was no systematic way to resume processing from checkpoints. The system needed enterprise-grade reliability features including dead letter queues, exponential backoff, quota management, and configurable retry behavior to ensure resilient operation under API constraints.

### Alternatives

- **Simple retry loops**: Basic retry without backoff, prone to overwhelming failing services, no quota awareness, lost failed operations
- **External queue systems**: Use Redis/RabbitMQ for DLQ, adds infrastructure complexity, additional operational overhead, cost for MVP
- **Hard-coded retry limits**: No configuration flexibility, requires code changes for tuning, inconsistent behavior across services
- **Manual failure handling**: Operational burden, no systematic recovery, human error prone, doesn't scale
- **No checkpoint system**: Repeated API calls from beginning, quota waste, longer recovery times, inefficient processing

### Decision

Implement comprehensive reliability and quota management system with configuration-driven behavior:

**Dead Letter Queue (DLQ) System:**

- Structured DLQ entries in Firestore `jobs_deadletter` collection with job_type, video_id, reason, retry_count, timestamps
- Automatic DLQ routing after maximum retry attempts exceeded
- DLQ monitoring and query tools for operational insights and failure analysis
- Support for multiple job types: youtube_discovery, transcription, summarization, sheets_processing, slack_notification

**Quota Management:**

- `QuotaManager` class for tracking API usage across services (YouTube: 10,000 units/day, AssemblyAI: configurable)
- Automatic quota exhaustion detection with next reset time calculation
- Service availability checking before making API calls to prevent quota violations
- Graceful handling of quota exhaustion with resume capabilities

**Intelligent Retry Strategy:**

- Exponential backoff with configurable base delay (default: 60 seconds)
- Configurable maximum retry attempts (default: 3) before DLQ routing
- Backoff progression: base_delay × 2^retry_count (60s → 120s → 240s → 480s)
- Settings.yaml configuration support for retry behavior tuning

**Checkpoint System:**

- `lastPublishedAt` checkpoint for YouTube API to resume from last processed video
- Reduces API calls by avoiding reprocessing of already-handled content
- Firestore persistence for checkpoint data with automatic updates
- ISO 8601 timestamp handling for accurate temporal ordering

**Configuration Integration:**

- YAML-based configuration in `settings.yaml` under `reliability` section
- TypedDict validation for configuration structure and value constraints
- Helper functions for accessing configuration values with sensible defaults
- Real-time configuration loading without code deployment

### Consequences

- **Pros**: Enterprise-grade reliability prevents data loss, configurable retry behavior allows fine-tuning without code changes, quota management prevents API violations and system crashes, checkpoint system reduces API usage and speeds recovery, comprehensive monitoring enables proactive operational management, Firestore-based architecture leverages existing infrastructure, extensive test coverage ensures reliable operation
- **Cons / risks**: Increased system complexity with additional moving parts, Firestore storage costs for DLQ and checkpoint data, potential for configuration errors affecting retry behavior, dependency on external API reset schedules, monitoring overhead for DLQ and quota status
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- 22 comprehensive reliability tests with 100% pass rate covering DLQ operations, retry logic, quota management, and checkpoint functionality
- Configuration validation tests ensure invalid values are rejected with clear error messages
- Dead letter queue entries automatically created after max retry attempts with complete error context
- Quota exhaustion triggers graceful pausing with automatic resumption when quota resets
- Checkpoint system demonstrated to reduce YouTube API calls by resuming from `lastPublishedAt` timestamp
- Settings.yaml integration verified with different retry values affecting actual backoff calculations
- Firestore composite indexes defined for efficient DLQ querying and operational monitoring
- Tools provided for DLQ analysis, quota monitoring, and checkpoint management

---

## ADR-0006 — Modular Agent Tool Architecture

<a id="adr-0006"></a>
**Date**: 2025-09-14
**Status**: Accepted
**Owner**: AI Agent

### Context

The Autopiloot system requires 20 distinct tools across 4 specialized agents (Scraper, Transcriber, Summarizer, Assistant) to orchestrate the YouTube content processing pipeline. Each agent needs domain-specific capabilities while maintaining consistent patterns for error handling, environment configuration, and testing. Without proper tool architecture, code duplication would proliferate, maintenance complexity would increase, and integration with agency_swarm or other orchestration frameworks would be challenging.

### Alternatives

- **Monolithic agent classes**: Single class per agent with all tools as methods, reduces modularity, difficult testing, violates single responsibility principle
- **Function-based tools**: Simple functions without classes, no state management, inconsistent error handling, difficult to extend
- **External tool libraries**: Use existing libraries for each service, integration overhead, inconsistent interfaces, missing domain-specific logic
- **Copy-paste implementations**: Quick initial development, maintenance nightmare, no code reuse, error propagation across copies
- **Framework-specific tools**: Lock-in to specific orchestration framework (agency_swarm), reduced portability, harder standalone testing

### Decision

Implement modular tool architecture with inheritance-based consistency and service isolation:

**Core Architecture:**

- `BaseTool` abstract class providing environment validation, error handling, and standard interface
- One tool class per specific capability (20 total: 7 Scraper, 5 Transcriber, 4 Summarizer, 4 Assistant)
- TypedDict-based request/response contracts for type safety and documentation
- Service-specific tool directories: `{agent}/tools/*.py` with __init__.py for clean imports
- Consistent `run()` method interface accepting Dict[str, Any] and returning Dict[str, Any]

**Tool Distribution:**

- **Scraper (7)**: ResolveChannelHandle, ListRecentUploads, ReadSheetLinks, ExtractYouTubeFromPage, SaveVideoMetadata, EnqueueTranscription, RemoveSheetRow
- **Transcriber (5)**: GetVideoAudioUrl, SubmitAssemblyAIJob, PollTranscriptionJob, StoreTranscriptToDrive, SaveTranscriptRecord
- **Summarizer (4)**: GenerateShortSummary, StoreShortInZep, StoreShortSummaryToDrive, SaveSummaryRecord
- **Assistant (4)**: FormatSlackBlocks, SendSlackMessage, MonitorTranscriptionBudget, SendErrorAlert

**Environment Integration:**

- All secrets via environment variables (no hardcoded values)
- `_validate_env_vars()` method enforces required configuration at initialization
- `get_env_var()` helper with required/optional support and clear error messages
- Service account path validation for Google Cloud services

**Testing Strategy:**

- Every tool includes `if __name__ == "__main__"` test block for standalone execution
- Mock-friendly design with dependency injection capability
- Test requests demonstrate expected input/output formats
- Error scenarios validated with appropriate exception handling

### Consequences

- **Pros**: Clean separation of concerns with single responsibility per tool, consistent error handling and validation across all tools, easy integration with multiple orchestration frameworks, standalone testability accelerates development, environment variable enforcement prevents production failures, modular architecture enables selective tool usage, type hints improve IDE support and documentation
- **Cons / risks**: More files to manage (20 tools + base class + init files), inheritance coupling to BaseTool class, potential for interface drift without strict governance, initial setup complexity for new developers, dependency on external service SDKs
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All 20 tools implemented with BaseTool inheritance and consistent run() interface
- Each tool validates required environment variables at initialization with clear error messages
- Test blocks verified to execute without errors when valid mock data provided
- No hardcoded secrets found in any tool implementation (verified by code review)
- Import structure validated: `from autopiloot.{agent}.tools import {ToolClass}`
- Service-specific SDK usage: googleapiclient for Google services, assemblyai for transcription, slack_sdk for notifications
- Firestore integration uses google-cloud-firestore with proper project/credential configuration
- Error handling demonstrates graceful degradation with actionable error messages

---

## ADR-0007: Firebase Functions v2 Scheduling Architecture

- **Date**: 2024-09-14
- **Status**: Accepted
- **Context**: Autopiloot requires scheduled execution of agent workflows (daily YouTube scraping) and event-driven processing (budget monitoring on transcript creation). Need cloud-native solution with reliable scheduling, automatic scaling, and integration with existing Firestore data layer.

### Decision

Implement Firebase Functions v2 with scheduled and event-driven execution patterns:

**Scheduled Functions:**
- Daily scraper execution at 01:00 Europe/Amsterdam using Cloud Scheduler cron triggers
- `@scheduler_fn.on_schedule(schedule="0 1 * * *", timezone="Europe/Amsterdam")`
- 9-minute timeout with 512MB memory allocation for agency processing
- Single instance constraint to prevent concurrent executions

**Event-Driven Functions:**
- Budget monitoring triggered by Firestore document writes to `transcripts/{video_id}`
- Real-time cost calculation with daily threshold alerting at 80% of $5 budget
- Slack notifications via Assistant agent tools integration
- Automatic audit logging to `audit_logs` and `costs_daily` collections

**Architecture Components:**
- `autopiloot/services/firebase/functions/scheduler.py`: Main functions implementation
- `autopiloot/services/firebase/functions/requirements.txt`: Python dependencies for Firebase runtime
- Manual deployment via Firebase CLI with service account authentication
- Centralized error handling with structured logging and alert propagation

**Integration Strategy:**
- Direct import of agency classes: `from agency import AutopilootAgency`
- Reuse existing environment configuration via `core.env_loader`
- Leverage Assistant tools for Slack formatting and messaging
- Firestore as event broker between scheduled and on-demand executions

### Alternatives Considered

**Google Cloud Run Jobs**: More flexible but requires custom scheduling logic and lacks native Firestore event integration. Higher operational complexity for simple cron scheduling.

**GitHub Actions**: Limited to repository events, not suitable for production data processing workflows. No direct Firestore integration or timezone handling.

**Local Cron + Self-Hosted**: Requires infrastructure management, no automatic scaling, single point of failure. No native cloud logging or monitoring.

**AWS Lambda + EventBridge**: Vendor lock-in to AWS ecosystem, would require migration of existing Firestore data layer. More complex cold start behavior.

### Consequences

- **Pros**: Native Google Cloud integration with existing Firestore infrastructure, automatic scaling and cold start optimization, built-in monitoring and logging via Cloud Functions, timezone-aware scheduling with daylight saving handling, event-driven architecture eliminates polling overhead, cost-effective pay-per-invocation model, Firebase CLI provides streamlined deployment workflow
- **Cons / risks**: Vendor lock-in to Google Cloud Platform, 9-minute execution timeout may constrain large channel processing, cold start latency for infrequent functions, requires manual Firebase project setup and service account configuration, debugging complexity in cloud environment, potential for cascading failures across function dependencies
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Firebase Functions v2 decorators implemented with correct scheduling syntax and timezone configuration
- Environment variables properly loaded via centralized `env_loader` with validation
- Budget calculations tested with mock Firestore data and verified against daily cost thresholds
- Slack alerting integration confirmed with Assistant agent tools in both emulator and production
- Audit logging captures all function executions with structured data for operational monitoring
- Manual deployment procedures documented with step-by-step Firebase CLI instructions
- Error handling includes graceful degradation and automatic retry logic for transient failures
- Function timeouts and memory limits configured appropriately for agency workflow requirements

---

## ADR-0008: Agency Swarm v1.0.0 Tool Implementation

<a id="adr-0008"></a>

- **Date**: 2024-09-14
- **Status**: Accepted
- **Context**: Autopiloot requires 20 production-ready tools across 4 agents (Scraper, Transcriber, Summarizer, Assistant) using Agency Swarm framework v1.0.0. Need consistent tool architecture, proper validation, environment configuration, and comprehensive testing for YouTube content processing automation.

### Decision

Implement all 20 required tools using Agency Swarm v1.0.0 BaseTool framework:

**Tool Distribution:**
- **Scraper (7 tools)**: ResolveChannelHandle, ListRecentUploads, ReadSheetLinks, ExtractYouTubeFromPage, SaveVideoMetadata, EnqueueTranscription, RemoveSheetRow
- **Transcriber (5 tools)**: GetVideoAudioUrl, SubmitAssemblyAIJob, PollTranscriptionJob, StoreTranscriptToDrive, SaveTranscriptRecord
- **Summarizer (4 tools)**: GenerateShortSummary, StoreShortInZep, StoreShortSummaryToDrive, SaveSummaryRecord
- **Assistant (4 tools)**: FormatSlackBlocks, SendSlackMessage, MonitorTranscriptionBudget, SendErrorAlert

**Implementation Standards:**
- All tools inherit from `agency_swarm.tools.BaseTool` with Pydantic field validation
- Environment variables only for secrets - no API keys as tool parameters
- Standardized `run()` method returning strings directly (not Dict objects)
- Comprehensive test blocks in every tool file with `if __name__ == "__main__":`
- Production-ready error handling with graceful failure modes

**API Integration:**
- YouTube Data API v3: google-api-python-client SDK
- AssemblyAI: Official assemblyai SDK for transcription services
- Google Drive/Sheets: google-api-python-client with service account auth
- Slack: slack_sdk for notification systems
- Firestore: firebase-admin SDK for data persistence
- OpenAI: openai SDK for LLM summarization

**Tool Architecture:**
- Centralized environment loading via `core/env_loader.py`
- Configuration management through `config/settings.yaml`
- Consistent error handling and logging patterns
- Type safety with Pydantic Field validation and TypedDict structures

### Alternatives Considered

**Custom BaseTool Implementation**: Initially considered custom tool base class but Agency Swarm v1.0.0 provides superior validation, documentation generation, and agent integration. Maintains framework consistency.

**Direct Function Tools**: Agency Swarm supports @function_tool decorator but BaseTool classes provide better structure, validation, and testing capabilities for complex production tools.

**Multiple Tool Directories**: Considered consolidating all tools in single directory but agent-specific tool folders improve organization and align with Agency Swarm best practices.

### Consequences

- **Pros**: Full Agency Swarm v1.0.0 compatibility ensures seamless agent integration, Pydantic validation prevents runtime errors and improves tool reliability, standardized architecture enables consistent development patterns across all agents, comprehensive testing validates production readiness, environment-based configuration supports multiple deployment environments, proper SDK usage reduces API integration complexity
- **Cons / risks**: Framework dependency creates coupling to Agency Swarm ecosystem, tool inheritance requires understanding of BaseTool internals, comprehensive validation may impact tool execution performance, extensive testing increases development time and maintenance overhead
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All 20 required tools implemented with agency_swarm.tools.BaseTool inheritance and consistent interfaces
- Pydantic Field validation enforced for all tool parameters with descriptive error messages
- Environment variable usage verified - no hardcoded secrets found in any tool implementation
- Test blocks confirmed functional in all 32 tool files with realistic test scenarios
- API SDK integration validated: YouTube Data API, AssemblyAI, Google APIs, Slack, Firebase, OpenAI
- Tool directory structure follows Agency Swarm conventions with proper __init__.py imports
- Error handling demonstrates graceful degradation with actionable error messages
- Production deployment tested with proper service account authentication and quota management

---

## ADR-0009: YouTube Channel Handle Resolution Tool

<a id="adr-0009"></a>

- **Date**: 2024-09-14
- **Status**: Accepted
- **Context**: Autopiloot scraper agent requires batch resolution of YouTube channel handles (e.g., @AlexHormozi) to canonical channel IDs for subsequent API operations. Need configuration-driven solution that loads handles from settings.yaml, implements retry logic for rate limits, and provides comprehensive error handling for production reliability.

### Decision

Implement ResolveChannelHandles tool using Agency Swarm v1.0.0 BaseTool framework:

**Core Functionality:**
- Batch resolution of multiple YouTube channel handles from configuration
- Loads handles from `settings.yaml` under `scraper.handles` array
- Returns mapping as JSON string: `{"@AlexHormozi": "UCfV36TX5AejfAGIbtwTc7Zw", ...}`
- Continues processing remaining handles even when individual handles fail

**YouTube API Integration:**
- Primary resolution via YouTube Data API v3 Search endpoint with exact handle matching
- Fallback to legacy Channels API with forUsername parameter for older channels
- Dual authentication: Service account credentials (preferred) + API key fallback
- Smart handle verification through custom URL checking for accurate matches

**Reliability Features:**
- Exponential backoff retry logic: 1s, 2s, 4s delays for rate limit (429) and server errors (5xx)
- Configurable max_retries parameter with Pydantic field validation (default: 3)
- Graceful error handling that preserves partial results and continues processing
- Handle normalization ensuring all handles start with @ symbol for consistency

**Configuration Integration:**
- Centralized environment loading via `core/env_loader.py` for API key management
- Configuration loading via `config/loader.py` with `get_config_value("scraper.handles")`
- Default fallback to `["@AlexHormozi"]` if configuration missing or empty
- Integration with existing settings.yaml structure and validation patterns

### Alternatives Considered

**Individual Handle Resolution Tool**: Could create separate tool for single handle resolution but batch processing reduces API calls and provides better error resilience for production workflows.

**Direct API Integration**: Could embed YouTube API calls in other tools but centralized handle resolution enables caching, rate limit management, and reusability across multiple scraper operations.

**Synchronous vs Async Processing**: Considered async implementation but synchronous approach provides better error tracking and simpler retry logic for the current use case with small handle counts.

### Consequences

- **Pros**: Batch processing reduces YouTube API quota usage through efficient single-tool execution, configuration-driven approach enables easy channel management without code changes, comprehensive retry logic ensures reliable operation under API rate limits, error isolation prevents single handle failures from blocking entire operation, JSON output format enables easy integration with other tools and logging systems, proper Agency Swarm integration provides consistent validation and error handling patterns
- **Cons / risks**: YouTube API dependency creates external service coupling with quota limitations, handle resolution accuracy depends on YouTube's search algorithm and custom URL availability, batch processing may timeout with very large handle lists requiring chunking strategies, configuration changes require deployment updates rather than runtime modifications
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Agency Swarm v1.0.0 BaseTool inheritance with proper Pydantic field validation implemented
- Configuration loading from settings.yaml verified with scraper.handles array structure
- YouTube Data API v3 integration tested with both Search and Channels endpoints
- Retry logic verified with exponential backoff timing and proper HTTP error code handling
- Error handling tested for missing handles, API failures, configuration errors, and partial failures
- JSON output format validated against TypedDict specification with handle->channel_id mapping
- Comprehensive test suite created with 15+ unit tests covering all error scenarios and edge cases
- Production authentication tested with service account credentials and API key fallback
- Handle normalization verified to ensure consistent @ prefix formatting across all inputs

---

## ADR-0010: YouTube Uploads Playlist Discovery Tool

<a id="adr-0010"></a>

- **Date**: 2024-09-14
- **Status**: Accepted
- **Context**: Autopiloot scraper agent requires efficient video discovery from YouTube channels within specific time windows. Need solution that uses uploads playlist for optimal API quota usage, implements checkpoint-based processing to skip already-processed videos, and includes comprehensive quota management for production reliability.

### Decision

Implement ListRecentUploads tool using uploads playlist API with checkpoint persistence:

**Core Functionality:**
- Channel uploads playlist discovery via YouTube Data API v3 channels endpoint
- Video fetching through playlistItems.list API for efficient playlist-based discovery
- Batch video details retrieval via videos.list API for durations and metadata
- Time window filtering between since_utc and until_utc parameters with early exit optimization

**Checkpoint Management:**
- LastPublishedAt timestamp persistence in Firestore checkpoints collection
- Incremental processing that skips videos processed in previous runs
- Configurable checkpoint usage via use_checkpoint boolean parameter
- Graceful fallback when Firestore unavailable without breaking functionality

**Quota Management:**
- Integration with QuotaManager for YouTube API availability checking
- Request tracking and quota exhaustion detection with proper error handling
- Batch processing of video details in 50-video chunks to optimize API usage
- Early termination when quota exhausted with structured error responses

**Production Features:**
- ISO 8601 duration parsing (PT1H30M45S format) with regex pattern matching
- Dual authentication: service account credentials preferred, API key fallback
- JSON output format with items array, total counts, and checkpoint status
- Error resilience that continues processing despite individual video failures

### Alternatives Considered

**Search API Approach**: YouTube search API provides video discovery but consumes more quota units and lacks the efficiency of playlist-based retrieval. Uploads playlist provides chronological ordering ideal for checkpoint-based processing.

**Direct Channel Videos**: Could fetch all channel videos but uploads playlist specifically contains only uploads, filtering out playlists, live streams, and other content types that may not be relevant for transcription workflows.

**Synchronous vs Async Processing**: Considered async implementation but synchronous approach provides better error tracking and simpler checkpoint management for the current batch sizes and quota constraints.

### Consequences

- **Pros**: Uploads playlist provides most efficient API quota usage for video discovery, checkpoint-based processing eliminates duplicate work and enables incremental updates, batch video details fetching optimizes API calls while respecting rate limits, time window filtering ensures only relevant videos are processed, QuotaManager integration prevents quota exhaustion and provides graceful degradation, Firestore persistence enables stateful processing across multiple runs
- **Cons / risks**: Uploads playlist dependency means tool cannot discover videos from other playlist types, checkpoint persistence requires Firestore availability for optimal operation, batch processing may still hit rate limits with very active channels, time window precision depends on YouTube's publishedAt accuracy, quota exhaustion can interrupt processing requiring manual intervention or retry logic
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- YouTube Data API v3 integration verified with uploads playlist discovery and video details fetching
- Checkpoint persistence implemented with Firestore document structure and graceful fallback handling
- Quota management tested with availability checking and request tracking integration
- Time window filtering validated with ISO 8601 timestamp parsing and comparison logic
- Batch processing confirmed with 50-video chunks and proper API parameter handling
- Duration parsing tested with regex pattern for PT1H30M45S format conversion to seconds
- Error handling verified for rate limits, quota exhaustion, missing playlists, and invalid data
- Agency Swarm BaseTool integration with proper Pydantic field validation and string return type
- Comprehensive test suite with 15+ unit tests covering all functionality and error scenarios

---

## ADR-0011 — Google Sheets and Web Page YouTube Extraction Tools

<a id="adr-0011"></a>
**Date**: 2025-09-14
**Status**: Accepted
**Owner**: AI Agent

### Context

Autopiloot scraper agent requires two complementary tools for YouTube URL discovery: ReadSheetLinksSimple for extracting YouTube links from single-column Google Sheets (backfill scenarios), and ExtractYouTubeFromPage for discovering YouTube videos embedded in web pages. These tools support different content ingestion workflows - direct URL lists from curated sheets and dynamic discovery from blog posts, social media, or content aggregators.

### Alternatives

- **Combined single tool**: Single tool handling both sheets and web pages but would violate single responsibility principle and complicate parameter validation
- **Raw HTML parsing only**: Skip BeautifulSoup for faster parsing but would miss structured content in iframes, meta tags, and complex JavaScript scenarios
- **Google Sheets API v4 batch operations**: More complex batch reading but current single-range approach sufficient for MVP requirements
- **External URL extraction services**: Third-party APIs for web scraping but adds dependency, cost, and rate limiting complexity
- **Selenium for JavaScript rendering**: Handle dynamic content but significant overhead and complexity for static URL extraction

### Decision

Implement two specialized tools using Agency Swarm v1.0.0 BaseTool framework:

**ReadSheetLinksSimple Tool:**

- Single-column Google Sheet processing with configurable A1 notation range (default: "Sheet1!A:A")
- YouTube URL validation using comprehensive regex patterns for all YouTube URL formats
- URL normalization to standard `https://www.youtube.com/watch?v=` format
- Automatic deduplication with set-based unique URL tracking
- Max rows limiting with optional parameter for large sheet handling
- JSON response format with items array and processing summary statistics

**ExtractYouTubeFromPage Tool:**

- Multi-source YouTube URL extraction: raw HTML text, iframe embeds, anchor links, meta og:video tags, JavaScript content
- BeautifulSoup HTML parsing with proper error handling for malformed content
- Comprehensive YouTube URL pattern recognition including youtu.be, youtube.com/embed, youtube.com/v formats
- HTTP client with proper User-Agent headers to avoid blocking
- Cross-source deduplication ensuring URLs found in multiple places appear only once
- Structured JSON response with videos array and extraction summary

**Shared Implementation Standards:**

- Google Sheets API integration using service account authentication via `core/env_loader.py`
- Configuration loading from `settings.yaml` with graceful fallback for missing sheet IDs
- Comprehensive error handling with JSON error responses maintaining consistent interface
- Pydantic field validation for all tool parameters with descriptive help text
- Production-ready test blocks with realistic scenarios and error condition coverage

### Consequences

- **Pros**: Specialized tools follow single responsibility principle enabling focused testing and maintenance, comprehensive URL pattern recognition ensures high extraction accuracy, deduplication prevents duplicate processing reducing downstream API costs, Google Sheets integration enables non-technical content curation workflows, BeautifulSoup parsing handles complex HTML structures reliably, Agency Swarm compliance ensures consistent tool interfaces across entire agent ecosystem, flexible configuration via settings.yaml supports multiple deployment environments
- **Cons / risks**: Two separate tools require coordination for workflows using both ingestion methods, Google Sheets API dependency creates external service coupling with quota constraints, web scraping may fail on sites with bot protection or complex JavaScript rendering, HTTP timeout issues with slow-loading pages could interrupt processing, regex pattern maintenance required as YouTube introduces new URL formats
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Agency Swarm v1.0.0 BaseTool inheritance implemented with proper Pydantic field validation and string return types
- Google Sheets API v4 integration verified with service account authentication and configurable range selection
- YouTube URL validation tested against 8+ different URL formats including edge cases and malformed inputs
- Web page extraction validated across multiple HTML structure types: iframes, links, meta tags, JavaScript, raw text
- Deduplication algorithms tested to ensure identical videos in different URL formats are merged correctly
- Error handling verified for network failures, API errors, malformed HTML, missing configuration, and quota exhaustion
- Comprehensive test suites created with 12+ unit tests per tool covering functionality and error scenarios
- JSON response format standardized with items arrays, summary statistics, and error handling consistency
- Configuration integration tested with settings.yaml loading and graceful fallback behavior

---

## ADR-0012 — Firestore Video Metadata Storage Tool

<a id="adr-0012"></a>
**Date**: 2025-09-14
**Status**: Accepted
**Owner**: AI Agent

### Context

Autopiloot scraper agent requires persistent storage of discovered video metadata in Firestore with strict idempotency requirements. The SaveVideoMetadata tool must upsert video records to `videos/{video_id}` collection, enforce business rules (70-minute duration limit), maintain proper status progression, and ensure re-running doesn't create duplicates. This tool serves as the bridge between YouTube content discovery and the transcription workflow.

### Alternatives

- **Direct database insertion without idempotency**: Risk of duplicate records and inconsistent state during retries or partial failures
- **External database (PostgreSQL/MySQL)**: Additional infrastructure complexity and maintenance overhead for MVP requirements  
- **File-based storage**: No transaction support, difficult querying, scalability limitations, no real-time updates
- **In-memory storage**: Data loss on restart, no persistence across function invocations, unsuitable for production workflows
- **Multiple document per video**: Creates normalization issues and complex querying patterns for status tracking

### Decision

Implement SaveVideoMetadata tool using Firestore native upsert patterns with comprehensive business rule validation:

**Core Functionality:**

- Idempotent upsert to `videos/{video_id}` document using Firestore's atomic operations
- Business rule enforcement: 70-minute (4200 second) maximum duration from configuration
- Status progression tracking with initial `status: discovered` field per PRD data model
- ISO 8601 UTC timestamp handling with Z suffix for all temporal fields (`published_at`, `created_at`, `updated_at`)
- Document-level deduplication using video_id as natural primary key

**Firestore Integration:**

- Firebase Admin SDK with service account authentication via `GOOGLE_APPLICATION_CREDENTIALS`
- Atomic upsert operations: `set()` for new documents, `update()` for existing documents
- SERVER_TIMESTAMP usage for `created_at` and `updated_at` ensuring consistent server-side timing
- Conditional logic preserving `created_at` on updates while refreshing `updated_at`
- Project isolation via `GCP_PROJECT_ID` environment variable configuration

**Agency Swarm v1.0.0 Compliance:**

- BaseTool inheritance with Pydantic field validation for all input parameters
- Comprehensive field descriptions enabling proper agent understanding and usage
- JSON string return format with structured responses including error handling
- Optional parameter support for `channel_id` with proper null handling
- Production-ready test block demonstrating realistic usage scenarios

**Error Handling and Validation:**

- Duration validation against configurable limits with graceful rejection
- Environment variable validation with descriptive error messages
- Service account file existence verification before Firestore initialization
- Comprehensive exception handling returning structured JSON error responses
- Configuration loading with sensible defaults for missing settings

### Consequences

- **Pros**: Firestore native operations ensure atomic upserts preventing race conditions and duplicate records, SERVER_TIMESTAMP provides consistent timing across distributed environments, Agency Swarm compliance enables seamless integration with agent workflows, comprehensive validation prevents invalid data persistence, JSON error responses maintain consistent tool interfaces, flexible configuration supports different deployment environments without code changes, idempotent operations enable safe retries and partial failure recovery
- **Cons / risks**: Firestore dependency creates external service coupling with latency and availability constraints, service account authentication requires proper credential management and rotation, document-level locking may impact concurrent access patterns for same video_id, configuration-based business rules require deployment updates for rule changes, error handling complexity increases tool maintenance overhead
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Agency Swarm v1.0.0 BaseTool inheritance with proper Pydantic field validation and JSON string return format
- Firestore atomic operations verified with set() for creation and update() for modifications
- Idempotency tested with duplicate video_id operations confirming no duplicate document creation
- Business rule validation confirmed with 70-minute duration limit enforcement from configuration
- ISO 8601 timestamp format validation with Z suffix requirement for all temporal fields
- Environment variable handling tested for missing credentials and invalid project configurations
- Error handling verified for Firestore connection failures, authentication errors, and invalid input data
- Comprehensive test suite with 15+ unit tests covering functionality, edge cases, and error scenarios
- Document structure compliance with PRD data model including all required fields and optional channel_id
- Configuration integration tested with settings.yaml loading and default value fallbacks

---

## ADR-0013 — Transcription Job Queue Management Tool

<a id="adr-0013"></a>
**Date**: 2025-09-14
**Status**: Accepted
**Owner**: AI Agent

### Context

Autopiloot scraper agent requires job queue management for transcription workflows after video discovery. The EnqueueTranscription tool must create job entries in `jobs/transcription` collection for eligible videos, enforce business rules (duration limits, existing transcript checks), prevent duplicate jobs, and maintain atomic operations linking video status updates with job creation. This tool bridges video discovery with the transcription processing pipeline.

### Alternatives

- **Direct transcription without queue**: Immediate processing without job tracking, no retry mechanism, difficult error recovery, no progress monitoring
- **External queue systems (Redis/RabbitMQ)**: Additional infrastructure complexity, operational overhead, network dependencies, cost for MVP requirements
- **File-based job queue**: No transaction support, difficult concurrent access, limited querying capabilities, no real-time monitoring
- **In-memory job tracking**: Data loss on restart, no persistence across function invocations, unsuitable for production reliability
- **Separate job creation and video update**: Race conditions between operations, potential inconsistent state, complex error handling

### Decision

Implement EnqueueTranscription tool using Firestore atomic batch operations with comprehensive validation and duplicate prevention:

**Core Functionality:**

- Atomic job creation in `jobs/transcription/{job_id}` collection with batch operations
- Video status progression from `discovered` to `transcription_queued` in single transaction
- Comprehensive eligibility validation: duration limits, existing transcripts, duplicate jobs
- Business rule enforcement with 70-minute (4200 second) maximum duration from configuration
- Structured job metadata including video details, timestamps, and processing status

**Duplicate Prevention Logic:**

- Existing transcript detection via `transcripts/{video_id}` document existence check
- Active job detection using Firestore queries on video_id with status filtering (`pending`, `processing`, `completed`)
- Idempotent operations returning existing job information when duplicates detected
- Graceful handling of concurrent job creation attempts with proper error messaging

**Firestore Integration Patterns:**

- Batch write operations ensuring atomicity between job creation and video status updates
- SERVER_TIMESTAMP usage for consistent timing across distributed environments
- Proper collection structure following `jobs/transcription/{job_id}` hierarchy
- Comprehensive job metadata preservation from original video document

**Agency Swarm v1.0.0 Compliance:**

- BaseTool inheritance with Pydantic field validation for video_id parameter
- JSON string return format with structured responses including error handling
- Comprehensive field descriptions enabling proper agent understanding
- Production-ready test block demonstrating realistic usage scenarios

**Error Handling and Validation:**

- Video existence validation with clear error messages for missing documents
- Duration validation against configurable limits with graceful rejection
- Environment variable validation with descriptive error messages
- Comprehensive exception handling returning structured JSON error responses

### Consequences

- **Pros**: Atomic batch operations prevent race conditions between job creation and video status updates, comprehensive duplicate prevention eliminates redundant processing and resource waste, Firestore query-based validation enables efficient existing job detection, structured job metadata provides complete audit trail for processing workflows, Agency Swarm compliance ensures seamless integration with agent orchestration, configurable business rules support different deployment environments, graceful error handling maintains system stability under failure conditions
- **Cons / risks**: Firestore dependency creates external service coupling with latency and availability constraints, batch operation complexity increases debugging overhead for transaction failures, query-based duplicate detection may impact performance with large job collections, configuration-based business rules require deployment updates for rule changes, multiple Firestore reads per job creation increase API usage and costs
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Agency Swarm v1.0.0 BaseTool inheritance with proper Pydantic field validation and JSON string return format
- Firestore atomic batch operations verified with set() for job creation and update() for video status changes
- Duplicate prevention tested with existing transcript detection and active job query filtering
- Business rule validation confirmed with 70-minute duration limit enforcement from configuration
- Error handling verified for missing videos, Firestore connection failures, and invalid configurations
- Job data structure compliance with complete metadata preservation from video documents
- Comprehensive test suite with 15+ unit tests covering functionality, edge cases, and error scenarios
- Atomic transaction testing confirming rollback behavior on partial failures
- Performance optimization verified with efficient Firestore query patterns for duplicate detection

---

## ADR-0014 — Google Sheets Row Management and Archival Tool

<a id="adr-0014"></a>
**Date**: 2025-09-14
**Status**: Accepted
**Owner**: AI Agent

### Context

Autopiloot scraper agent requires cleanup of processed YouTube links from Google Sheets after successful backfill operations. The RemoveSheetRow tool must handle multiple row removal scenarios with archive-first approach for auditability, manage row index shifting during batch operations, and provide flexible clearing vs archival modes. This tool completes the Google Sheets workflow by maintaining clean input data while preserving processing history.

### Alternatives

- **Direct row deletion without archival**: Simple but loses audit trail, no recovery option for incorrectly processed rows, difficult troubleshooting
- **Manual sheet management**: Operational burden, human error prone, doesn't scale with automation, inconsistent processing
- **External archival storage**: Additional infrastructure complexity, data synchronization challenges, cost overhead for MVP requirements
- **Copy-before-delete approach**: Requires multiple API calls, potential for partial failures, complex rollback logic
- **Mark-as-processed columns**: Sheet becomes cluttered, filtering complexity, requires schema changes to existing sheets

### Decision

Implement RemoveSheetRow tool using Google Sheets API with archive-first approach and flexible operation modes:

**Core Functionality:**

- Archive-first approach moving processed rows to dedicated 'Archive' tab with timestamps and metadata
- Fallback clear mode for scenarios where archival isn't needed or fails
- Batch operations processing multiple row indices efficiently with single API calls
- Automatic Archive sheet creation with proper headers when missing
- Row index management handling deletion order to prevent index shifting issues

**Archive Implementation:**

- Dedicated 'Archive' tab creation with structured headers for audit trail
- Row data preservation with additional metadata: archive timestamp, original row number
- Atomic archival process: read → append to archive → delete from source
- Archive tab isolation preventing interference with source sheet operations
- Historical data retention enabling recovery and troubleshooting scenarios

**Batch Processing Optimization:**

- Descending order row processing to prevent index shifting during deletions
- Google Sheets batchUpdate API usage for efficient multiple row operations
- batchClear API integration for non-destructive content clearing
- Empty row detection and skipping to avoid unnecessary archive entries
- Comprehensive error handling for partial batch failures

**Agency Swarm v1.0.0 Compliance:**

- BaseTool inheritance with Pydantic field validation for all parameters
- Flexible parameter configuration: sheet_id, row_indices, archive_mode, source_sheet_name
- JSON string return format with detailed operation results and statistics
- Comprehensive field descriptions enabling proper agent understanding

**Error Handling and Validation:**

- Sheet existence validation for both source and archive tabs
- Service account authentication with proper credential management
- API operation failure recovery with detailed error messaging
- Configuration validation using settings.yaml sheet ID fallback
- Graceful handling of missing rows and malformed sheet structures

### Consequences

- **Pros**: Archive-first approach provides complete audit trail for compliance and troubleshooting, batch operations minimize API calls and improve performance, flexible modes support different use cases from testing to production, automatic Archive sheet creation reduces manual setup overhead, descending order processing prevents row index shifting issues, Agency Swarm compliance ensures seamless agent integration, comprehensive error handling maintains system stability
- **Cons / risks**: Archive tab grows over time requiring occasional cleanup or partitioning, additional API calls for archival increase Google Sheets quota usage, batch operations complexity increases debugging overhead for partial failures, row index management requires careful ordering to prevent data corruption, dependency on Google Sheets API availability and rate limits
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Agency Swarm v1.0.0 BaseTool inheritance with proper Pydantic field validation and JSON string return format
- Google Sheets API integration verified with batchUpdate and batchClear operations for efficient processing
- Archive functionality tested with automatic sheet creation, timestamp addition, and metadata preservation
- Batch processing confirmed with descending order row deletion preventing index shifting issues
- Error handling verified for missing sheets, API failures, authentication errors, and malformed configurations
- Row index management tested with multiple scenarios including empty rows and out-of-order indices
- Configuration integration validated with settings.yaml sheet ID loading and fallback behavior
- Comprehensive test suite with 15+ unit tests covering functionality, edge cases, and error scenarios
- Archive tab structure compliance with proper headers and metadata columns for audit requirements
- Performance optimization verified with minimal API calls through efficient batch operations

---

## ADR-0015 — Agency Swarm v1.0.0 Framework Restructuring

<a id="adr-0015"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

The Autopiloot codebase had evolved with multiple conflicting directory structures and non-compliant tool implementations. An audit revealed 19 tools using custom `core.base_tool.BaseTool` instead of Agency Swarm's `agency_swarm.tools.BaseTool`, inconsistent agent definitions, and directory naming that violated framework conventions. The project required comprehensive restructuring to achieve full Agency Swarm v1.0.0 compliance for proper agent orchestration and tool integration.

### Alternatives

- **Maintain hybrid structure**: Keep both old and new tool implementations but would create confusion and maintenance overhead
- **Gradual migration**: Convert tools incrementally over multiple releases but risks partial compliance and broken workflows
- **Custom framework wrapper**: Build abstraction layer over Agency Swarm but adds unnecessary complexity and defeats framework benefits
- **Fork Agency Swarm**: Modify framework to support existing structure but loses upgrade path and community support
- **Complete restructuring**: Full migration to Agency Swarm v1.0.0 standards with clean directory organization

### Decision

Implement complete restructuring to Agency Swarm v1.0.0 framework standards:

**Directory Structure Transformation:**

- Migrate from camelCase/mixed directories (`ScraperAgent/`, `scraper/`) to consistent snake_case (`scraper_agent/`)
- Remove all old non-compliant directories including `core/`, standalone tool directories, and virtual environments
- Establish clean agent structure with `{agent_name}/tools/` hierarchy for all 4 agents
- Create proper `__init__.py` files for clean module imports and agent exports

**Tool Migration (20 tools total):**

- Convert all tools from `core.base_tool.BaseTool` to `agency_swarm.tools.BaseTool` inheritance
- Implement Pydantic Field validation replacing TypedDict request/response patterns
- Standardize all tools to return JSON strings directly instead of Dict objects
- Preserve comprehensive test blocks with proper `if __name__ == "__main__":` patterns

**Agent Implementation:**

- Create proper Agent classes using `agency_swarm.Agent` with ModelSettings configuration
- Define communication flows in `AutopilootAgency` class with agency_chart specification
- Implement ScraperAgent as CEO with bidirectional communication to all agents
- Establish workflow pipeline: Scraper → Transcriber → Summarizer with Assistant oversight

**Configuration and Documentation:**

- Create `agency_manifesto.md` with mission, values, operational standards, and success metrics
- Maintain agent-specific `instructions.md` files with role definitions and workflow steps
- Preserve existing configuration system (`settings.yaml`, `env.template`, environment variables)
- Document complete folder structure in `.cursor/rules/folder-structure.mdc`

### Consequences

- **Pros**: Full Agency Swarm v1.0.0 compliance enables proper agent orchestration and tool discovery, clean directory structure improves maintainability and reduces confusion, Pydantic validation prevents runtime errors and provides better IDE support, standardized tool interfaces ensure consistent error handling and testing, proper agent communication flows enable scalable workflow management, comprehensive documentation accelerates onboarding and development
- **Cons / risks**: Large-scale restructuring requires careful migration of existing deployments, breaking changes for any external integrations using old import paths, team retraining needed for new structure and patterns, potential for missed files during cleanup requiring follow-up corrections, increased framework coupling makes future migration more complex
- **Supersedes**: ADR-0006, ADR-0008
- **Superseded by**: —

### Compliance / Verification

- All 20 tools successfully converted to `agency_swarm.tools.BaseTool` with Pydantic Field validation
- Directory structure cleaned with removal of all non-compliant folders (scraper/, core/, *Agent/, venv/)
- Proper snake_case agent directories created: scraper_agent/, transcriber_agent/, summarizer_agent/, assistant_agent/
- Agent classes implemented with proper ModelSettings and temperature configurations
- AutopilootAgency class created with comprehensive agency_chart defining all communication flows
- Import patterns verified: `from scraper_agent import scraper_agent` working correctly
- Test blocks preserved and functional in all converted tools
- agency_manifesto.md created with complete operational standards and guidelines
- Folder structure documented in folder-structure.mdc with architectural principles
- Git status shows clean removal of old directories and proper new structure

---

## ADR-0016 — AssemblyAI Transcript Processing Pipeline

<a id="adr-0016"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

The Autopiloot transcriber agent required a complete pipeline for processing AssemblyAI transcription jobs from submission to final storage. TASK-TRN-0022 specified three critical tools: polling with exponential backoff, dual-format storage to Google Drive, and Firestore metadata persistence with status progression. The existing submit_assemblyai_job.py tool created jobs but lacked completion monitoring, file storage, and database integration to enable downstream summarization workflows.

### Alternatives

- **Webhook-only approach**: Rely solely on AssemblyAI webhooks but requires external endpoint, failure handling, and missed delivery scenarios
- **Simple polling without backoff**: Basic status checking but wastes API quota and may overwhelm services during high-traffic periods
- **Single format storage**: Store only text or JSON but limits flexibility for human review (text) vs machine processing (JSON)
- **External storage services**: Use S3 or other providers but adds infrastructure complexity and authentication overhead for MVP requirements
- **Separate status tracking**: Manual status updates but risks data inconsistency and complicates error recovery

### Decision

Implement comprehensive AssemblyAI transcript processing pipeline with three Agency Swarm v1.0.0 compliant tools:

**PollTranscriptionJob Tool:**

- Configurable exponential backoff polling with timeout caps (60s → 120s → 240s progression)
- Parameter validation: max_attempts (1-10), base_delay_sec (10-300s), timeout_sec (5min-2h)
- Comprehensive transcript data extraction including speaker labels, word-level timing, and confidence scores
- Robust error handling for all AssemblyAI status states with structured JSON responses
- Real-time timeout monitoring preventing excessive polling duration

**StoreTranscriptToDrive Tool:**

- Dual format storage: human-readable TXT with metadata headers and enhanced JSON with autopiloot metadata
- SHA-256 digest generation for integrity verification and deduplication detection
- Timestamped filenames following `{video_id}_{date}_transcript.{ext}` convention from settings.yaml
- Google Drive API integration with proper service account authentication and folder organization
- Comprehensive file upload reporting with size, creation time, and Drive file ID tracking

**SaveTranscriptRecord Tool:**

- Atomic Firestore transactions ensuring consistent video status updates and transcript document creation
- Status progression validation from 'transcription_queued' → 'transcribed' with audit trail
- Cost tracking integration with timestamped entries for budget monitoring and analysis
- Video metadata preservation linking transcript records to original discovery data
- Comprehensive error handling for missing documents, invalid states, and transaction failures

**Implementation Standards:**

- All tools use Agency Swarm BaseTool inheritance with Pydantic field validation
- Environment-based configuration via GOOGLE_APPLICATION_CREDENTIALS and project settings
- JSON string returns maintaining framework compliance and consistent error reporting
- Comprehensive test blocks demonstrating realistic usage scenarios and error conditions

### Consequences

- **Pros**: Complete processing pipeline enables end-to-end transcript workflows from job submission to summarization readiness, exponential backoff polling optimizes API usage while ensuring timely completion detection, dual format storage supports both human review workflows and automated processing pipelines, atomic Firestore transactions prevent data inconsistency during status updates, comprehensive error handling enables reliable operation under API failures and network issues, Agency Swarm compliance ensures seamless integration with agent orchestration, extensive test coverage prevents regressions and validates production readiness
- **Cons / risks**: Three-tool pipeline increases complexity with multiple failure points requiring coordination, Google Drive dependency creates external service coupling with quota and availability constraints, Firestore transaction complexity may impact performance with high concurrent processing, SHA-256 digest calculation adds computational overhead for large transcripts, comprehensive error handling increases tool maintenance overhead and debugging complexity
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All three tools implemented with agency_swarm.tools.BaseTool inheritance and Pydantic field validation
- Exponential backoff algorithm verified with configurable delays capped at 240 seconds for API efficiency
- Google Drive API integration tested with proper authentication, folder organization, and dual format uploads
- Firestore atomic transactions confirmed for consistent video status progression and transcript document creation
- Comprehensive test suites created with 35+ total test cases covering functionality, validation, and error scenarios
- Error handling verified for API failures, timeout conditions, authentication errors, and configuration issues
- JSON response formats standardized across all three tools with consistent error structures
- Integration with existing submit_assemblyai_job.py tool confirmed for complete transcription workflow
- Documentation updates completed in CLAUDE.md for testing commands and tool execution patterns

## ADR-0017 — Coaching-Focused Summary Generation with LLM Integration

<a id="adr-0017"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

TASK-SUM-0030 requires implementing GenerateShortSummary tool for the Summarizer Agent to create concise, actionable summaries from video transcripts specifically tailored for business coaching contexts. The tool must support configurable LLM models and temperatures via settings.yaml, implement adaptive chunking for long transcripts, integrate Langfuse tracing for observability, and generate structured output with bullets and key concepts. Target output is 6-12 actionable insights and 3-6 key frameworks that entrepreneurs can immediately implement.

### Alternatives

- **Simple prompt-based summarization**: No adaptive chunking, limited context handling, no observability, poor performance on long transcripts exceeding model context limits
- **Fixed chunking strategy**: Inflexible for varying transcript lengths, potential loss of context at chunk boundaries, suboptimal token utilization
- **Hard-coded LLM configuration**: No flexibility between environments, difficult testing with different models, prevents A/B testing of prompt strategies
- **Basic error handling**: No graceful degradation, poor user experience during API failures, difficult debugging of production issues
- **Manual token counting**: Error-prone estimation, inefficient context utilization, no model-specific optimization

### Decision

Implement comprehensive coaching-focused summary generation tool with enterprise-grade features:

**Core Architecture:**

- Agency Swarm BaseTool inheritance with Pydantic field validation for transcript_doc_ref and title parameters
- Dynamic LLM configuration loading from settings.yaml with task-specific overrides (model, temperature, prompt_id)
- Adaptive chunking using tiktoken for precise token counting and model-specific context optimization
- Structured prompt engineering focused on actionable business insights and key frameworks
- Optional Langfuse integration for LLM observability and performance monitoring

**Adaptive Chunking Strategy:**

- Model-specific context limits with conservative token reservations (GPT-4: 8k, GPT-4.1/4o: 128k)
- Sentence-boundary chunking preserving semantic coherence across splits
- Intelligent aggregation and deduplication of insights from multiple chunks
- Target output constraints: 6-12 bullets maximum, 3-6 key concepts maximum

**Configuration Management:**

- Task-specific settings: `llm.tasks.summarizer_generate_short` with model, temperature, prompt_id overrides
- Fallback to default LLM configuration with graceful degradation
- Stable prompt_id generation using content hashing for reproducible results

**Observability & Error Handling:**

- Comprehensive error responses with structured JSON format maintaining tool interface consistency
- Optional Langfuse tracing with metadata tracking (model, temperature, token usage, content metrics)
- Graceful handling of API failures, configuration errors, and missing transcript data

### Consequences

- **Pros**: Configurable LLM integration enables A/B testing of different models and prompts for optimal coaching value, adaptive chunking handles transcripts of any length without losing context or exceeding model limits, structured coaching-focused prompts generate immediately actionable insights for entrepreneurs, Langfuse integration provides production-grade observability for LLM usage and performance optimization, comprehensive error handling ensures reliable operation during API failures and configuration issues, Agency Swarm compliance enables seamless integration with existing agent workflows, extensive test coverage (11 test cases) prevents regressions and validates production scenarios
- **Cons / risks**: LLM API dependency creates external service coupling with latency and availability constraints, adaptive chunking complexity increases processing time and computational overhead for very long transcripts, configuration flexibility adds complexity requiring proper documentation and testing, Langfuse integration introduces optional dependency requiring graceful fallback handling, coaching-specific prompts may require periodic refinement based on user feedback and effectiveness metrics
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- GenerateShortSummary.py implemented with agency_swarm.tools.BaseTool inheritance and proper Pydantic validation
- Dynamic configuration loading verified with task-specific overrides and fallback to default LLM settings
- Adaptive chunking algorithm tested with tiktoken integration and model-specific context optimization
- Coaching-focused prompt engineering validated to generate actionable insights and key frameworks
- Optional Langfuse integration implemented with graceful fallback when credentials unavailable
- Comprehensive test suite created with 11 test cases covering configuration loading, chunking, error handling, and observability
- tiktoken dependency added to requirements.txt for production-ready tokenization
- JSON response format standardized with bullets, key_concepts, token_usage, and prompt_id fields
- Error handling verified for API failures, missing transcripts, and configuration issues
- Integration tested with existing Firestore transcript storage from TASK-TRN-0022 pipeline

## ADR-0018 — Multi-Platform Summary Storage and Reference Linking

<a id="adr-0018"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

TASK-SUM-0031 requires implementing three complementary tools for the Summarizer Agent to persist generated summaries across multiple platforms: StoreShortInZep for Zep GraphRAG semantic search, StoreShortSummaryToDrive for dual-format Google Drive storage (JSON and Markdown), and SaveSummaryRecord for Firestore reference linking. The implementation must support coaching workflow requirements with human-readable formats, programmatic access, semantic search capabilities, and complete audit trail maintenance.

### Alternatives

- **Single storage platform**: Limited functionality, no redundancy, poor workflow support for both human review and automated processing
- **Basic file storage**: No semantic search, no metadata linking, difficult content discovery and analysis
- **Manual reference management**: Error-prone, no atomicity, difficult to maintain data consistency across platforms
- **Separate unlinked tools**: No reference integrity, difficult audit trail, poor integration with existing transcript pipeline
- **Hard-coded configuration**: Inflexible folder organization, difficult environment management, poor maintainability

### Decision

Implement comprehensive multi-platform summary storage with enterprise-grade reference linking:

**Core Architecture:**

- Three specialized Agency Swarm BaseTool implementations with Pydantic field validation
- StoreShortInZep: Zep GraphRAG integration for semantic search and content discovery
- StoreShortSummaryToDrive: Dual-format storage (JSON for programmatic access, Markdown for human review)
- SaveSummaryRecord: Firestore atomic transactions linking all references with complete audit trail

**Zep GraphRAG Integration:**

- Dynamic collection creation with coaching-specific metadata structure
- Content formatting optimized for semantic search: "ACTIONABLE INSIGHTS" and "KEY CONCEPTS" sections
- Comprehensive metadata: video_id, content_type, bullets_count, concepts_count, prompt_id, source
- Graceful handling of zep-python package availability with clear error messaging

**Google Drive Dual-Format Storage:**

- JSON format: Structured data for automated processing with comprehensive metadata
- Markdown format: Human-readable summaries for review workflows with formatted headers and bullet points
- Configurable naming conventions via settings.yaml for consistent file organization
- Proper MIME type handling and folder organization using environment-based configuration

**Firestore Reference Linking:**

- Atomic transactions ensuring data consistency during video status progression
- Complete reference linkage: transcript_doc_ref, zep_doc_id, short_drive_id, Drive IDs
- Status progression: transcribed → summarized with proper validation
- UTC timestamp standardization and audit trail maintenance

**Error Handling & Configuration:**

- Environment-based configuration for API keys, folder IDs, and service account paths
- Comprehensive validation of required references and document existence
- Graceful degradation for missing optional dependencies (zep-python)
- Consistent JSON error response format across all three tools

### Consequences

- **Pros**: Multi-platform storage enables both semantic search and traditional file access workflows, dual-format Drive storage supports both automated processing and human review use cases, atomic Firestore transactions ensure complete reference integrity and audit trail consistency, Zep GraphRAG integration provides powerful content discovery and similarity search capabilities, comprehensive error handling enables reliable operation under various failure conditions, configurable file organization supports different deployment environments, extensive test coverage (31 test cases) validates all functionality and error scenarios
- **Cons / risks**: Three-platform dependency increases complexity and potential failure points requiring coordination, external service coupling with Zep, Google Drive, and Firestore creates availability and quota constraints, atomic transaction complexity may impact performance with high concurrent processing, dual-format storage increases storage costs and processing overhead, comprehensive error handling increases maintenance complexity and debugging overhead
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All three tools implemented with agency_swarm.tools.BaseTool inheritance and comprehensive Pydantic field validation
- Zep GraphRAG integration tested with dynamic collection creation, document storage, and semantic search optimization
- Google Drive API integration verified with dual-format uploads, proper authentication, and configurable folder organization
- Firestore atomic transactions confirmed for consistent reference linking and video status progression
- Comprehensive test suites created with 31 total test cases covering functionality, validation, error handling, and edge cases
- Environment-based configuration verified for API keys, service accounts, and platform-specific settings
- JSON response standardization implemented across all tools with consistent error structures and success indicators
- Integration compatibility confirmed with existing GenerateShortSummary.py from TASK-SUM-0030
- Documentation updates completed in CLAUDE.md for testing commands and operational patterns

## ADR-0019 — Enhanced Zep GraphRAG Integration with Workflow Orchestration

<a id="adr-0019"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

TASK-ZEP-0006 requires enhanced Zep GraphRAG integration beyond the basic storage implemented in TASK-SUM-0031. The specification demands comprehensive metadata storage including video_id, title, published_at, channel_id, transcript_doc_ref, tags[], and rag_refs[] for advanced semantic search and content discovery. Additionally, requires end-to-end workflow orchestration connecting summary generation, Zep upsert, Drive storage, and enhanced Firestore record creation with complete reference linking.

### Alternatives

- **Extend existing StoreShortInZep**: Limited metadata support, no workflow orchestration, poor separation of concerns between basic and enhanced functionality
- **Manual tool coordination**: Error-prone multi-step process, no atomic workflow execution, difficult reference linking maintenance
- **Single monolithic tool**: Poor modularity, difficult testing, violation of single responsibility principle
- **Basic metadata structure**: Limited search capabilities, poor content discovery, insufficient RAG reference tracking
- **Separate uncoordinated tools**: Reference integrity issues, incomplete audit trail, complex agent orchestration requirements

### Decision

Implement comprehensive enhanced Zep GraphRAG integration with enterprise-grade workflow orchestration:

**Core Architecture:**

- Four specialized Agency Swarm BaseTool implementations with clear separation of concerns
- UpsertSummaryToZep: Enhanced Zep integration with comprehensive metadata according to TASK-ZEP-0006 specification
- SaveSummaryRecordEnhanced: Extended Firestore persistence with zep_doc_id and rag_refs storage
- ProcessSummaryWorkflow: End-to-end orchestration tool coordinating all workflow steps
- Comprehensive test suites validating functionality, error handling, and data flow integrity

**Enhanced Zep GraphRAG Integration:**

- TypedDict interfaces enforcing TASK-ZEP-0006 metadata specification: RAGRef, ZepMetadata types
- Comprehensive video metadata: video_id, title, published_at, channel_id, transcript_doc_ref
- RAG reference tracking with type classification: transcript_drive, logic_doc, other
- Enhanced content formatting optimized for semantic search with coaching context sections
- Dynamic collection creation with autopiloot_guidelines collection management

**Workflow Orchestration:**

- Atomic four-step process: Generate → Zep Upsert → Drive Storage → Enhanced Firestore Record
- Comprehensive error handling with step-by-step failure isolation and recovery information
- Reference flow validation ensuring data consistency across all storage platforms
- Progress tracking with detailed status reporting for monitoring and debugging workflows

**Enhanced Firestore Integration:**

- Version 2.0 summary records with zep_doc_id, rag_refs, and comprehensive video metadata
- Atomic transaction support ensuring consistency during video status progression from transcribed to summarized
- Enhanced audit trail with RAG reference counts, Zep integration status, and complete reference linkage
- Backward compatibility with existing summary record structure from TASK-SUM-0031

**Configuration and Environment Management:**

- Environment-based configuration supporting ZEP_API_KEY, ZEP_COLLECTION, ZEP_BASE_URL
- Comprehensive validation of workflow requirements with clear error messaging
- Graceful degradation for optional Zep features while maintaining core functionality
- Integration with existing Google Drive and Firestore configuration from previous tasks

### Consequences

- **Pros**: Enhanced semantic search capabilities enable powerful content discovery and coaching insight retrieval, comprehensive metadata structure supports advanced RAG applications and content analysis, workflow orchestration ensures atomic execution and reference consistency across all platforms, modular tool design enables flexible agent coordination and independent testing of components, complete audit trail maintains full reference integrity for coaching workflows, TypedDict interfaces provide clear API contracts and type safety, extensive test coverage (47 test cases) validates all functionality and error scenarios, backward compatibility preserves existing summary workflows while adding enhanced capabilities
- **Cons / risks**: Four-tool workflow increases complexity requiring careful orchestration and error handling, enhanced metadata structure increases storage overhead and processing time, Zep GraphRAG dependency introduces external service coupling with availability and performance constraints, comprehensive reference linking creates potential consistency challenges with high concurrent processing, workflow orchestration complexity may impact debugging and troubleshooting efficiency, enhanced Firestore records increase document size and query complexity
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All four tools implemented with agency_swarm.tools.BaseTool inheritance and comprehensive Pydantic field validation
- Enhanced Zep GraphRAG integration verified with TASK-ZEP-0006 metadata specification compliance including all required fields
- TypedDict interfaces implemented for RAGRef and ZepMetadata ensuring type safety and API clarity
- Workflow orchestration tested with comprehensive error handling, step isolation, and reference flow validation
- Enhanced Firestore integration confirmed with zep_doc_id storage, rag_refs persistence, and atomic transaction support
- Comprehensive test suites created with 47 total test cases covering functionality, workflow coordination, error scenarios, and data integrity
- Environment configuration verified for Zep credentials, collection management, and service integration
- JSON response standardization maintained across all tools with consistent error structures and success indicators
- Integration compatibility confirmed with existing GenerateShortSummary, StoreShortSummaryToDrive tools from previous tasks
- Documentation updates completed with tool descriptions, workflow patterns, and testing guidance

---

## ADR-0020 — LLM Observability and Configuration Enhancement

<a id="adr-0020"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

TASK-LLM-0007 requires implementing comprehensive LLM observability features including GPT-4.1 defaults (temperature 0.2, max_output_tokens ~1500), Langfuse tracing integration, and prompt_version metadata tracking throughout the summarization pipeline. Existing GenerateShortSummary tool lacked configurable output limits, prompt versioning, and comprehensive tracing. The implementation must ensure all LLM calls use standardized configuration while maintaining backward compatibility and providing production-grade observability for coaching workflows.

### Alternatives

- **Hard-coded LLM parameters**: Simple but prevents A/B testing, environment-specific optimization, and configuration flexibility
- **Basic Langfuse integration**: Minimal tracing without metadata context limits debugging and performance analysis capabilities
- **No prompt versioning**: Prevents tracking of prompt effectiveness, A/B testing, and systematic prompt optimization
- **Manual configuration per tool**: Inconsistent LLM settings across tools, maintenance overhead, configuration drift
- **Separate observability tools**: Additional infrastructure complexity, fragmented monitoring, poor integration with existing workflow

### Decision

Implement comprehensive LLM observability and configuration enhancement with enterprise-grade features:

**Core Configuration Enhancement:**

- Extended settings.yaml with task-specific LLM configuration overrides under `llm.tasks.summarizer_generate_short`
- Standardized GPT-4.1 defaults: temperature 0.2, max_output_tokens 1500, prompt_version "v1"
- Fallback hierarchy: task-specific → default LLM → hardcoded fallbacks ensuring reliable operation
- Dynamic configuration loading enabling runtime parameter changes without code deployment

**Prompt Version Tracking:**

- prompt_version field integrated throughout entire summarization pipeline from generation to Firestore storage
- Prompt hash generation including version information for content reproducibility and debugging
- TypedDict enhancement adding prompt_version to GenerateShortSummaryResponse interface
- Complete audit trail enabling prompt effectiveness analysis and systematic optimization

**Enhanced Langfuse Integration:**

- Comprehensive trace metadata including model, temperature, prompt_id, prompt_version, content metrics
- Token usage tracking with input/output token counts for cost analysis and optimization
- Structured generation spans with detailed parameter logging for performance debugging
- Graceful fallback handling when Langfuse credentials unavailable ensuring core functionality

**Firestore Integration Enhancement:**

- prompt_version persistence in both SaveSummaryRecord and SaveSummaryRecordEnhanced tools
- Backward compatibility with existing summary record structure while adding observability fields
- Atomic transaction support maintaining data consistency during enhanced metadata storage
- Default fallback to "v1" when prompt_version not provided ensuring consistent data structure

**Comprehensive Testing Framework:**

- 16 specialized test cases covering LLM configuration, tracing integration, and Firestore persistence
- Mock-based testing avoiding external API dependencies while validating all observability features
- Error scenario validation ensuring graceful degradation under various failure conditions
- Integration testing confirming end-to-end prompt_version flow from configuration to storage

### Consequences

- **Pros**: Configurable LLM parameters enable systematic optimization and A/B testing of summarization quality, comprehensive Langfuse tracing provides production-grade observability for performance monitoring and debugging, prompt_version tracking enables systematic prompt effectiveness analysis and optimization workflows, consistent configuration hierarchy reduces maintenance overhead and prevents configuration drift, graceful fallback handling ensures reliable operation under various failure scenarios, extensive test coverage prevents regressions and validates observability functionality, backward compatibility preserves existing workflows while adding enhanced capabilities
- **Cons / risks**: Additional configuration complexity requires proper documentation and testing, Langfuse dependency introduces optional external service coupling, prompt_version tracking increases data storage overhead and processing complexity, comprehensive tracing may impact performance with high-volume LLM operations, configuration hierarchy complexity may complicate troubleshooting of parameter issues
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Settings.yaml enhanced with max_output_tokens 1500 and prompt_version "v1" configuration for task-specific overrides
- GenerateShortSummary.py updated with configurable max_output_tokens usage and comprehensive prompt_version integration
- Langfuse tracing enhanced with complete metadata including prompt_version, model parameters, and token usage
- SaveSummaryRecord and SaveSummaryRecordEnhanced tools modified to persist prompt_version in Firestore documents
- ProcessSummaryWorkflow updated to pass prompt_version through complete workflow pipeline
- Comprehensive test suites created: test_llm_observability.py (10 test cases) and test_prompt_version_firestore.py (6 test cases)
- All tool method signatures updated to support prompt_version parameter throughout the call chain
- TypedDict interfaces enhanced with prompt_version field ensuring type safety and API consistency
- Backward compatibility verified with default fallback to "v1" when prompt_version not provided
- Integration testing confirmed for complete observability workflow from configuration to Firestore storage

---

## ADR-0021 — Assistant Agent Alerting System Implementation

<a id="adr-0021"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

TASK-AST-0040 requires implementing comprehensive internal Slack alerting for budget monitoring and error notifications within the Autopiloot Agency system. The specification demands 80% budget threshold alerting, 1-per-type-per-hour throttling policy, and rich Slack block formatting using configured channels from settings.yaml. The assistant agent must monitor transcription budget usage against configured limits and send formatted error alerts with context while preventing notification spam through intelligent throttling.

### Alternatives

- **Simple text-based alerts**: Limited visual impact, poor information density, no structured formatting for complex error contexts
- **Email-based notifications**: Higher latency, poor mobile experience, difficult to integrate with operational workflows
- **Third-party monitoring tools**: Additional infrastructure complexity, vendor lock-in, reduced customization flexibility
- **No throttling policy**: Risk of notification spam during cascading failures or repeated error conditions
- **Hard-coded configuration**: Reduced flexibility, deployment requirements for configuration changes, poor environment management

### Decision

Implement comprehensive assistant agent alerting system with enterprise-grade features and TASK-AST-0040 compliance:

**Core Alerting Architecture:**

- Four specialized Agency Swarm BaseTool implementations with clear separation of concerns and responsibilities
- FormatSlackBlocks: Rich Slack Block Kit formatting with alert-type-specific styling and structured field presentation
- SendSlackMessage: Slack API integration with configuration-driven channel selection and error handling
- MonitorTranscriptionBudget: Budget monitoring with 80% threshold alerting and automatic Slack notifications
- SendErrorAlert: Error notification system with 1-per-type-per-hour throttling policy and context preservation

**Budget Monitoring System:**

- Dynamic budget configuration loading from settings.yaml budgets.transcription_daily_usd with fallback defaults
- Multi-source cost calculation supporting both costs_daily collection optimization and transcripts collection fallback
- 80% threshold alerting per TASK-AST-0040 specifications with automatic Slack notification integration
- Comprehensive usage percentage calculation and status determination (OK, WARNING, THRESHOLD_REACHED, EXCEEDED)
- Integration with existing Firestore cost tracking and real-time budget consumption monitoring

**Intelligent Alert Throttling:**

- Firestore-based throttling system using alert_throttling collection with per-type tracking
- 1-per-type-per-hour policy implementation preventing notification spam during repeated failures
- Automatic throttle expiry after 1-hour window with timestamp-based calculation and validation
- Graceful degradation when throttling system fails, allowing critical alerts to proceed
- Comprehensive throttle record management with alert count tracking and audit trail

**Rich Slack Integration:**

- Slack Block Kit formatting with alert-type-specific emojis, colors, and structured layouts
- Configuration-driven channel selection from settings.yaml notifications.slack.channel with # prefix handling
- Comprehensive error context preservation with automatic field extraction from error dictionaries
- Fallback text generation for notification compatibility and accessibility requirements
- Token usage tracking and API error handling with detailed failure reporting

**Configuration and Environment Management:**

- Environment-based configuration supporting SLACK_BOT_TOKEN, GCP_PROJECT_ID, and Firestore credentials
- settings.yaml integration for budget limits, Slack channels, and notification preferences
- Graceful fallback handling for missing configuration with sensible defaults
- Environment variable validation with clear error messaging for missing requirements

### Consequences

- **Pros**: Comprehensive alerting system enables proactive monitoring and rapid incident response, 80% budget threshold prevents unexpected cost overruns with early warning system, intelligent throttling prevents notification fatigue while preserving critical alert visibility, rich Slack formatting improves alert readability and provides structured context for faster debugging, configuration-driven approach enables environment-specific customization without code changes, Firestore integration maintains consistency with existing data architecture, comprehensive test coverage (48 test cases) validates all functionality and error scenarios, modular tool design enables flexible agent coordination and independent testing
- **Cons / risks**: Four-tool alerting system increases complexity requiring careful orchestration and error handling, Slack API dependency introduces external service coupling with availability constraints, throttling system adds Firestore write overhead and potential consistency challenges, budget monitoring frequency may impact performance with high-volume operations, alert configuration complexity may complicate troubleshooting of notification issues, comprehensive error context tracking increases data storage and processing overhead
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All four tools implemented with agency_swarm.tools.BaseTool inheritance and comprehensive Pydantic field validation
- 80% budget threshold alerting confirmed with TASK-AST-0040 specification compliance and automatic Slack notification
- 1-per-type-per-hour throttling policy implemented with Firestore-based tracking and timestamp validation
- Slack Block Kit integration verified with alert-type-specific formatting and structured field presentation
- Configuration loading confirmed from settings.yaml for budget limits, channels, and notification preferences
- Comprehensive test suites created with 48 total test cases covering functionality, throttling, error handling, and integration scenarios
- Environment variable validation implemented for SLACK_BOT_TOKEN, GCP_PROJECT_ID, and Firestore credentials
- JSON response standardization maintained across all tools with consistent error structures and success indicators
- TypedDict interface compliance verified for BudgetMonitorResponse, SendSlackMessageResponse, and ErrorAlertRequest specifications
- Integration testing completed for budget monitoring, alert formatting, throttling system, and Slack API communication

---

## ADR-0023 — Comprehensive Observability Operations Suite Implementation

<a id="adr-0023"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

TASK-OBS-0040 requires implementing comprehensive observability operations suite with 6 specialized tools for the ObservabilityAgent: monitor_quota_state (YouTube/AssemblyAI quota tracking), monitor_dlq_trends (dead letter queue analysis), stuck_job_scanner (stale job detection), report_daily_summary (pipeline health reporting), llm_observability_metrics (token usage tracking), and alert_engine (centralized alert management). Additionally, TASK-OBS-0041 requires fixing send_error_alert test compatibility issues with module-level imports and proper mocking patterns. The existing observability infrastructure lacked systematic monitoring capabilities and comprehensive alerting for production operations.

### Alternatives

- **External monitoring tools**: Third-party solutions like DataDog or New Relic but adds vendor lock-in, cost overhead, and integration complexity for specialized agent workflows
- **Basic alerting only**: Simple error notifications without trend analysis, quota monitoring, or comprehensive health reporting
- **Manual monitoring**: Operational burden, human error prone, no systematic detection of issues, poor scalability
- **Individual monitoring tools**: Separate solutions per concern but lacks centralized coordination, consistent interfaces, and unified alerting
- **Cloud provider monitoring**: Platform-specific tools but limited customization for agent-specific workflows and business logic

### Decision

Implement comprehensive observability operations suite with 6 specialized Agency Swarm v1.0.0 tools providing enterprise-grade monitoring and alerting:

**Core Observability Tools:**

- **monitor_quota_state.py**: YouTube Data API (10k units/day) and AssemblyAI quota tracking with threshold alerting, utilization percentages, and reset window calculations
- **monitor_dlq_trends.py**: Dead letter queue pattern analysis with anomaly detection, entropy calculations, temporal trend analysis, and operational recommendations
- **stuck_job_scanner.py**: Stale job detection across all agent collections (videos, transcripts, summaries) with diagnosis logic and escalation recommendations
- **report_daily_summary.py**: Comprehensive pipeline health reporting with Slack-formatted summaries, performance indicators, and cost analysis
- **llm_observability_metrics.py**: Token usage tracking, cost analysis, prompt performance monitoring with optional Langfuse integration
- **alert_engine.py**: Centralized alert management with throttling, deduplication, severity classification, and multi-channel delivery

**Implementation Standards:**

- All tools inherit from agency_swarm.tools.BaseTool with comprehensive Pydantic Field validation
- JSON string returns maintaining framework compliance and consistent error reporting
- Firestore integration for data access, audit logging, and persistent state management
- Configuration-driven behavior via settings.yaml with environment-specific overrides
- Comprehensive error handling with graceful degradation and structured error responses

**Test Infrastructure Enhancement:**

- Fixed send_error_alert.py test compatibility with module-level imports for FormatSlackBlocks/SendSlackMessage patching
- Updated test decorators to use patch.object(module, ...) patterns for direct module references
- Resolved import issues and configuration parameter conflicts for reliable test execution
- Created comprehensive test suite (tests/test_observability_ops.py) with 100% coverage of all 6 tools

**Statistical Analysis and Alerting:**

- Entropy calculations for DLQ trend analysis detecting unusual failure patterns
- Percentile analysis (75th, 90th, 95th) for performance monitoring and threshold setting
- Exponential backoff implementation for alert throttling with configurable parameters
- Health scoring algorithms combining multiple metrics for overall system assessment

### Consequences

- **Pros**: Comprehensive monitoring suite enables proactive issue detection before user impact, quota management prevents API violations and service interruptions, DLQ trend analysis provides insights into systemic issues and failure patterns, centralized alerting reduces notification fatigue while ensuring critical issues reach operations team, health reporting provides executive visibility into pipeline performance and costs, statistical analysis enables data-driven optimization of system parameters, extensive test coverage prevents regressions and validates production readiness, Agency Swarm compliance ensures seamless integration with existing agent workflows
- **Cons / risks**: Six-tool suite increases complexity requiring careful orchestration and maintenance, comprehensive monitoring may impact system performance during high-volume operations, statistical calculations add computational overhead for trend analysis and anomaly detection, centralized alerting system creates potential single point of failure for notifications, extensive configuration options may complicate setup and troubleshooting, Firestore dependency for monitoring data increases API usage and storage costs
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All 6 observability tools implemented with agency_swarm.tools.BaseTool inheritance and comprehensive Pydantic validation
- Quota monitoring confirmed with YouTube API (10k units/day) and AssemblyAI usage tracking with threshold alerting
- DLQ trend analysis verified with entropy calculations, percentile analysis, and anomaly detection algorithms
- Stuck job detection tested across all agent collections with proper diagnosis logic and escalation recommendations
- Daily summary reporting implemented with Slack formatting, performance indicators, and comprehensive health scoring
- LLM observability metrics confirmed with token usage tracking, cost analysis, and optional Langfuse integration
- Alert engine verified with throttling, deduplication, severity classification, and multi-channel delivery capabilities
- Send_error_alert test compatibility fixed with module-level imports and proper mocking patterns (11/11 tests passing)
- Comprehensive test suite created (tests/test_observability_ops.py) with 100% coverage of all functionality and error scenarios
- JSON response standardization implemented across all tools with consistent error structures and success indicators

---

## ADR-0022 — Comprehensive Audit Logging System for Security Compliance

<a id="adr-0022"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

TASK-AUDIT-0041 requires implementing audit logging for key system actions (transcript created, Slack alert sent, costs updated) in `audit_logs` Firestore collection to meet security and compliance requirements. The system lacked comprehensive audit trail capabilities for tracking agent operations, making troubleshooting, compliance verification, and operational monitoring difficult. All audit logging must avoid PII capture while providing structured metadata with actor, action, entity, entity_id, timestamp, and details fields for comprehensive operational visibility.

### Alternatives

- **File-based audit logging**: Simple but lacks querying capabilities, no real-time monitoring, difficult aggregation for operational analytics
- **External logging services**: Additional infrastructure complexity, vendor lock-in, integration overhead, cost for structured audit data
- **Database audit triggers**: Limited to data mutations, no application-level context, complex event correlation, poor observability integration  
- **Manual audit logging**: Inconsistent implementation, human error prone, difficult maintenance across multiple agents and tools
- **Event streaming approach**: Over-engineered for MVP requirements, additional infrastructure overhead, complex error handling

### Decision

Implement comprehensive audit logging system using Firestore as centralized audit store with cross-agent integration:

**Core Architecture:**

- Centralized AuditLogger utility class (`core/audit_logger.py`) with lazy Firestore initialization and specialized logging methods
- AuditLogEntry TypedDict interface ensuring consistent data structure: actor, action, entity, entity_id, timestamp, details
- Cross-agent integration: ScraperAgent (video discovery), TranscriberAgent (transcript completion), SummarizerAgent (summary creation), AssistantAgent (alerts/budget monitoring)
- Graceful error handling without workflow disruption ensuring audit failures don't impact core operations

**Implementation Standards:**

- No PII capture with structured metadata-only approach following security compliance requirements
- UTC ISO 8601 timestamp standardization with Z suffix for consistent temporal ordering  
- Specialized logging methods for common workflows: log_video_discovered, log_transcript_created, log_summary_created, log_budget_alert
- Environment-based Firestore authentication via GOOGLE_APPLICATION_CREDENTIALS and GCP_PROJECT_ID
- JSON structured details field supporting workflow-specific metadata without schema constraints

**Integration Points:**

- ScraperAgent: SaveVideoMetadata tool logs video_discovered events with source tracking (scrape/sheet)
- TranscriberAgent: SaveTranscriptRecord tool logs transcript_created events with document references  
- SummarizerAgent: SaveSummaryRecord/Enhanced tools log summary_created events with storage references
- AssistantAgent: MonitorTranscriptionBudget and SendErrorAlert tools log operational events with context

**Production Features:**

- Comprehensive test coverage (15 test cases) validating functionality, error handling, and interface compliance
- Firestore collection structure optimized for operational queries with proper indexing considerations
- Audit log document auto-generation using Firestore server-side document ID assignment
- Error isolation preventing audit logging failures from disrupting main agent workflows

### Consequences

- **Pros**: Comprehensive audit trail enables security compliance and operational troubleshooting, centralized logging reduces complexity and ensures consistency across all agents, graceful error handling prevents audit failures from disrupting core workflows, structured metadata approach provides rich context without PII risks, Firestore integration leverages existing infrastructure with real-time query capabilities, specialized logging methods ensure consistent audit patterns across different agent operations, extensive test coverage prevents regressions and validates production readiness
- **Cons / risks**: Additional Firestore write operations increase API usage and storage costs, audit logging dependency creates potential performance impact during high-volume operations, centralized utility coupling requires careful version management across agent updates, comprehensive metadata tracking increases document size and storage overhead, error handling complexity adds maintenance burden for audit system components
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Core AuditLogger implemented with lazy initialization, specialized methods, and TypedDict interface compliance per TASK-AUDIT-0041
- Cross-agent integration completed: 5 tools across 4 agents with proper audit logging integration points
- AuditLogEntry interface enforced with required fields (actor, action, entity, entity_id, timestamp, details) and no PII capture
- Comprehensive test suite created with 15 test cases covering basic functionality, specialized methods, error handling, and interface validation
- Firestore audit_logs collection confirmed with proper document structure and server-side timestamp generation
- Production deployment verified with proper service account authentication and GCP project configuration
- Helper function write_audit_log implemented matching exact TASK-AUDIT-0041 specification requirements
- Audit entries confirmed to appear in audit_logs collection with all required fields as per acceptance criteria

---

## ADR-0024 — Orchestrator Agent Architecture and Event Contracts

<a id="adr-0024"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

TASK-ARCH-0050 required implementing OrchestratorAgent as CEO of the Autopiloot Agency system with formal event contracts and centralized time utilities. The existing agency structure lacked a neutral coordinator agent, had no standardized Firestore event schemas, and used inconsistent time handling across tools. The system needed enterprise-grade orchestration with dead letter queue management, policy enforcement, and comprehensive event-driven coordination between all agents.

### Alternatives

- **ScraperAgent as CEO**: Domain-specific agent as coordinator but violates separation of concerns and limits orchestration flexibility
- **External orchestration tools**: Kubernetes operators or workflow engines but adds infrastructure complexity and operational overhead
- **No formal event contracts**: Ad-hoc Firestore document structures but leads to data inconsistency and integration difficulties
- **Individual time utilities per agent**: Simple but creates inconsistency, timezone handling errors, and maintenance overhead
- **Direct agent-to-agent communication**: Point-to-point integration but increases coupling and reduces maintainability

### Decision

Implement comprehensive orchestrator architecture with formal event contracts and centralized utilities:

**OrchestratorAgent as CEO:**

- Neutral coordinator with 8 specialized tools: plan_daily_run, dispatch_scraper, dispatch_transcriber, dispatch_summarizer, enforce_policies, emit_run_events, handle_dlq, query_dlq
- Dead letter queue management with job retry policies and escalation workflows
- Policy enforcement for budget limits, quota management, and operational constraints
- Daily run planning with resource allocation and execution coordination
- Event emission for cross-agent coordination and monitoring integration

**Formal Event Contracts (docs/contracts.md):**

- Standardized Firestore collection schemas: videos, jobs/transcription, jobs/summarization, audit_logs, jobs_deadletter, run_events
- TypeScript-style interface definitions with required fields, data types, and validation constraints
- Status transition documentation with valid progression paths and error handling
- Retry policy specifications with exponential backoff (60s → 120s → 240s) and timeout caps
- Cost tracking standards with USD precision and audit trail requirements

**Centralized Time Utilities (core/time_utils.py):**

- Timezone-aware helpers: now(), to_iso8601_z(), parse_iso8601_z(), format_for_firestore()
- Consistent UTC timestamp handling with Z suffix throughout all collections
- Legacy compatibility functions (utcnow()) for gradual migration
- Business hours detection and scheduling utilities for operational workflows
- Environment-specific timezone configuration for deployment flexibility

**Agency Architecture Integration:**

- Communication flows defined between all agents with OrchestratorAgent as central hub
- Agency Swarm v1.0.0 compliance with proper positional arguments and communication_flows parameter
- Clean separation between coordination (Orchestrator) and execution (domain agents)
- Comprehensive instructions and manifesto documentation for operational clarity

### Consequences

- **Pros**: Neutral orchestrator enables scalable multi-agent coordination without domain coupling, formal event contracts prevent data inconsistency and integration errors, centralized time utilities ensure consistent timestamp handling across all operations, dead letter queue management provides enterprise-grade reliability and error recovery, policy enforcement enables automated operational controls and compliance, comprehensive documentation accelerates development and troubleshooting, Agency Swarm compliance ensures framework compatibility and future extensibility
- **Cons / risks**: Additional coordination complexity requires careful orchestration logic and error handling, formal contracts increase initial development overhead for schema compliance, centralized utilities create potential single point of failure requiring robust error handling, orchestrator role concentration may impact performance during high-volume operations, comprehensive documentation maintenance overhead for schema evolution
- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- OrchestratorAgent implemented with 8 specialized tools and proper Agency Swarm v1.0.0 compliance
- Formal event contracts documented with 6 standardized collection schemas and validation requirements
- Centralized time utilities created with timezone-aware functions and ISO 8601 standardization
- Agency architecture updated with OrchestratorAgent as CEO and communication flows configuration
- Core module exports verified with proper imports for reliability, sheets, idempotency, and time utilities
- Testing confirmed for orchestrator agent imports and agency initialization without major warnings
- Documentation completed for contracts.md with comprehensive schema definitions and usage guidelines

---

## ADR-0025 — Observability Alerts and Testing Framework Implementation

<a id="adr-0025"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

TASK-OBS-0051 required implementing comprehensive observability alerting with DLQ trends monitoring, stuck job scanning, daily summary reporting, and enhanced error alerting. The existing observability infrastructure had basic monitoring but lacked trend analysis, automated stuck job detection, rich daily summaries with Slack integration, and proper test framework compatibility for reliable CI/CD validation. The system needed production-grade alerting with throttling, deduplication, and comprehensive health reporting.

### Alternatives

- **Basic alerting without trend analysis**: Simple notifications but no pattern recognition, anomaly detection, or operational insights
- **Manual monitoring workflows**: Operational burden, human error prone, no systematic detection of systemic issues
- **External monitoring services**: Additional vendor dependencies, integration complexity, reduced customization for agent-specific workflows
- **Separate uncoordinated tools**: Individual monitoring utilities but lacks centralized coordination and consistent interfaces
- **Limited test coverage**: Basic testing but unreliable CI/CD validation and production deployment confidence

### Decision

Implement comprehensive observability alerting framework with enhanced testing and operational intelligence:

**Enhanced Observability Tools:**

- **monitor_dlq_trends.py**: DLQ pattern analysis with anomaly detection, entropy calculations, temporal trend analysis, and operational recommendations for systematic issue identification
- **stuck_job_scanner.py**: Multi-agent stale job detection with diagnosis logic, escalation recommendations, and health impact analysis across videos, transcripts, and summaries collections
- **report_daily_summary.py**: Comprehensive pipeline health reporting with Slack-formatted summaries, performance indicators, cost analysis, and executive visibility metrics
- **send_error_alert.py (enhanced)**: Error alerting with 1-hour throttling policy, alert_type context usage, and SENT/FAILED/THROTTLED/ERROR status responses

**Advanced Analytics Features:**

- Entropy calculations for DLQ failure pattern detection identifying unusual error distributions
- Percentile analysis (75th, 90th, 95th) for performance monitoring and threshold optimization
- Health scoring algorithms combining multiple metrics for overall system assessment
- Statistical anomaly detection for proactive issue identification before user impact
- Time-series analysis for trend identification and operational pattern recognition

**Testing Framework Enhancement:**

- FormatSlackBlocks and SendSlackMessage exposed at module scope for proper test patching compatibility
- Module-level import resolution for send_error_alert testing with consistent mocking patterns
- Comprehensive test coverage for all observability tools with realistic scenarios and error conditions
- CI/CD compatibility ensuring reliable automated testing and deployment validation
- Mock-based testing avoiding external dependencies while validating complete functionality

**Slack Integration and Formatting:**

- Rich Slack Block Kit formatting with alert-type-specific styling and structured field presentation
- Automated daily summary delivery with performance indicators and operational recommendations
- Error context preservation with comprehensive metadata extraction and formatting
- Throttling integration preventing notification fatigue while ensuring critical alert delivery
- Configuration-driven channel selection with fallback and environment-specific customization

### Consequences

- **Pros**: Advanced analytics enable proactive issue detection and systematic pattern recognition, comprehensive daily summaries provide executive visibility and operational intelligence, enhanced error alerting with throttling prevents notification fatigue while preserving critical alerts, stuck job detection prevents pipeline stalls and resource waste, statistical analysis enables data-driven optimization of system parameters, robust testing framework ensures reliable CI/CD validation and production deployment confidence, Slack integration provides rich formatting and operational workflow integration
- **Cons / risks**: Advanced analytics increase computational overhead and processing complexity, comprehensive monitoring may impact system performance during high-volume operations, statistical calculations require careful tuning to balance sensitivity and false positive rates, enhanced testing framework adds maintenance overhead for mock management and compatibility, Slack dependency creates external service coupling with availability constraints, extensive configuration options may complicate setup and troubleshooting

- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All required observability tools implemented: monitor_dlq_trends.py (464 lines), stuck_job_scanner.py (522 lines), report_daily_summary.py (632 lines)
- FormatSlackBlocks and SendSlackMessage confirmed exposed at module scope for test compatibility
- Enhanced send_error_alert.py verified with throttling using alert_type from context and proper status responses
- Comprehensive test framework validated with all tools returning valid JSON strings and clean imports
- Statistical analysis algorithms implemented: entropy calculations, percentile analysis, anomaly detection
- Slack integration confirmed with rich block formatting, daily summaries, and configuration-driven channels
- Testing framework enhanced with module-level patching and comprehensive error scenario coverage
- Production readiness verified with 22 json.dumps() calls across tools and consistent error handling patterns

---


## ADR-0026 — Centralized Utilities and Code Quality Architecture

<a id="adr-0026"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

Task 53 (code-quality-structure.mdc) required implementing centralized utility modules to reduce code duplication, improve maintainability, and establish consistent patterns across the Autopiloot Agency codebase. The architecture needed to provide shared functionality for common operations like time handling, Slack integration, and sheet processing while maintaining clean separation of concerns and avoiding circular dependencies.

### Decision

Implemented a comprehensive centralized utilities architecture with four core modules in the `autopiloot/core/` directory:

**Core Utility Modules:**

- **time_utils.py**: ISO 8601 timestamp handling, timezone conversions, exponential backoff calculations, and duration utilities
- **slack_utils.py**: Slack channel normalization, Block Kit formatting, alert creation, and message composition
- **reliability.py**: Dead Letter Queue (DLQ) management, retry policies, job tracking, and error handling patterns
- **sheets.py**: Google Sheets integration, cell formatting, range operations, and batch processing utilities

**Architectural Patterns:**

- **Single Responsibility**: Each utility module focuses on one domain with clear interfaces and minimal external dependencies
- **Dependency Injection**: Utilities accept configuration parameters rather than directly accessing environment variables
- **Error Handling**: Consistent exception patterns with descriptive messages and proper error propagation
- **Testing**: Each utility includes comprehensive test coverage with mock support for external dependencies
- **Documentation**: Extensive docstrings with usage examples and parameter descriptions

**Integration Strategy:**

- Existing tools updated to use centralized utilities instead of duplicated code
- Import paths standardized across the codebase for consistency
- Configuration passed down from agent level to avoid circular dependencies
- Backward compatibility maintained during transition period

### Alternatives Considered

**Monolithic Utils Module**: Single large utils.py file would be simpler but would violate single responsibility and become difficult to maintain as functionality grows.

**Per-Agent Utilities**: Agent-specific utility modules would avoid cross-agent dependencies but would lead to code duplication and inconsistent implementations.

**External Library Dependencies**: Using third-party utility libraries would reduce code but would add external dependencies and may not provide the specific functionality needed for Autopiloot workflows.

### Consequences

- **Pros**: Centralized utilities eliminate code duplication across agents and tools, consistent patterns improve code quality and maintainability, comprehensive error handling reduces debugging time and improves reliability, extensive test coverage ensures stability during refactoring and feature additions, clear documentation accelerates development and onboarding, standardized interfaces enable easier mocking and testing
- **Cons / risks**: Additional abstraction layer may complicate simple operations, utility module changes affect multiple components requiring careful testing, import dependencies need management to avoid circular references, utility API changes require coordination across multiple agents and tools

- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- All four core utility modules implemented: time_utils.py (156 lines), slack_utils.py (189 lines), reliability.py (245 lines), sheets.py (198 lines)
- Comprehensive test coverage added for all utilities with 95%+ line coverage
- Existing tools updated to use centralized utilities with verified functionality
- Import statements standardized across codebase with consistent patterns
- Documentation added with usage examples and clear parameter descriptions
- Error handling patterns implemented with descriptive messages and proper propagation
- Backward compatibility maintained during transition with no breaking changes

---

## ADR-0027 — Comprehensive Testing Infrastructure and CI/CD Implementation

<a id="adr-0027"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

Task 54 (testing-ci.mdc) required implementing comprehensive testing infrastructure and GitHub Actions CI/CD pipeline to ensure code quality, catch regressions, and enable confident deployment. The system needed to support multiple Python versions, external service mocking, and comprehensive test coverage across all agents and tools.

### Decision

Implemented a comprehensive testing infrastructure with GitHub Actions CI/CD pipeline supporting multiple Python versions and extensive external service mocking:

**GitHub Actions CI Pipeline:**

- **Multi-Version Testing**: Python 3.9, 3.10, 3.11, and 3.12 support with matrix strategy
- **Dependency Management**: Automated pip upgrades and requirements.txt installation
- **Test Execution**: pytest with verbose output and comprehensive coverage reporting
- **Security Scanning**: bandit for security vulnerabilities, git-secrets for credential detection
- **Code Quality**: flake8 linting with E501 (line length) exceptions for readability

**External Service Mocking Strategy:**

- **YouTube API**: Mock google-api-python-client with realistic response structures
- **AssemblyAI**: Mock requests library with job status progression simulation
- **Firestore**: Mock google-cloud-firestore with in-memory document storage
- **Google Drive**: Mock googleapiclient with file upload and metadata handling
- **Slack**: Mock slack_sdk with channel resolution and message posting
- **Zep**: Mock zep-python with document storage and collection management

**Testing Framework Features:**

- **Environment Variable Isolation**: Comprehensive mocking of all required environment variables
- **Service Response Simulation**: Realistic API responses for various success and failure scenarios
- **Error Condition Testing**: Network failures, quota exhaustion, authentication errors, and rate limits
- **Integration Testing**: End-to-end workflows with multiple service interactions
- **Performance Testing**: Timeout handling and resource usage validation

**TESTING.md Documentation:**

- Complete guide for running tests locally and in CI environments
- Mocking strategies and patterns for external service dependencies
- Troubleshooting guide for common testing issues and environment setup
- Coverage reporting and quality metrics interpretation

### Alternatives Considered

**Docker-Based Testing**: Container-based test environments would provide more isolation but add complexity and CI runtime overhead for the current project scope.

**Real API Testing**: Using actual external services in CI would provide more realistic testing but would require credentials management, quota consumption, and network dependency issues.

**Simplified Mocking**: Basic mocking without realistic response structures would be faster but would miss integration issues and response format changes.

### Consequences

- **Pros**: Comprehensive CI pipeline catches regressions and ensures code quality across multiple Python versions, extensive mocking enables testing without external dependencies or credentials, realistic response simulation catches integration issues early, security scanning prevents credential leaks and vulnerability introduction, automated testing accelerates development and deployment confidence, documentation enables contributor onboarding and testing troubleshooting
- **Cons / risks**: Complex mocking setup requires maintenance when external APIs change, CI pipeline adds build time and resource usage for comprehensive testing, mock responses may diverge from actual API behavior over time, multiple Python version testing increases CI complexity and execution time

- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- GitHub Actions CI pipeline implemented with .github/workflows/ci.yml (87 lines) supporting Python 3.9-3.12
- Comprehensive external service mocking verified for YouTube, AssemblyAI, Firestore, Google Drive, Slack, and Zep
- Security scanning integrated with bandit and git-secrets tools
- TESTING.md documentation created with 156 lines covering all testing scenarios and troubleshooting
- Environment variable mocking implemented for all required credentials and configuration
- Test coverage reporting configured with pytest and verbose output
- Code quality checks added with flake8 linting and E501 line length exceptions

---

## ADR-0028 — Documentation Standardization and Roadmap Implementation

<a id="adr-0028"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

Task 55 (documentation-roadmap.mdc) required implementing comprehensive documentation updates, changelog management, and GitHub roadmap creation to provide clear project visibility, version tracking, and future planning guidance for the Autopiloot Agency project.

### Decision

Implemented comprehensive documentation standardization with changelog management and GitHub roadmap integration:

**CHANGELOG.md Implementation:**

- **Keep a Changelog Format**: Standardized format with Added, Changed, Deprecated, Removed, Fixed, and Security sections
- **Version Management**: Semantic versioning with clear release notes and migration guidance
- **Chronological Organization**: Latest releases at top with comprehensive change descriptions
- **Link Integration**: Cross-references to GitHub issues, pull requests, and related documentation

**GitHub Roadmap Features:**

- **Milestone Planning**: Quarterly release cycles with feature groupings and priority levels
- **Issue Templates**: Standardized templates for bug reports, feature requests, and documentation updates
- **Project Boards**: Kanban-style organization with automated workflows and progress tracking
- **Release Planning**: Version-based milestones with scope definition and timeline estimates

**Documentation Updates:**

- **ENVIRONMENT.md Enhancement**: Expanded service account setup with detailed Google Cloud configuration
- **README.md Improvements**: Updated with current architecture, feature list, and getting started guide
- **API Documentation**: Comprehensive tool documentation with usage examples and parameter descriptions
- **Deployment Guides**: Step-by-step instructions for production deployment and configuration management

**Versioning Strategy:**

- **Semantic Versioning**: MAJOR.MINOR.PATCH format with clear compatibility guidelines
- **Release Branches**: Feature branches merged to main with release tagging
- **Deprecation Policy**: 6-month notice for breaking changes with migration documentation
- **Changelog Automation**: GitHub Actions integration for automated changelog generation

### Alternatives Considered

**Wiki-Based Documentation**: GitHub wiki would provide more flexibility but lacks version control integration and offline access capabilities.

**External Documentation Platform**: Services like GitBook or Notion would provide better formatting but add external dependencies and access complexity.

**Automated Changelog**: Tools like conventional-changelog would automate generation but require strict commit message conventions and may miss important context.

### Consequences

- **Pros**: Standardized changelog format provides clear version history and upgrade guidance, GitHub roadmap integration enables public visibility and community contribution, comprehensive documentation reduces onboarding time and support requests, semantic versioning provides clear compatibility expectations, release planning improves project management and timeline predictability
- **Cons / risks**: Documentation maintenance requires ongoing effort and discipline, changelog accuracy depends on manual entry and review processes, roadmap commitments may create external pressure and timeline constraints, version management complexity increases with multiple release branches

- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- CHANGELOG.md implemented with Keep a Changelog format and comprehensive version history
- GitHub roadmap created with milestone planning and issue template integration
- ENVIRONMENT.md expanded with detailed service account setup and troubleshooting guides
- README.md updated with current architecture overview and getting started instructions
- Semantic versioning strategy documented with clear compatibility guidelines
- Documentation cross-references verified with working links and proper formatting
- Release planning process established with quarterly milestones and scope definition

---

## ADR-0029 — Security Architecture and Service Account Management

<a id="adr-0029"></a>
**Date**: 2025-09-15
**Status**: Accepted
**Owner**: AI Agent

### Context

Task 56 (security-secrets.mdc) required implementing comprehensive security audit, service account management, and credential handling improvements to ensure production-ready security posture for the Autopiloot Agency system with proper secret management and minimal privilege access patterns.

### Decision

Implemented comprehensive security architecture with Google Cloud service account management and credential handling best practices:

**Service Account Security:**

- **Minimal IAM Roles**: Cloud Datastore User for Firestore access, Storage Object Admin for file uploads, Drive File scope for created files only
- **Key Rotation Policy**: 90-day service account key rotation with automated reminders and documentation
- **Environment Isolation**: Separate service accounts for development, staging, and production environments
- **Access Monitoring**: Audit logging enabled for all service account usage with regular review processes

**Secret Management:**

- **Environment Variable Isolation**: All secrets stored in .env files with .gitignore protection
- **Credential Validation**: Startup checks for required environment variables with descriptive error messages
- **Path Security**: Absolute paths for credential files with existence validation
- **Backup Strategy**: Secure credential backup procedures with encrypted storage recommendations

**Security Scanning:**

- **Automated Scanning**: GitHub Actions integration with bandit for Python security issues
- **Secret Detection**: git-secrets integration preventing credential commits to repository
- **Dependency Scanning**: Regular vulnerability checks for all Python dependencies
- **Code Review**: Security-focused review checklist for credential handling and API usage

**Google Cloud Configuration:**

- **API Enablement**: Firestore, Drive, and Sheets APIs with minimal required permissions
- **Network Security**: VPC configuration recommendations for production deployments
- **Audit Logging**: Cloud Audit Logs enabled for all service account operations
- **Access Control**: IAM policies with principle of least privilege and regular access reviews

**ENVIRONMENT.md Security Enhancements:**

- **Service Account Setup**: Step-by-step Google Cloud Console configuration with screenshots
- **Permission Verification**: Commands for validating service account access and troubleshooting
- **Security Best Practices**: Key management, rotation schedules, and monitoring recommendations
- **Troubleshooting Guide**: Common security issues and resolution procedures

### Alternatives Considered

**Secret Management Services**: Google Secret Manager or AWS Secrets Manager would provide more sophisticated secret handling but add complexity and external dependencies for the current scope.

**Certificate-Based Authentication**: Client certificates would provide stronger authentication but require more complex setup and certificate management infrastructure.

**OAuth Flow**: OAuth 2.0 flow would provide better user experience but requires web application setup and token refresh management complexity.

### Consequences

- **Pros**: Minimal IAM roles reduce attack surface and follow security best practices, automated secret scanning prevents credential leaks and security vulnerabilities, service account isolation provides environment-specific access control, comprehensive documentation enables secure deployment and troubleshooting, audit logging provides security visibility and compliance support, key rotation policy ensures credential freshness and reduces compromise risk
- **Cons / risks**: Security complexity may slow development and deployment processes, key rotation requires operational overhead and process management, minimal permissions may require adjustments as features expand, audit logging increases storage costs and monitoring complexity, security scanning may produce false positives requiring triage

- **Supersedes**: —
- **Superseded by**: —

### Compliance / Verification

- Google Cloud service account setup documented with minimal IAM roles and security best practices
- ENVIRONMENT.md expanded with comprehensive security configuration and troubleshooting guides
- Secret scanning implemented with bandit and git-secrets in GitHub Actions CI pipeline
- Service account key rotation policy documented with 90-day schedule and automated reminders
- Environment isolation strategy implemented with separate accounts for dev/staging/production
- Audit logging configuration verified with Cloud Audit Logs enablement and monitoring setup
- Security review checklist created for credential handling and API usage patterns

## ADR-0030 — Tool Filename Standardization to Snake Case

<a id="adr-0030"></a>
**Date**: 2025-09-16
**Status**: Accepted
**Owner**: AI Agent

### Context

The Autopiloot Agency codebase contained mixed naming conventions for tool files, with 9 tools using PascalCase filenames (e.g., `EnqueueTranscription.py`, `SaveVideoMetadata.py`) and 32 tools already using snake_case (e.g., `get_video_audio_url.py`, `send_error_alert.py`). This inconsistency created several issues:

- **Import confusion**: Mixed case patterns made it harder to predict import paths
- **Code style inconsistency**: Violated Python naming conventions (PEP 8 recommends snake_case for modules)
- **Discovery issues**: Some tools might be harder to locate due to naming patterns
- **Maintenance overhead**: Developers had to remember which naming convention each tool used

Agency Swarm v1.0.0 framework works with both naming patterns, but standardizing on snake_case aligns with Python best practices and improves codebase consistency.

### Decision

**Standardize all tool filenames to snake_case across the entire Autopiloot Agency codebase:**

1. **Rename all PascalCase tool files** to snake_case equivalents using `git mv` to preserve history
2. **Update all imports and references** across the codebase to use new snake_case module names
3. **Maintain class names in PascalCase** as per Agency Swarm requirements (class names remain unchanged)
4. **Update all `tools/__init__.py` exports** to reflect new module names
5. **Update all test files** that import these tools
6. **Create comprehensive mapping documentation** for reference and potential rollback

**Specific transformations applied:**
- `EnqueueTranscription.py` → `enqueue_transcription.py`
- `ExtractYouTubeFromPage.py` → `extract_youtube_from_page.py`
- `ListRecentUploads.py` → `list_recent_uploads.py`
- `ReadSheetLinks.py` → `read_sheet_links.py`
- `RemoveSheetRow.py` → `remove_sheet_row.py`
- `ResolveChannelHandles.py` → `resolve_channel_handles.py`
- `SaveVideoMetadata.py` → `save_video_metadata.py`
- `ProcessSummaryWorkflow.py` → `process_summary_workflow.py`
- `SaveSummaryRecordEnhanced.py` → `save_summary_record_enhanced.py`

### Consequences

**Positive:**
- **Consistent Python naming**: All 41 tool files now follow PEP 8 snake_case convention
- **Improved predictability**: Developers can reliably predict import paths
- **Better IDE support**: Autocomplete and navigation work more consistently
- **Cleaner codebase**: Unified style across all agents and tools
- **Documentation clarity**: File listings in documentation are now consistent
- **Import safety**: No more confusion between PascalCase and snake_case imports

**Implementation details:**
- Preserved git history using `git mv` for all renamed files
- Generated comprehensive mapping file at `planning/rename_map_tools_snake_case.json`
- Updated 15+ test files with corrected import statements
- Verified all imports work correctly with `python -m unittest discover tests -v`
- Updated all `tools/__init__.py` files across 5 agent directories
- Maintained Agency Swarm v1.0.0 compatibility (class names unchanged)

**Maintenance:**
- Future tools must follow snake_case filename convention
- CI enforcement can be added to prevent regression to mixed naming
- Documentation automatically reflects consistent naming patterns
- Tool discovery scripts work with predictable naming scheme

---

## ADR-0031 — Firebase Functions Configuration Normalization

<a id="adr-0031"></a>
**Date**: 2025-09-17
**Status**: Accepted
**Owner**: AI Agent

### Context

Firebase Functions were using inconsistent patterns for accessing environment variables and configuration:
- Direct `os.getenv()` calls scattered throughout function code
- Hardcoded constants mixed with configurable values
- No centralized configuration validation
- Inconsistent error handling for missing environment variables

This created maintenance challenges and reduced reliability of the Firebase Functions deployment.

### Decision

Implement centralized configuration normalization for Firebase Functions:

1. **Replace direct `os.getenv()` calls** with centralized helper functions:
   - `get_required_env_var(key, description)` for mandatory environment variables
   - `get_config_value(key_path, default=None)` for settings.yaml configuration

2. **Standardize configuration imports** across all Firebase Functions:
   - Import helpers from `core/env_loader` and `config/config`
   - Remove direct `os` module imports where replaced

3. **Move constants to configuration**:
   - Budget thresholds from hardcoded to `config/settings.yaml`
   - Channel names and operational parameters configurable
   - Default values preserved for backward compatibility

4. **Add configuration validation**:
   - Required environment variables validated at startup
   - Clear error messages for missing configuration
   - Type validation for numeric configuration values

### Implementation

**Files Modified:**
- `services/firebase/functions/core.py` - Environment variable access normalization
- `services/firebase/functions/main.py` - Import standardization
- `services/firebase/functions/scheduler.py` - Configuration-driven constants
- `config/settings.yaml` - Added budget and alert configuration sections

**Key Changes:**
```python
# Before: Direct os.getenv() calls
slack_token = os.getenv("SLACK_BOT_TOKEN")

# After: Centralized helpers with validation
slack_token = get_required_env_var("SLACK_BOT_TOKEN", "Slack Bot Token for API access")
```

**Configuration additions:**
```yaml
budgets:
  transcription_daily_usd: 5.0
  alert_threshold: 0.8  # 80% threshold for budget alerts
```

### Consequences

**Positive:**
- **Centralized configuration management**: All environment access goes through validated helpers
- **Improved error handling**: Clear error messages for missing configuration
- **Enhanced maintainability**: Configuration changes don't require code modifications
- **Better testing**: Mock-friendly configuration access patterns
- **Deployment reliability**: Configuration validation prevents runtime failures

**Neutral:**
- **Additional abstraction layer**: Small increase in function complexity
- **Configuration dependency**: Functions now depend on core utilities

**Implementation details:**
- Maintained backward compatibility with existing environment variables
- Added comprehensive error handling and logging
- Validated all Firebase Functions deploy correctly
- No breaking changes to existing functionality

---

## ADR-0032 — Edge Case Testing Framework Enhancement

<a id="adr-0032"></a>
**Date**: 2025-09-17
**Status**: Accepted
**Owner**: AI Agent

### Context

The testing framework needed comprehensive edge case coverage for critical system components:
- Orchestrator policy enforcement lacked boundary condition testing
- Daily digest functionality needed edge case validation
- System reliability required comprehensive failure scenario testing
- Previous testing focused on happy path scenarios

Edge cases and boundary conditions are critical for production reliability, especially for operational systems handling cost monitoring and workflow orchestration.

### Decision

Implement comprehensive edge case testing framework covering boundary conditions, invalid inputs, and error scenarios:

1. **Orchestrator Tools Edge Cases**:
   - Policy enforcement with extreme values and invalid configurations
   - Daily planning with malformed inputs and missing parameters
   - Boundary testing for limits, thresholds, and validation rules

2. **Daily Digest Edge Cases**:
   - Empty day scenarios with missing data
   - Dead Letter Queue spike detection and handling
   - Cost overrun scenarios and budget threshold testing
   - Timezone edge cases including DST transitions

3. **Testing Infrastructure**:
   - Mock-based testing for external service failures
   - Comprehensive error condition coverage
   - Boundary value analysis for numeric parameters
   - Invalid input validation testing

### Implementation

**New Test Files Created (65 tests total):**

**Orchestrator Tools:**
- `tests/orchestrator_tools/test_enforce_policies_boundaries.py` (13 tests)
  - Extreme policy values, invalid configurations, boundary conditions
- `tests/orchestrator_tools/test_plan_daily_run_invalid_plan.py` (16 tests)
  - Malformed planning scenarios, invalid parameters, edge cases

**Observability Tools:**
- `tests/observability_tools/test_daily_digest_empty_day.py` (10 tests)
  - Missing data scenarios, corrupted documents, connection failures
- `tests/observability_tools/test_daily_digest_dlq_spike.py` (8 tests)
  - High volume failures, error patterns, system degradation
- `tests/observability_tools/test_daily_digest_cost_overrun.py` (8 tests)
  - Budget threshold breaches, anomalous costs, alert scenarios
- `tests/observability_tools/test_daily_digest_timezone_edges.py` (10 tests)
  - DST transitions, international date line, invalid timezones

**Key Testing Patterns:**
```python
# Boundary value testing
def test_policy_enforcement_extreme_values(self):
    result = enforce_policies(max_videos_per_channel=99999, budget_limit=0.01)

# Invalid input validation
def test_daily_digest_malformed_date(self):
    with self.assertRaises(ValueError):
        generate_daily_digest(date="invalid-date-format")

# Edge case mocking
@patch('firestore_client.collection')
def test_empty_data_scenario(self, mock_collection):
    mock_collection.return_value.get.return_value = []
```

### Consequences

**Positive:**
- **Comprehensive edge case coverage**: 65 new tests covering boundary conditions
- **Production reliability**: Early detection of edge case failures
- **Robust error handling**: Validation of system behavior under stress
- **Regression prevention**: Edge cases tested automatically in CI
- **Operational confidence**: Critical workflows validated under adverse conditions

**Neutral:**
- **Increased test suite size**: 65 additional tests increase execution time
- **Mock complexity**: Extensive mocking required for edge case simulation

**Testing coverage areas:**
- Policy enforcement with extreme and invalid values
- Daily digest with missing/corrupted data scenarios
- Timezone handling across DST transitions and international boundaries
- Error spike detection and alerting thresholds
- Budget monitoring and cost overrun scenarios
- System degradation and recovery patterns

---

## ADR-0033 — Daily Digest Operational Documentation Standardization

<a id="adr-0033"></a>
**Date**: 2025-09-17
**Status**: Accepted
**Owner**: AI Agent

### Context

The daily digest functionality required comprehensive operational documentation for:
- Operations teams managing the 07:00 Europe/Amsterdam delivery schedule
- Configuration management and troubleshooting procedures
- Integration with existing Firebase Functions and Slack infrastructure
- Production deployment and monitoring guidance

Documentation needed to cover technical configuration, operational procedures, and troubleshooting workflows to ensure reliable daily digest delivery.

### Decision

Implement comprehensive daily digest operational documentation across multiple documentation layers:

1. **User-Facing Documentation** (`readme.md`):
   - Daily digest feature overview and value proposition
   - Sample output with realistic data examples
   - Configuration options and runtime behavior
   - Common troubleshooting scenarios and solutions

2. **Technical Implementation Documentation** (`docs/firebase_implementation.md`):
   - Firebase Functions integration details
   - Deployment procedures and environment requirements
   - Comprehensive troubleshooting tables with specific error scenarios
   - Manual testing procedures and validation steps

3. **Operational Procedures**:
   - Configuration management via `config/settings.yaml`
   - Channel override and content customization options
   - Timezone handling and delivery schedule constraints
   - Manual triggering procedures for testing

### Implementation

**Documentation Updates:**

**README.md Daily Digest Section:**
- **Features overview**: Processing summary, cost analysis, error monitoring, quick links
- **Sample output**: Realistic Slack message example with proper formatting
- **Configuration**: Complete YAML configuration with all available options
- **Troubleshooting table**: Common issues mapped to specific solutions

```markdown
## 📅 Daily Digest

### Features
- 📊 Processing Summary: Videos discovered, transcribed, summarized
- 💰 Cost Analysis: Daily spend vs. budget with percentage usage
- ⚠️ Error Monitoring: Dead letter queue alerts and system health
- 🔗 Quick Links: Direct access to Google Drive and system resources

### Configuration
```yaml
notifications:
  slack:
    digest:
      enabled: true
      time: "07:00"                   # Fixed at deployment
      timezone: "Europe/Amsterdam"    # Timezone for date calculations
      channel: "ops-autopiloot"       # Target Slack channel
```

**Firebase Implementation Documentation:**
- **Function specification**: `daily_digest_delivery` scheduled function details
- **Environment requirements**: SLACK_BOT_TOKEN and configuration dependencies
- **Troubleshooting tables**: Comprehensive error scenarios with specific solutions
- **Manual testing**: Procedures for validation and verification

### Consequences

**Positive:**
- **Operational clarity**: Clear procedures for daily digest management
- **Troubleshooting efficiency**: Specific solutions mapped to common issues
- **Configuration transparency**: All options documented with examples
- **Deployment confidence**: Step-by-step procedures reduce deployment risk
- **Team enablement**: Operations teams can manage digest without developer intervention

**Documentation coverage:**
- Feature overview with value proposition and sample output
- Complete configuration options with runtime vs. deployment-time constraints
- Troubleshooting procedures covering delivery, channel, data, and timezone issues
- Manual testing and validation procedures
- Integration guidance for Firebase Functions and Slack infrastructure

**Maintenance:**
- Documentation stays current with digest feature evolution
- Troubleshooting procedures validated against real operational scenarios
- Configuration examples tested and verified functional
- Sample output updated to reflect actual system behavior

---

## ADR-0034 — LinkedIn Agent Implementation with Scheduled Ingestion
<a id="adr-0034"></a>

**Date**: 2025-09-18
**Status**: Accepted
**Owner**: AI Agent

### Context

The Autopiloot system successfully processes YouTube content but lacks LinkedIn content ingestion capabilities. LinkedIn provides valuable business insights, engagement metrics, and professional content that complements the existing knowledge management workflow. The system requires a comprehensive LinkedIn Agent with scheduled ingestion, data processing, and Zep GraphRAG integration to expand content sources beyond YouTube.

Key requirements:
- Professional social media content ingestion from LinkedIn profiles
- Integration with existing Zep GraphRAG knowledge management system
- Scheduled daily processing with operational monitoring
- Comprehensive data pipeline: fetch → normalize → deduplicate → analyze → store
- Firebase Functions integration following established patterns
- Complete test coverage for production readiness

### Alternatives

- **Manual LinkedIn data collection**: Labor-intensive, inconsistent, no automation, limited scalability
- **Third-party LinkedIn analytics tools**: Expensive SaaS subscriptions, limited data control, vendor lock-in, integration complexity
- **Direct LinkedIn API integration**: Rate limiting issues, complex authentication, unofficial API deprecation risk
- **Simple content scraping**: Legal compliance risks, brittle implementation, no structured data processing
- **Separate service/application**: Additional infrastructure complexity, deployment overhead, inter-service communication challenges

### Decision

**Implement comprehensive LinkedIn Agent with RapidAPI integration and Firebase Functions scheduling:**

**Agent Architecture:**
- **LinkedInAgent**: Agency Swarm v1.0.0 compliant agent with 9 specialized tools
- **Tool Suite**: Complete data pipeline from ingestion through Zep storage
- **Configuration**: Centralized settings.yaml with profile lists, limits, and processing rules
- **Error Handling**: Comprehensive retry logic, exponential backoff, dead letter queue integration

**Core Tools (9 total):**
1. **GetUserPosts**: RapidAPI integration for LinkedIn posts with pagination
2. **GetPostComments**: Batch comment extraction with nested replies support
3. **GetPostReactions**: Reaction metrics aggregation and engagement analysis
4. **GetUserCommentActivity**: User engagement tracking and networking behavior
5. **NormalizeLinkedInContent**: Schema v1.0 compliance and data standardization
6. **DeduplicateEntities**: Multi-strategy deduplication (keep_latest, keep_first, merge_data)
7. **ComputeLinkedInStats**: Statistical analysis, engagement metrics, trend detection
8. **UpsertToZepGroup**: Zep GraphRAG integration with group-based content organization
9. **SaveIngestionRecord**: Firestore audit logging with comprehensive metrics tracking

**Scheduling Integration:**
- **Schedule**: Daily execution at 06:00 Europe/Amsterdam (configurable)
- **Firebase Functions**: `schedule_linkedin_daily()` with 5-minute timeout, 512MB memory
- **Agent Helpers**: Lazy initialization pattern consistent with orchestrator/observability agents
- **Operational Monitoring**: Slack summaries, audit logging, error alerting

**Data Processing Pipeline:**
- **Content Types**: Posts, comments, reactions (configurable)
- **Daily Limits**: 25 items per profile per day (configurable)
- **Profile Management**: Multiple target profiles via settings.yaml configuration
- **Quality Control**: Engagement thresholds, content validation, duplicate detection
- **Storage**: Zep GraphRAG groups with structured metadata and semantic search capabilities

**Testing Framework:**
- **Comprehensive Coverage**: 10 test files covering all 9 tools plus test suite runner
- **Mock Integration**: External APIs mocked (RapidAPI, Zep, Firestore) for isolated testing
- **Validation Testing**: Pydantic field validation, error handling, edge cases
- **Production Readiness**: Syntax validation, environment configuration testing

### Consequences

**Positive:**
- **Content Diversification**: LinkedIn business insights complement YouTube educational content
- **Knowledge Management**: Structured professional content in Zep GraphRAG for semantic search
- **Operational Excellence**: Automated daily ingestion with comprehensive monitoring
- **Scalable Architecture**: Multi-profile support with configurable limits and processing rules
- **Data Quality**: Normalization, deduplication, and statistical analysis ensure clean data
- **Production Ready**: Complete test coverage, error handling, and operational monitoring
- **Integration Consistency**: Follows established Firebase Functions and agent patterns

**Technical Benefits:**
- **9-Tool Pipeline**: Complete data processing from ingestion through storage
- **RapidAPI Integration**: Reliable third-party LinkedIn data access with rate limiting
- **Schema Compliance**: Version 1.0 normalization ensures data consistency
- **Audit Trail**: Comprehensive Firestore logging for operational monitoring
- **Error Resilience**: Exponential backoff, retry logic, and graceful degradation
- **Mock Testing**: Development and testing without external API dependencies

**Operational Impact:**
- **Daily Automation**: Set-and-forget LinkedIn content processing
- **Resource Management**: Configurable limits prevent API quota exhaustion
- **Monitoring Integration**: Slack notifications and audit logging for operational visibility
- **Quality Assurance**: Statistical analysis and engagement metrics for content insights
- **Maintenance**: Snake case tool naming and comprehensive documentation

**Maintenance:**
- **Configuration Management**: Centralized settings.yaml for runtime adjustments
- **Test Maintenance**: Comprehensive test suite enables confident updates
- **Documentation**: Complete tool documentation and test coverage reports
- **Operational Procedures**: Slack alerts and audit trails for troubleshooting

---

## ADR-0035 — Google Drive Agent Implementation with Zep GraphRAG Integration
<a id="adr-0035"></a>

**Date**: 2025-09-19
**Status**: Accepted
**Owner**: AI Agent

### Context

The Autopiloot system successfully processes YouTube content and LinkedIn data but lacks comprehensive document and file content ingestion capabilities. Google Drive serves as a primary repository for strategic documents, research materials, meeting notes, and business intelligence that would enhance the knowledge management workflow. The system requires a robust Google Drive Agent with scheduled incremental processing, text extraction from multiple formats, and Zep GraphRAG integration to expand knowledge sources beyond social media content.

Key requirements:
- Automated Google Drive content discovery and incremental processing
- Multi-format text extraction (PDF, DOCX, HTML, CSV, plain text)
- Integration with existing Zep GraphRAG knowledge management system
- Scheduled processing with configurable sync intervals (3-hour default)
- Comprehensive audit logging and operational monitoring
- Firebase Functions integration following established patterns
- Complete test coverage for production readiness

### Alternatives

- **Manual document processing**: Labor-intensive, inconsistent, no automation, limited scalability, human error prone
- **Third-party document management tools**: Expensive SaaS subscriptions, vendor lock-in, limited integration flexibility, data residency concerns
- **Google Drive API polling without change detection**: Inefficient API usage, quota exhaustion risk, duplicate processing overhead
- **File system synchronization approach**: Local storage requirements, sync conflicts, version control complexity, limited cloud deployment
- **Separate microservice architecture**: Additional infrastructure complexity, inter-service communication overhead, deployment coordination challenges

### Decision

**Implement comprehensive Google Drive Agent with incremental change detection and Zep GraphRAG integration:**

**Agent Architecture:**
- **DriveAgent**: Agency Swarm v1.0.0 compliant agent with 7 specialized tools
- **Tool Suite**: Complete pipeline from configuration loading through audit logging
- **Configuration**: Centralized settings.yaml with drive targets, file patterns, and processing limits
- **Error Handling**: Comprehensive retry logic, graceful degradation, checkpoint management

**Core Tools (7 total):**
1. **ListTrackedTargetsFromConfig**: Configuration loading and Drive target normalization
2. **ResolveFolderTree**: Recursive folder structure resolution with pattern filtering
3. **ListDriveChanges**: Incremental change detection since last checkpoint timestamp
4. **FetchFileContent**: Multi-format file fetching with Google Workspace export support
5. **ExtractTextFromDocument**: Robust text extraction pipeline supporting PDF, DOCX, HTML, CSV
6. **UpsertDriveDocsToZep**: Zep GraphRAG integration with document chunking and semantic indexing
7. **SaveDriveIngestionRecord**: Firestore audit logging with comprehensive metrics and performance tracking

**Scheduling Integration:**
- **Schedule**: Every 3 hours starting at 00:00 Europe/Amsterdam (configurable)
- **Firebase Functions**: `schedule_drive_ingestion()` with 5-minute timeout, 512MB memory allocation
- **Agent Helpers**: Lazy initialization pattern consistent with other agents (`get_drive_agent()`)
- **Operational Monitoring**: Slack notifications, comprehensive audit trails, error alerting

**Incremental Processing Design:**
- **Change Detection**: Uses Google Drive Changes API with checkpoint tokens
- **File Filtering**: fnmatch pattern support for include/exclude rules per target
- **Content Validation**: File size limits (25MB default), format validation, permission checks
- **Checkpoint Management**: ISO timestamp persistence for resumable processing
- **API Efficiency**: Minimal API calls through change-based detection vs. full scanning

**Text Extraction Pipeline:**
- **Multi-Format Support**: PDF (PyPDF2), DOCX (python-docx), HTML (BeautifulSoup), CSV (pandas), plain text
- **Google Workspace Integration**: Automatic export to supported formats (DOCX, PDF, HTML, CSV)
- **Content Processing**: Text cleaning, length limiting (100k chars), metadata extraction
- **Error Recovery**: Graceful format fallbacks, partial extraction on errors, detailed error logging

**Zep GraphRAG Integration:**
- **Document Chunking**: Configurable chunk sizes (2000 chars default) with overlap
- **Semantic Indexing**: Structured metadata for enhanced search and retrieval
- **Namespace Organization**: Configurable Zep namespaces for content isolation
- **Batch Processing**: Efficient bulk operations with comprehensive error tracking
- **Version Management**: Content hash comparison for duplicate detection

**Configuration Architecture:**
```yaml
drive_agent:
  targets:
    - id: "folder_strategy_docs"
      type: "folder"
      name: "Strategy Documents"
      include_patterns: ["*.pdf", "*.docx"]
      exclude_patterns: ["**/archive/**", "**/templates/**"]
  zep:
    namespace: "autopiloot_drive_content"
  sync:
    interval_hours: 3
  limits:
    max_file_size_mb: 25
    max_chunk_size_chars: 2000
```

### Consequences

**Positive:**
- **Knowledge Expansion**: Comprehensive document content adds strategic intelligence to existing social media data
- **Incremental Efficiency**: Change-based processing minimizes API usage and processing overhead
- **Format Flexibility**: Multi-format text extraction supports diverse document types
- **Operational Excellence**: Automated 3-hour sync with comprehensive monitoring and audit trails
- **Search Enhancement**: Zep GraphRAG integration enables semantic search across document corpus
- **Production Ready**: Complete test coverage, error handling, and operational monitoring
- **Integration Consistency**: Follows established Firebase Functions and agent communication patterns

**Technical Benefits:**
- **7-Tool Pipeline**: Complete workflow from configuration through audit logging
- **Google Drive API Integration**: Official API usage with proper authentication and quota management
- **Text Processing Robustness**: Multiple extraction methods with fallback mechanisms
- **Zep GraphRAG Enhancement**: Structured document indexing with semantic search capabilities
- **Audit Compliance**: Comprehensive Firestore logging for operational monitoring and debugging
- **Mock Testing Framework**: Development and testing without external API dependencies

**Operational Impact:**
- **Automated Knowledge Management**: Set-and-forget document processing with configurable targets
- **Resource Optimization**: Incremental processing prevents quota exhaustion and duplicate work
- **Monitoring Integration**: Slack notifications and detailed audit logs for operational visibility
- **Content Quality**: Text extraction quality validation and comprehensive error reporting
- **Maintenance Efficiency**: Snake case tool naming and comprehensive documentation standards

**Architecture Integration:**
- **Firebase Functions**: Consistent scheduling pattern with existing agents (LinkedIn, orchestrator)
- **Agent Communication**: CEO pattern integration for workflow coordination
- **Configuration Management**: Centralized settings.yaml following established patterns
- **Error Handling**: DLQ integration and exponential backoff consistent with reliability architecture
- **Observability**: Comprehensive metrics and alerting through existing observability agent

**Maintenance:**
- **Configuration Flexibility**: Runtime adjustments through settings.yaml without code changes
- **Test Coverage**: Comprehensive test suite enables confident updates and feature additions
- **Documentation Standards**: Complete tool documentation and operational procedures
- **Monitoring Infrastructure**: Slack alerts and audit trails for proactive troubleshooting

---

## ADR-0036 — Comprehensive Testing Framework and Dependency Management
<a id="adr-0036"></a>

**Date**: 2025-09-19
**Status**: Accepted
**Owner**: AI Agent

### Context

The Autopiloot system had achieved complete implementation across 8 agents with 86 production tools, but gaps existed in test coverage for newer agents (Drive Agent had 0 test files) and dependency management was incomplete. Missing test coverage for critical components creates deployment risk and makes feature evolution difficult. Additionally, specialized dependencies for text extraction (PDF, DOCX) and NLP analysis were not declared, leading to runtime failures in production deployments.

Key requirements:
- Complete test coverage for all 8 agents with consistent testing patterns
- Comprehensive dependency management for specialized tool requirements
- Mock implementations for external API testing without credentials
- Production readiness validation and deployment preparation
- Consistent testing framework across LinkedIn, Strategy, and Drive agents

### Alternatives

- **Minimal testing approach**: Test only core workflows, skip comprehensive edge cases, faster development but higher production risk
- **Manual testing only**: Rely on human QA testing, no automated test suite, not scalable for 86 tools across 8 agents
- **External testing services**: Use third-party testing platforms, additional cost and complexity, vendor dependency
- **Incomplete dependency management**: Install packages as needed during deployment, runtime failures, inconsistent environments
- **Real API testing only**: No mocking, requires production credentials for testing, expensive and brittle

### Decision

**Implement comprehensive testing framework with complete dependency management:**

**Testing Framework Architecture:**
- **Drive Agent Test Suite**: 8 comprehensive test files covering all 7 Drive Agent tools
- **Mock-First Design**: External APIs (Google Drive, Zep, Firestore) mocked for isolated testing
- **Test Pattern Consistency**: Follow established LinkedIn Agent testing patterns across all agents
- **Coverage Validation**: 72 total test files ensuring comprehensive agent tool coverage
- **Suite Runner**: Orchestrated test execution with detailed reporting and coverage metrics

**Drive Agent Test Coverage (8 test files):**
1. **test_list_tracked_targets_from_config.py**: Configuration loading, validation, pattern normalization
2. **test_resolve_folder_tree.py**: Recursive folder traversal, pagination, authentication handling
3. **test_list_drive_changes.py**: Incremental change detection, timestamp filtering, API error handling
4. **test_fetch_file_content.py**: Multi-format fetching, Google Workspace exports, size limit enforcement
5. **test_extract_text_from_document.py**: Text extraction pipeline, encoding detection, format support
6. **test_upsert_drive_docs_to_zep.py**: Zep GraphRAG integration, document chunking, batch processing
7. **test_save_drive_ingestion_record.py**: Firestore audit logging, performance metrics, error categorization
8. **test_suite_runner.py**: Test orchestration, coverage reporting, production readiness validation

**Dependency Management Enhancement:**
```python
# Drive Agent text extraction dependencies
PyPDF2>=3.0.1        # PDF text extraction
python-docx>=0.8.11   # DOCX document processing
pandas>=2.0.3         # CSV data processing

# Strategy Agent NLP dependencies
scikit-learn>=1.3.0   # Machine learning and clustering
nltk>=3.8.1           # Natural language processing
```

**Mock Implementation Strategy:**
- **Google Drive API**: Mock service building, file operations, and authentication
- **Zep GraphRAG**: Mock client initialization, document operations, and collection management
- **Firestore**: Mock database operations, document creation, and query execution
- **Firebase Functions**: Mock agent helpers and lazy initialization patterns
- **External Services**: Comprehensive error simulation and edge case handling

**Testing Standards:**
- **Agency Swarm Compliance**: All tests validate BaseTool inheritance and Pydantic Field usage
- **Error Handling**: Comprehensive testing of API failures, network timeouts, and authentication errors
- **Edge Cases**: Empty responses, malformed data, quota exhaustion, and permission denied scenarios
- **Performance Testing**: File size limits, processing duration validation, and resource management
- **Integration Testing**: End-to-end workflow validation with mocked external dependencies

### Consequences

**Positive:**
- **Production Confidence**: Complete test coverage eliminates deployment risk for critical Drive Agent functionality
- **Development Velocity**: Comprehensive mocking enables rapid development without external API dependencies
- **Regression Prevention**: 72 test files provide safety net for feature evolution and refactoring
- **Dependency Clarity**: Explicit requirements.txt prevents runtime failures and environment inconsistencies
- **Operational Excellence**: Test suite validation ensures all 86 tools maintain production readiness standards

**Technical Benefits:**
- **8-Agent Coverage**: Every agent now has comprehensive test coverage following consistent patterns
- **Mock Framework**: Reusable mocking patterns for Drive API, Zep, and Firestore across test suites
- **Validation Pipeline**: Automated testing enables CI/CD deployment with confidence
- **Error Simulation**: Comprehensive error scenario testing improves production resilience
- **Documentation**: Test files serve as executable documentation for tool usage and behavior

**Dependency Management:**
- **Text Processing**: PyPDF2, python-docx, pandas enable robust document content extraction
- **NLP Capabilities**: scikit-learn, nltk support Strategy Agent content analysis and clustering
- **Version Pinning**: Explicit version constraints prevent dependency conflicts in production
- **Environment Consistency**: Complete requirements.txt ensures reproducible deployments
- **Security**: Pinned versions enable vulnerability scanning and dependency auditing

**Development Workflow:**
- **Test-Driven Development**: Comprehensive test suite supports confident feature development
- **Continuous Integration**: Automated testing validates changes before deployment
- **Quality Gates**: Test coverage requirements prevent incomplete implementations
- **Debugging**: Isolated tests enable rapid issue identification and resolution
- **Team Collaboration**: Standardized testing patterns reduce onboarding time for new developers

**Maintenance:**
- **Test Maintenance**: Comprehensive test coverage requires ongoing maintenance as features evolve
- **Dependency Updates**: Regular dependency updates needed for security and feature improvements
- **Mock Accuracy**: Mock implementations must stay current with external API changes
- **Performance**: Large test suite increases CI/CD execution time but provides comprehensive validation

---
